# (PART) Module 14 {-}

# Welcome to Large Language Models

```{r include = FALSE}
source("common.R")

ds4p_funyoutube <- read.csv("admin/csv/ds4p_funyoutube.csv", sep = "")
ds4p_urls <- read.csv("./admin/csv/ds4p_urls.csv")

# install.packages("devtools")

if (!require("tweetrmd"))  devtools::install_github("gadenbuie/tweetrmd")
library(tweetrmd) #... embedding tweets
library(vembedr)  #... embedding youtube videos
library(knitr)
library(tidyverse)
```

This module introduces Large Language Models (LLMs) and their applications in data science. We'll explore basic concepts, use cases, ethical considerations, and hands-on applications of LLMs. Eventually, there will be video lectures. Once those videos exist, please watch the videos and work your way through the notes. **The videos will eventually start on the next page.**  You will eventually be able to find the video playlist for this module [here][pl_llm]. The slides used to make the videos in this module will be able to be found in the [slides repo][course_slides]. 

This module is in active development, as part of an AI teaching workshop sponsored by the Wake Forest University's Center for Teaching.

## Module Materials

* Slides from Lectures:
  * [Intro to LLMs][d31_llmintro], 
  * [LLMs in Data Science][d32_llmapplications], 
* Suggested Readings
  * All subchapters of this module, including
    * ...
  * R4DS
    * [TBD](https://r4ds.had.co.nz/), including
      * [TBD]
      * [TBD]
  * Key Papers:
    * [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al., NeurIPS  ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount&query=%24.citationCount&label=citation) 2017.
      - This paper introduces the Transformer model, a fundamental architecture for modern LLMs.
    * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf) by Devlin et al., NAACL 2019.
      - BERT introduces bidirectional training and significantly improves NLP tasks.
    * [Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) by Brown et al., NeurIPS 2020.
      - This paper details GPT-3 and its few-shot learning capabilities.
    * [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) by Ouyang et al., NeurIPS 2022.
      - Discusses the integration of human feedback to improve LLM performance and ethical considerations.

* Activities
  * Exploring LLM Capabilities with OpenAI's API [TBD]
* Lab
  * [TBD](#labTBD)

## Estimated Video Length

<!--https://ytplaylist-len.herokuapp.com/ -->

No of videos : TBD

Average length of video : TBD

Total length of playlist : TBD




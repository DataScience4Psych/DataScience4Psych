# (PART) Module 11 {-}



# Welcome to Overfitting and Cross-Validation

This module is designed to introduce you to cross-validation and overfitting. Please watch the videos and work your way through the notes. **The videos start on the next page.** You can find the video playlist for this module [here][pl_11]. Most of the slides used to make the videos in this module can be found in the [slides repo][course_slides].



## Module Materials

* Videos and Slides from Lectures
  * [Overfitting](https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html)
  * [Cross-validation](https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html)
* Suggested Readings
  * All subchapters of this module
  * Articles
    * [de Rooij, M., & Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248-263.](https://journals.sagepub.com/doi/full/10.1177/2515245919898466)
    * [MacCallum, R. C., Zhang, S., Preacher, K. J., & Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40.](http://www.quantpsy.org/pubs/maccallum_zhang_preacher_rucker_2002.pdf)
  * R4DS
    * [Many Models](https://r4ds.had.co.nz/many-models.html)

# Lecture: Overfitting

You can follow along with the slides [here](https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html) if you would like to open them full-screen. The embedded code for feature enginering can be found [here](#featurenotes)

<iframe src="https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#1" width="672" height="400px" data-external="1"></iframe>

## Prediction



```{=html}
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/U70OmbO-DP4" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
```


<iframe src="https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#2" width="672" height="400px" data-external="1"></iframe>

## Workflow



```{=html}
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/R4h9u-sQHwI" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
```


<iframe src="https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#17" width="672" height="400px" data-external="1"></iframe>

# Lecture: Cross-Validation

You can follow along with the slides [here](https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html) if you would like to open them full-screen.


```{=html}
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/KQ9f8s7RB5g" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
```


<iframe src="https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html" width="672" height="400px" data-external="1"></iframe>

## V-Fold


```{=html}
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/quEVKV-Tk0Y" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
```


<iframe src="https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html#35" width="672" height="400px" data-external="1"></iframe>

# Notes on Feature Engineering {#featurenotes}



## Feature engineering

* We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity

* Variables that go into the model and how they are represented are just as critical to success of the model

* **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)

### Same training and testing sets as before


``` r
# Fix random numbers by setting the seed
# Enables analysis to be reproducible when random numbers are used
set.seed(1066)

# Put 80% of the data into the training set
email_split <- initial_split(email, prop = 0.80)

# Create data frames for the two sets:
train_data <- training(email_split)
test_data <- testing(email_split)
```

### A simple approach: `mutate()`


``` r
train_data %>%
  mutate(
    date = date(time),
    dow = wday(time),
    month = month(time)
  ) %>%
  select(time, date, dow, month) %>%
  sample_n(size = 5) # shuffle to show a variety
#> # A tibble: 5 × 4
#>   time                date         dow month
#>   <dttm>              <date>     <dbl> <dbl>
#> 1 2012-02-07 19:17:38 2012-02-07     3     2
#> 2 2012-02-29 00:24:15 2012-02-29     4     2
#> 3 2012-03-12 12:07:31 2012-03-12     2     3
#> 4 2012-01-26 18:58:18 2012-01-26     5     1
#> 5 2012-02-21 13:50:29 2012-02-21     3     2
```

## Modeling workflow, revisited

* Create a **recipe** for feature engineering steps to be applied to the training data

* Fit the model to the training data after these steps have been applied

* Using the model estimates from the training data, predict outcomes for the test data

* Evaluate the performance of the model on the test data

## Building recipes

### Initiate a recipe


``` r
email_rec <- recipe(
  spam ~ ., # formula
  data = train_data # data to use for cataloguing names and types of variables
)

summary(email_rec)
```


```
#> # A tibble: 21 × 4
#>    variable     type      role      source  
#>    <chr>        <list>    <chr>     <chr>   
#>  1 to_multiple  <chr [3]> predictor original
#>  2 from         <chr [3]> predictor original
#>  3 cc           <chr [2]> predictor original
#>  4 sent_email   <chr [3]> predictor original
#>  5 time         <chr [1]> predictor original
#>  6 image        <chr [2]> predictor original
#>  7 attach       <chr [2]> predictor original
#>  8 dollar       <chr [2]> predictor original
#>  9 winner       <chr [3]> predictor original
#> 10 inherit      <chr [2]> predictor original
#> 11 viagra       <chr [2]> predictor original
#> 12 password     <chr [2]> predictor original
#> 13 num_char     <chr [2]> predictor original
#> 14 line_breaks  <chr [2]> predictor original
#> 15 format       <chr [3]> predictor original
#> 16 re_subj      <chr [3]> predictor original
#> 17 exclaim_subj <chr [2]> predictor original
#> 18 urgent_subj  <chr [3]> predictor original
#> 19 exclaim_mess <chr [2]> predictor original
#> 20 number       <chr [3]> predictor original
#> 21 spam         <chr [3]> outcome   original
```

### Remove certain variables


``` r
email_rec <- email_rec %>%
  step_rm(from, sent_email)
```


```
#> 
#> ── Recipe ──────────────────────────────────────────────────────────────────────────────────────────
#> 
#> ── Inputs
#> Number of variables by role
#> outcome:    1
#> predictor: 20
#> 
#> ── Operations
#> • Variables removed: from sent_email
```

### Feature engineer date


``` r
email_rec <- email_rec %>%
  step_date(time, features = c("dow", "month")) %>%
  step_rm(time)
```


```
#> 
#> ── Recipe ──────────────────────────────────────────────────────────────────────────────────────────
#> 
#> ── Inputs
#> Number of variables by role
#> outcome:    1
#> predictor: 20
#> 
#> ── Operations
#> • Variables removed: from sent_email
#> • Date features from: time
#> • Variables removed: time
```

### Discretize numeric variables

Proceed with major caution! And please be sure to read [MacCallum, R. C., Zhang, S., Preacher, K. J., & Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40.](http://www.quantpsy.org/pubs/maccallum_zhang_preacher_rucker_2002.pdf) and play around with the demo data from Kris's website: <http://www.quantpsy.org/mzpr.htm>


``` r
email_rec <- email_rec %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20))
```


```
#> 
#> ── Recipe ──────────────────────────────────────────────────────────────────────────────────────────
#> 
#> ── Inputs
#> Number of variables by role
#> outcome:    1
#> predictor: 20
#> 
#> ── Operations
#> • Variables removed: from sent_email
#> • Date features from: time
#> • Variables removed: time
#> • Cut numeric for: cc, attach, dollar
#> • Cut numeric for: inherit password
```

### Create dummy variables


``` r
email_rec <- email_rec %>%
  step_dummy(all_nominal(), -all_outcomes())
```


```
#> 
#> ── Recipe ──────────────────────────────────────────────────────────────────────────────────────────
#> 
#> ── Inputs
#> Number of variables by role
#> outcome:    1
#> predictor: 20
#> 
#> ── Operations
#> • Variables removed: from sent_email
#> • Date features from: time
#> • Variables removed: time
#> • Cut numeric for: cc, attach, dollar
#> • Cut numeric for: inherit password
#> • Dummy variables from: all_nominal() -all_outcomes()
```

### Remove zero variance variables

Variables that contain only a single value


``` r
email_rec <- email_rec %>%
  step_zv(all_predictors())
```


```
#> 
#> ── Recipe ──────────────────────────────────────────────────────────────────────────────────────────
#> 
#> ── Inputs
#> Number of variables by role
#> outcome:    1
#> predictor: 20
#> 
#> ── Operations
#> • Variables removed: from sent_email
#> • Date features from: time
#> • Variables removed: time
#> • Cut numeric for: cc, attach, dollar
#> • Cut numeric for: inherit password
#> • Dummy variables from: all_nominal() -all_outcomes()
#> • Zero variance filter on: all_predictors()
```

### All in one place


``` r
email_rec <- recipe(spam ~ ., data = email) %>%
  step_rm(from, sent_email) %>%
  step_date(time, features = c("dow", "month")) %>%
  step_rm(time) %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())
```

## Building workflows

### Define model


``` r
email_mod <- logistic_reg() %>%
  set_engine("glm")

email_mod
#> Logistic Regression Model Specification (classification)
#> 
#> Computational engine: glm
```

### Define workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.


``` r
email_wflow <- workflow() %>%
  add_model(email_mod) %>%
  add_recipe(email_rec)
```


```
#> ══ Workflow ════════════════════════════════════════════════════════════════════════════════════════
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> ── Preprocessor ────────────────────────────────────────────────────────────────────────────────────
#> 7 Recipe Steps
#> 
#> • step_rm()
#> • step_date()
#> • step_rm()
#> • step_cut()
#> • step_cut()
#> • step_dummy()
#> • step_zv()
#> 
#> ── Model ───────────────────────────────────────────────────────────────────────────────────────────
#> Logistic Regression Model Specification (classification)
#> 
#> Computational engine: glm
```

### Fit model to training data


``` r
email_fit <- email_wflow %>%
  fit(data = train_data)
#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```


``` r
tidy(email_fit) %>% print(n = 31)
#> # A tibble: 32 × 5
#>    term               estimate  std.error statistic  p.value
#>    <chr>                 <dbl>      <dbl>     <dbl>    <dbl>
#>  1 (Intercept)        -1.11       0.274    -4.06    4.91e- 5
#>  2 image              -1.69       0.965    -1.75    7.98e- 2
#>  3 viagra              2.41     300.        0.00804 9.94e- 1
#>  4 num_char            0.0462     0.0271    1.70    8.89e- 2
#>  5 line_breaks        -0.00559    0.00151  -3.71    2.08e- 4
#>  6 exclaim_subj       -0.0667     0.271    -0.247   8.05e- 1
#>  7 exclaim_mess        0.0101     0.00214   4.70    2.58e- 6
#>  8 to_multiple_X1     -2.82       0.377    -7.48    7.59e-14
#>  9 cc_X.1.68.         -0.161      0.467    -0.345   7.30e- 1
#> 10 attach_X.1.21.      2.36       0.381     6.20    5.56e-10
#> 11 dollar_X.1.64.      0.0569     0.222     0.257   7.97e- 1
#> 12 winner_yes          2.20       0.418     5.27    1.39e- 7
#> 13 inherit_X.1.5.    -10.4     1226.       -0.00852 9.93e- 1
#> 14 inherit_X.5.10.     1.97       1.27      1.55    1.21e- 1
#> 15 password_X.1.5.    -1.77       0.757    -2.34    1.94e- 2
#> 16 password_X.5.10.  -13.4      697.       -0.0192  9.85e- 1
#> 17 password_X.10.20. -15.0     1319.       -0.0114  9.91e- 1
#> 18 password_X.20.28. -14.8     1342.       -0.0110  9.91e- 1
#> 19 format_X1          -0.808      0.161    -5.02    5.19e- 7
#> 20 re_subj_X1         -2.78       0.400    -6.96    3.47e-12
#> 21 urgent_subj_X1      2.55       1.12      2.29    2.22e- 2
#> 22 number_small       -0.659      0.170    -3.87    1.09e- 4
#> 23 number_big          0.221      0.246     0.897   3.70e- 1
#> 24 time_dow_Mon       -0.0727     0.309    -0.235   8.14e- 1
#> 25 time_dow_Tue        0.289      0.280     1.03    3.02e- 1
#> 26 time_dow_Wed       -0.167      0.288    -0.579   5.63e- 1
#> 27 time_dow_Thu       -0.165      0.293    -0.564   5.73e- 1
#> 28 time_dow_Fri       -0.0520     0.292    -0.178   8.58e- 1
#> 29 time_dow_Sat        0.313      0.305     1.03    3.05e- 1
#> 30 time_month_Feb      0.903      0.183     4.93    8.35e- 7
#> 31 time_month_Mar      0.644      0.186     3.46    5.48e- 4
#> # ℹ 1 more row
```

### Make predictions for test data


``` r
email_pred <- predict(email_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

email_pred
#> # A tibble: 785 × 23
#>    .pred_0  .pred_1 spam  to_multiple from     cc sent_email time                image attach dollar
#>      <dbl>    <dbl> <fct> <fct>       <fct> <int> <fct>      <dttm>              <dbl>  <dbl>  <dbl>
#>  1   0.948 0.0518   0     0           1         0 0          2012-01-01 09:09:49     0      0      0
#>  2   0.913 0.0866   0     0           1         0 0          2012-01-02 03:00:18     0      0      0
#>  3   0.972 0.0278   0     0           1         0 0          2012-01-02 05:42:16     0      0      5
#>  4   0.895 0.105    0     0           1         0 0          2012-01-02 01:58:14     0      0      0
#>  5   0.993 0.00722  0     0           1         0 0          2012-01-02 07:07:22     0      0     21
#>  6   1.000 0.000420 0     1           1         2 0          2012-01-02 18:09:45     0      0      0
#>  7   0.999 0.000671 0     0           1         1 0          2012-01-02 15:12:51     0      0      0
#>  8   1.000 0.000273 0     1           1         2 0          2012-01-02 21:24:21     0      0      0
#>  9   0.971 0.0288   0     0           1         0 0          2012-01-03 09:34:50     0      0     11
#> 10   0.978 0.0219   0     0           1         0 0          2012-01-03 13:33:28     0      0     18
#> # ℹ 775 more rows
#> # ℹ 12 more variables: winner <fct>, inherit <dbl>, viagra <dbl>, password <dbl>, num_char <dbl>,
#> #   line_breaks <int>, format <fct>, re_subj <fct>, exclaim_subj <dbl>, urgent_subj <fct>,
#> #   exclaim_mess <dbl>, number <fct>
```

### Evaluate the performance


``` r
email_pred %>%
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()
```

<img src="1101_cv_files/figure-html/roc-1.png" alt="" width="672" />


``` r
email_pred %>%
  roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
#> # A tibble: 1 × 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary         0.867
```

## Making decisions

### Cutoff probability: 0.5

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.5**.


``` r
cutoff_prob <- 0.5
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```



|                        | Email is not spam| Email is spam|
|:-----------------------|-----------------:|-------------:|
|Email labelled not spam |               698|            68|
|Email labelled spam     |                 9|            10|



### Cutoff probability: 0.25

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.25**.


``` r
cutoff_prob <- 0.25
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```



|                        | Email is not spam| Email is spam|
|:-----------------------|-----------------:|-------------:|
|Email labelled not spam |               671|            41|
|Email labelled spam     |                36|            37|



### Cutoff probability: 0.75

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.75**.


``` r
cutoff_prob <- 0.75
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```



|                        | Email is not spam| Email is spam|
|:-----------------------|-----------------:|-------------:|
|Email labelled not spam |               703|            73|
|Email labelled spam     |                 4|             5|





<!--DS4P Links-->
[course_web]: https://datascience4psych.github.io/DataScience4Psych
[course_git]: https://github.com/DataScience4Psych/DataScience4Psych
[course_repo]: https://github.com/DataScience4Psych
[course_slides]: https://github.com/DataScience4Psych/slides
[course_syllabus]: https://smasongarrison.github.io/syllabi/ 
<!-- https://smasongarrison.github.io/syllabi/data-science.html -->
[syllabi]: https://smasongarrison.github.io/syllabi
[pl_00]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYaEAnJX20Ryy4OSie375rVY
[pl_01]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYao_7t5ycK4KDXNKaY-ECup
[pl_02]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYZmr_T3PnuxjVIlj0C0kUNI
[pl_03]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYaHmjzdRvfg0yhOIYQnfjwE
[pl_04]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYYWFcel6_vp8__RUKLxhX4y
[pl_05]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYYMIguiV1F8RagMYibTY4iW
[pl_06]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYYV_KDod3Mk9-RmtFXii9Dv
[pl_07]: https://www.youtube.com/watch?list=PLKrrdtYgOUYZxvEvQ8-PcWrOY_dwY_ETI
[pl_08]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYZgOzYB_dmauw55M7jXvsdo
[pl_09]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYbaiTmldRY2ddsLrHp3z6yO
[pl_10]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYbPw5iYzYEzoOKa7mJKNIhq
[pl_11]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYZ-u6LzBbanrNFoeLHKaLL6
[pl_12]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYbwRS-9Htmb80_t1NG-021e
[pl_13]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYbWGmSnbLIYwdLOnGm6une6
[pl_14]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYbWGmSnbLIYwdLOnGm6une6
[pl_15]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYa5MoYrV8EsWQ5jIr5ZYMpM
[pl_all]: https://www.youtube.com/playlist?list=PLKrrdtYgOUYZomNqf-1dtCDW94ySdLv-9


<!--AE Links-->
[ae01a_unvotes]: https://github.com/DataScience4Psych/ae01a_unvotes
[ae01b_covid]: https://github.com/DataScience4Psych/ae01b_covid
[ae02_bechdel]: https://github.com/DataScience4Psych/ae-02-bechdel-rmarkdown
[ae03_starwars]: https://github.com/DataScience4Psych/ae-03-starwars-dataviz
[ae08_imdb]: https://github.com/DataScience4Psych/ae-08-imdb-webscraping

<!-- Lab Links-->

[lab01_hello]: https://github.com/DataScience4Psych/lab-01-hello-r
[lab02]: https://github.com/DataScience4Psych/lab-02-plastic-waste
[lab03]: https://github.com/DataScience4Psych/lab-03-nobel-laureates
[lab04]: https://github.com/DataScience4Psych/lab-04-viz-sp-data
[lab05]: https://github.com/DataScience4Psych/lab-05-wrangle-sp-data
[lab06]: https://github.com/DataScience4Psych/lab_06_sad_plots
[lab07]: https://github.com/DataScience4Psych/lab_07_betterviz
[lab08]: https://github.com/DataScience4Psych/lab-08-uoe-art
[lab09]: https://github.com/DataScience4Psych/lab-09-ethics-algorithmic-bias
[lab10]: https://github.com/DataScience4Psych/lab-10-slr-course-evals
[lab11]: https://github.com/DataScience4Psych/lab-11-mlr-course-evals
[lab12]: https://github.com/DataScience4Psych/lab-12-inference-smoking
[lab13]: https://github.com/DataScience4Psych/lab-13-simulating-mars

<!--Slides-->
[d01_welcome]: https://datascience4psych.github.io/slides/d01_welcome/d01_welcome.html
[d02_toolkit]: https://datascience4psych.github.io/slides/d02_toolkit/d02_toolkit.html
[d03_dataviz]: https://datascience4psych.github.io/slides/d03_dataviz/d03_dataviz.html
[d04_ggplot2]: https://datascience4psych.github.io/slides/d04_ggplot2/d04_ggplot2.html
[d05_viznum]: https://datascience4psych.github.io/slides/d05_viznum/d05_viznum.html
[d06_vizcat]: https://datascience4psych.github.io/slides/d06_vizcat/d06_vizcat.html
[d07_tidy]: https://datascience4psych.github.io/slides/d07_tidy/d07_tidy.html
[d08_grammar]: https://datascience4psych.github.io/slides/d08_grammar/d08_grammar.html
[d09_wrangle]: https://datascience4psych.github.io/slides/d09_wrangle/d09_wrangle.html
[d10_dfs]: https://datascience4psych.github.io/slides/d10_dfs/d10_dfs.html
[d11_types]: https://datascience4psych.github.io/slides/d11_types/d11_types.html
[d12_import]: https://datascience4psych.github.io/slides/d12_import/d12_import.html
[d13_goodviz]: https://datascience4psych.github.io/slides/d13_goodviz/d13_goodviz.html
[d13b_moreggplot]: https://datascience4psych.github.io/slides/d13_goodviz/d13b_moreggplot.html
[d14_confound]: https://datascience4psych.github.io/slides/d14_confound/d14_confound.html
[d15_goodtalk]: https://datascience4psych.github.io/slides/d15_goodtalk/d15_goodtalk.html
[d16_webscraping]: https://datascience4psych.github.io/slides/d16_webscraping/d16_webscraping.html
[d17_functions]: https://datascience4psych.github.io/slides/d17_functions/d17_functions.html
[d18_ethics]: https://datascience4psych.github.io/slides/d18_ethics/d18_ethics.html
[d19_bias]: https://datascience4psych.github.io/slides/d19_bias/d19_bias.html
[d20_language]: https://datascience4psych.github.io/slides/d20_language/d20_language.html
[d21_fitting]: https://datascience4psych.github.io/slides/d21_fitting/d21_fitting.html
[d22_nonlinear]: https://datascience4psych.github.io/slides/d22_nonlinear/d22_nonlinear.html
[d23_multiple]: https://datascience4psych.github.io/slides/d23_multiple/d23_multiple.html
[d24_overfitting]: https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html
[d25_crossvalidation]: https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html
[d26_quantify]: https://datascience4psych.github.io/slides/d26_quantify/d26_quantify.html
[d27_bootstrap]: https://datascience4psych.github.io/slides/d27_bootstrap/d27_bootstrap.html
[d28_interactive]: https://datascience4psych.github.io/slides/d28_interactive/d28_interactive.html
[d29_machine]: https://datascience4psych.github.io/slides/d29_machinelearning/d29_machine.html
[d30_simulations]: https://datascience4psych.github.io/slides/d30_simulations/d30_simulations.html
[d31_llmintro]: https://datascience4psych.github.io/slides/d31_llmintro/d31_llmintro.html
[d32_llmapplications]: https://datascience4psych.github.io/slides/d32_llmapplications/d32_llmapplications.html

<!--externals-->

[stat545]: https://stat545.com
[r4ds]: https://r4ds.had.co.nz
[cran]: https://cloud.r-project.org

# (PART) Module 11 {-}

```{r cv-setup, include = FALSE}
source("common.R")
ds4p_funyoutube <- read.csv("admin/csv/ds4p_funyoutube.csv", sep = "")
ds4p_urls <- read.csv("./admin/csv/ds4p_urls.csv")
```

# Welcome to Overfitting and Cross-Validation

This module is designed to introduce you to cross-validation and overfitting. Please watch the videos and work your way through the notes. **The videos start on the next page.** You can find the video playlist for this module [here][pl_11]. Most of the slides used to make the videos in this module can be found in the [slides repo][course_slides].

```{r cv-include-tweet, echo=FALSE, eval=FALSE}
try_include_tweet("https://twitter.com/Chops310/status/1382481152489979913")
```

## Module Materials

* Videos and Slides from Lectures
  * [Overfitting](https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html)
  * [Cross-validation](https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html)
* Suggested Readings
  * All subchapters of this module
  * Articles
    * [de Rooij, M., & Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248-263.](https://journals.sagepub.com/doi/full/10.1177/2515245919898466)
    * [MacCallum, R. C., Zhang, S., Preacher, K. J., & Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40.](http://www.quantpsy.org/pubs/maccallum_zhang_preacher_rucker_2002.pdf)
  * R4DS
    * [Many Models](https://r4ds.had.co.nz/many-models.html)

# Lecture: Overfitting

You can follow along with the slides [here](https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html) if you would like to open them full-screen. The embedded code for feature enginering can be found [here](#featurenotes)

```{r cv-include-slides-intro, echo=FALSE}
"https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#1" %>%
  include_url(height = "400px")
```

## Prediction

```{r cv-include-video-prediction, echo=FALSE,eval = knitr::is_latex_output()}
embed_youtube_alt("U70OmbO-DP4")
```
```{r cv-include-video-prediction-html, echo=FALSE,eval = knitr::is_html_output()}
"https://www.youtube.com/watch?v=U70OmbO-DP4" %>%
  embed_url() %>%
  use_align("center")
```

```{r cv-include-slides-prediction, echo=FALSE}
"https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#2" %>%
  include_url(height = "400px")
```

## Workflow

```{r cv-include-video-workflow, echo=FALSE,eval = knitr::is_latex_output()}
embed_youtube_alt("R4h9u-sQHwI")
```
```{r cv-include-video-workflow-html, echo=FALSE,eval = knitr::is_html_output()}
"https://www.youtube.com/watch?v=R4h9u-sQHwI" %>%
  embed_url() %>%
  use_align("center")
```

```{r cv-include-slides-workflow, echo=FALSE}
"https://datascience4psych.github.io/slides/d24_overfitting/d24_overfitting.html#17" %>%
  include_url(height = "400px")
```

# Lecture: Cross-Validation

You can follow along with the slides [here](https://datascience4psych.github.io/slides/d25_crossvalidation/d25_crossvalidation.html) if you would like to open them full-screen.

```{r cv-include-video-crossval, echo=FALSE}
# CV
"https://www.youtube.com/watch?v=KQ9f8s7RB5g" %>%
  embed_url() %>%
  use_align("center")
```

```{r cv-include-slides-crossval, echo=FALSE}
slide_url(ds4p_urls, "d25_crossvalidation") %>%
  include_url(height = "400px")
```

## V-Fold

```{r cv-include-video-vfold, echo=FALSE}
# v fold
"https://www.youtube.com/watch?v=quEVKV-Tk0Y" %>%
  embed_url() %>%
  use_align("center")
```

```{r cv-include-slides-vfold, echo=FALSE}
slide_url(ds4p_urls, "d25_crossvalidation", "#35") %>%
  include_url(height = "400px")
```

# Notes on Feature Engineering {#featurenotes}

```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(openintro)
library(lubridate)
library(knitr)
set.seed(1234)
options(
  warnPartialMatchArgs = FALSE,
  warnPartialMatchAttr = FALSE,
  warnPartialMatchDollar = FALSE,
  width = 100
)

data(email)
# email$spam_num=email$spam
email$spam <- as.factor(email$spam)

plot_colors <- c("#E48957", "#CA235F")
```

## Feature engineering

* We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity

* Variables that go into the model and how they are represented are just as critical to success of the model

* **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)

### Same training and testing sets as before

```{r cv-code-split-data, }
# Fix random numbers by setting the seed
# Enables analysis to be reproducible when random numbers are used
set.seed(1066)

# Put 80% of the data into the training set
email_split <- initial_split(email, prop = 0.80)

# Create data frames for the two sets:
train_data <- training(email_split)
test_data <- testing(email_split)
```

### A simple approach: `mutate()`

```{r cv-code-mutate-features, }
train_data %>%
  mutate(
    date = date(time),
    dow = wday(time),
    month = month(time)
  ) %>%
  select(time, date, dow, month) %>%
  sample_n(size = 5) # shuffle to show a variety
```

## Modeling workflow, revisited

* Create a **recipe** for feature engineering steps to be applied to the training data

* Fit the model to the training data after these steps have been applied

* Using the model estimates from the training data, predict outcomes for the test data

* Evaluate the performance of the model on the test data

## Building recipes

### Initiate a recipe

```{r initiate-recipe, results="hide"}
email_rec <- recipe(
  spam ~ ., # formula
  data = train_data # data to use for cataloguing names and types of variables
)

summary(email_rec)
```

```{r cv-print-recipe-summary, echo=FALSE}
summary(email_rec) %>% print(n = 21)
```

### Remove certain variables

```{r cv-code-remove-vars, }
email_rec <- email_rec %>%
  step_rm(from, sent_email)
```

```{r cv-print-recipe-remove, echo=FALSE}
email_rec
```

### Feature engineer date

```{r cv-code-engineer-date, }
email_rec <- email_rec %>%
  step_date(time, features = c("dow", "month")) %>%
  step_rm(time)
```

```{r cv-print-recipe-date, echo=FALSE}
email_rec
```

### Discretize numeric variables

Proceed with major caution! And please be sure to read [MacCallum, R. C., Zhang, S., Preacher, K. J., & Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40.](http://www.quantpsy.org/pubs/maccallum_zhang_preacher_rucker_2002.pdf) and play around with the demo data from Kris's website: <http://www.quantpsy.org/mzpr.htm>

```{r cv-code-discretize, }
email_rec <- email_rec %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20))
```

```{r cv-print-recipe-cut, echo=FALSE}
email_rec
```

### Create dummy variables

```{r cv-code-create-dummies, }
email_rec <- email_rec %>%
  step_dummy(all_nominal(), -all_outcomes())
```

```{r cv-print-recipe-dummy, echo=FALSE}
email_rec
```

### Remove zero variance variables

Variables that contain only a single value

```{r cv-code-remove-zv, }
email_rec <- email_rec %>%
  step_zv(all_predictors())
```

```{r cv-print-recipe-zv, echo=FALSE}
email_rec
```

### All in one place

```{r cv-code-recipe-complete, }
email_rec <- recipe(spam ~ ., data = email) %>%
  step_rm(from, sent_email) %>%
  step_date(time, features = c("dow", "month")) %>%
  step_rm(time) %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())
```

## Building workflows

### Define model

```{r cv-code-define-model, }
email_mod <- logistic_reg() %>%
  set_engine("glm")

email_mod
```

### Define workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.

```{r cv-code-define-workflow, }
email_wflow <- workflow() %>%
  add_model(email_mod) %>%
  add_recipe(email_rec)
```

```{r cv-print-workflow, echo=FALSE}
email_wflow
```

### Fit model to training data

```{r cv-code-fit-model, }
email_fit <- email_wflow %>%
  fit(data = train_data)
```

```{r cv-code-tidy-fit, }
tidy(email_fit) %>% print(n = 31)
```

### Make predictions for test data

```{r cv-code-make-predictions, }
email_pred <- predict(email_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

email_pred
```

### Evaluate the performance

```{r roc}
email_pred %>%
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()
```

```{r cv-code-calc-roc-auc, }
email_pred %>%
  roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```

## Making decisions

### Cutoff probability: 0.5

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.5**.

```{r confusion-matrix-medium-threshold}
cutoff_prob <- 0.5
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```

### Cutoff probability: 0.25

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.25**.

```{r confusion-matrix-low-threshold}
cutoff_prob <- 0.25
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```

### Cutoff probability: 0.75

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.75**.

```{r confusion-matrix-high-threshold}
cutoff_prob <- 0.75
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
  ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable(col.names = c("", "Email is not spam", "Email is spam"))
```

```{r courselinks, child="admin/md/courselinks.md"}
```

Meme-tastic data science activity: OpenAI API in the Tidyverse

Overview
- Goal: Learn how to call OpenAI API from R, handle and parse responses, and analyze results with Tidyverse.
- Theme: Use meme-inspired prompts and styles to explore prompt engineering, model behavior, and result analysis.
- Deliverables: a tidy dataframe of prompts and responses, simple visualizations, and a short interpretive write-up.
- Prereqs: Basic R/RStudio, tidyverse familiarity, an OpenAI API key.

Learning objectives
- Make RESTful API calls to OpenAI from R (Chat Completions via the v1/chat/completions endpoint).
- Build, send, and parse structured payloads; handle JSON responses and usage data (token counts).
- Apply tidyverse workflows to organize prompts, responses, and metadata; use purrr for mapping calls.
- Explore prompt engineering concepts: system messages, role messages, temperature, and max_tokens.
- Analyze results with simple metrics and visuals (response length, token usage, and meme-style qualitative signals).
- Reflect on cost, latency, bias, and reported usage to practice responsible AI integration.

Prerequisites and setup
- R installed ( >= 4.1 recommended).
- Packages: tidyverse, httr, jsonlite, purrr, stringr, readr, lubridate (optional).
- OpenAI API key: stored securely as an environment variable OPENAI_API_KEY.
  - In R: Sys.setenv(OPENAI_API_KEY = "your-key-here") or better, set it in your .Renviron.
- Optional: a local project directory to keep results reproducible.

What you’ll build (high level)
- A small dataset of prompts designed for meme-friendly prompts.
- A function to call OpenAI Chat API for each prompt and capture both the response and usage data.
- A tidy workflow to assemble results, compute simple metrics, and visualize outcomes.
- A short, meme-flavored write-up interpreting the results and observations.

Data and prompts (you will edit or extend)
Create a small tibble of prompts to send to the API. We’ll include a system message to set the meme-friendly style, and a few user prompts to generate varied outputs.

Example prompts (in R code)

# Install and load packages (only run if not already installed)
# install.packages(c("tidyverse", "httr", "jsonlite"))

library(tidyverse)

# System prompt to set meme tone (fun but professional)
system_message <- "You are a data science assistant who answers in a concise, meme-friendly style. Use clear language, but sprinkle light meme humor. Keep content accurate and helpful for a graduate-level data science audience."

# Prompt dataset
prompts <- tibble(
  id = 1:5,
  topic = c("data cleaning", "model evaluation", "prompt engineering", "data visualization critique", "cost-aware API usage"),
  user_prompt = c(
    "Provide a concise, bullet-point checklist for a reproducible data cleaning pipeline in tidyverse for a messy CSV.",
    "Explain how you would evaluate a classifier's performance with proper rigor, including ROC/PR curves and cross-validation considerations.",
    "Give me a meme-caption-style explanation of how prompt engineering can influence outputs, with a Drizzy-style caption.",
    "Critique a poorly designed ggplot that uses colorblind-inappropriate palettes and suggest fixes, with a meme-friendly punchline.",
    "Outline strategies to minimize API cost and latency when using OpenAI for a semester-long project, with practical tips."
  ),
  temperature = c(0.2, 0.4, 0.7, 0.6, 0.3),
  max_tokens = c(512, 768, 600, 600, 500)
)

# The payload you will send for each row
# (We'll build this in a function below)
# End of data setup

(Notes:
- The dataset has 5 prompts; you can add more.
- Temperature and max_tokens vary to illustrate how outputs differ with settings.)

API call scaffold (R code)
- We’ll use httr to POST to the OpenAI Chat API. The payload is a list with model, messages, temperature, max_tokens.
- We’ll collect: the assistant content (response text) and usage.total_tokens.

Important: Do not commit your API key in code. Use environment variables.

R function to call the API (Chat Completions)
library(httr)
library(jsonlite)

# Helper: call OpenAI Chat API for a single prompt
call_openai_chat <- function(prompt_text,
                            system_message = NULL,
                            model = "gpt-4-turbo",
                            temperature = 0.4,
                            max_tokens = 512,
                            api_key = Sys.getenv("OPENAI_API_KEY")) {

  if (nzchar(api_key) == FALSE) {
    stop("OPENAI_API_KEY environment variable is not set.")
  }

  messages <- list()
  if (!is.null(system_message) && nzchar(system_message)) {
    messages <- append(messages, list(list(role = "system", content = system_message)))
  }
  messages <- append(messages, list(list(role = "user", content = prompt_text)))

  payload <- list(
    model = model,
    messages = messages,
    temperature = temperature,
    max_tokens = max_tokens
  )

  resp <- POST(
    url = "https://api.openai.com/v1/chat/completions",
    add_headers(Authorization = paste("Bearer", api_key)),
    content_type_json(),
    body = toJSON(payload, auto_unbox = TRUE)
  )

  if (http_error(resp)) {
    status <- status_code(resp)
    stop("OpenAI API error: ", status, " ", content(resp, as = "text", encoding = "UTF-8"))
  }

  out <- content(resp, as = "text", encoding = "UTF-8") %>% fromJSON(flatten = TRUE)
  # Some responses structure: choices[ [message][content], usage]
  content_text <- out$choices[[1]]$message$content
  tokens <- as.integer(out$usage$total_tokens)

  list(text = content_text, tokens = tokens, raw = out)
}

How to run the calls for all prompts (tidyverse approach)
library(dplyr)
library(purrr)
library(tidyr)

results <- prompts %>%
  mutate(
    prompt_text = pmap_chr(list(topic, user_prompt),
                           ~ paste0("Topic: ", ..1, "\nPrompt: ", ..2))
  ) %>%
  mutate(
    api_out = map2(
      prompt_text,
      temperature,
      ~ call_openai_chat(
        prompt_text = .x,
        system_message = system_message,
        temperature = .y,
        max_tokens = max_tokens
      )
  ),
  response = map(api_out, "text"),
  tokens = map_int(api_out, "tokens")
)

# Flatten to a tidy table
tidy_results <- results %>%
  select(id, topic, user_prompt, response, tokens) %>%
  unnest_wider(response, names_sep = "_") %>%
  rename(ai_response = response_text)

tidy_results
Important notes about the API calls
- Rate limits: OpenAI may rate-limit. Respect per-minute quotas; insert small pauses if looping over many prompts (e.g., Sys.sleep(1)).
- Costs: Each call consumes tokens. Keep max_tokens in mind and consider using smaller models or lower max_tokens for teaching.
- Safety: Do not expose your API key. Do not print keys. Use environment variables.

Analyzing the results with Tidyverse
Goal: extract and visualize key aspects of the outputs.

1) Basic stats: length and tokens
library(ggplot2)

stats <- tidy_results %>%
  mutate(
    response_len = str_length(ai_response),
    prompt_len = str_length(user_prompt)
  )

# Visualize response length by prompt
ggplot(stats, aes(x = factor(id), y = response_len, fill = factor(id))) +
  geom_col() +
  labs(x = "Prompt ID", y = "Response length (characters)", title = "OpenAI responses - length by prompt") +
  theme_minimal()

# Visualize token usage
ggplot(stats, aes(x = factor(id), y = tokens)) +
  geom_col(fill = "steelblue") +
  labs(x = "Prompt ID", y = "Total tokens used", title = "Token usage per API call") +
  theme_minimal()

2) Quick qualitative checks: meme style scoring
# Simple memeiness score: count meme-related tokens and exclamations
meme_words <- c("wow","amaze","lol","lolwut","drake","meme","that escalated","shibe","wow","pog","grill","dang","noice","wow")

score_memeiness <- function(text) {
  t <- tolower(text)
  sum(str_count(t, str_c(meme_words, collapse = "|")))
}
tidy_results <- tidy_results %>%
  mutate(
    meme_score = map_int(ai_response, score_memeiness),
    response_sentences = str_split(ai_response, "(?<=[.!?])\\s+")
  )

tidy_results %>% select(id, meme_score)

3) Readability (simple heuristic)
readability <- tidy_results %>%
  mutate(
    words = str_count(ai_response, "\\w+"),
    sentences = map_int(response_sentences, length),
    avg_sentence_len = ifelse(sentences > 0, words / sentences, NA_real_)
  ) %>%
  select(id, avg_sentence_len)

readability

4) Simple visual summary
ggplot(tidy_results, aes(x = factor(id), y = meme_score)) +
  geom_col(fill = "tomato") +
  labs(x = "Prompt ID", y = "Meme score (count)", title = "Meme-style signal in responses") +
  theme_minimal()

5) Optional: Combine prompts and responses into a single summary
summary_tbl <- tidy_results %>%
  select(id, topic, user_prompt, ai_response, tokens, meme_score, avg_sentence_len)

summary_tbl

Interpreting the results
- Compare how different temperature settings affected the outputs. Do higher temps produce more varied, meme-rich outputs or more creative but less precise language?
- Look at token usage vs. response length. Do longer responses correlate with more meme content or more detail?
- Assess readability and clarity. Are some outputs too verbose or too terse for graduate-level comprehension?
- Reflect on biases and safety: Do any prompts elicit problematic or biased content? How might you constrain or steer outputs using system and user messages?

Meme-tastic prompts you can try (variations)
- Different memes in the prompt: “Explain this dataset cleaning workflow in the style of a Drake meme caption.”
- Style requests: “Provide a six-bullet summary with a humorous, meme-flavored tone but keep it technically accurate.”
- Humor moderation: “Rate the output on a scale of 1-5 for memeiness, while preserving technical correctness.”

Extensions and variations (for optional sections)
- Parallelize calls: Use furrr with plan(multisession) to run API calls in parallel if you have multiple prompts and a higher quota (be mindful of rate limits and keys sharing).
- Use the OpenAI R package (optional): If you prefer a wrapper, you can adapt to the openai package’s chat or completions functions.
- Add more prompts: Bring in prompts that test domain knowledge (e.g., data ethics, reproducibility, statistical methods) and compare outputs.
- Save and version outputs: Write results to a CSV or RDS, and track the date, model, temperature, and max_tokens as metadata for reproducibility.

Cleanup and reproducibility tips
- Wrap the pipeline in an R Script or R Markdown document so others can reproduce results with the same dataset and prompts.
- Store settings in an .Renviron or a config file to avoid hard-coding values.
- Use set.seed where applicable for any random sampling or shuffles in the workflow.

Deliverables you should produce
- A tidy dataframe with:
  - id, topic, user_prompt, ai_response, tokens, meme_score, avg_sentence_len
- A couple of simple plots:
  - Response length by prompt
  - Token usage by prompt
  - Meme score by prompt
- A short narrative (1–2 pages) interpreting the results, discussing:
  - Variation with temperature
  - Readability and usefulness for graduate-level tasks
  - Practical considerations for using the API in research workflows (costs, latency, ethics)

Tips for a smooth in-class or remote session
- Provide a starter R project with an .Rproj file and a simple README describing environment setup and API key handling.
- Have students run a dry-run with a tiny subset (e.g., 2 prompts) to verify API access before scaling up.
- Emphasize reproducibility: pin package versions in a small packrat/renv environment or a Dockerfile if your course supports it.
- Encourage students to reflect on prompt design: how the system message shapes outputs and how varying temperature changes creativity vs. precision.

Safety and best practices
- Never share your OpenAI API key in code or slides. Use environment variables.
- Be mindful of OpenAI usage costs; set conservative max_tokens and temperatures, especially in class.
- Ensure prompts and outputs comply with your institution’s policy on data privacy and AI usage.

If you want, I can tailor the prompts to a specific graduate course topic (e.g., Bayesian data analysis, time-series forecasting, or ethics in ML), or provide a ready-to-run R script you can paste into a .R file and execute in class.

[["index.html", "Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R Welcome to PSY 703 Mason Notes", " Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R S. Mason Garrison 2025-02-17 Welcome to PSY 703 Welcome to class! This website is designed to accompany Mason Garrison’s Data Science for Psychologists (DS4P). DS4P is a graduate-level quantitative methods course at Wake Forest University. This class assumes zero knowledge of programming, computer science, linear algebra, probability, or really anything fancy. I encourage anyone who is quant-curious to work their way through these course notes. The course notes include lectures, worked examples, readings, activities, and labs. You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. All the embedded lecture videos can be found on a youtube playlist. Mason Notes This website is constantly changing. I am actively developing this course, and is approximately 94% done. I have made this process explicitly transparent because I want you to see how you can use R to produce some pretty neat things. Indeed, I’ve included the source code for this website in the class repo. I encourage you to contribute to the course code. If you catch typos or errors, please issue a pull request with the fixes. If you find cool or useful resources, please add them. By the end of the semester, I would love for everyone to have contributed to the course materials. How to use these notes To navigate these course notes, use the table of contents on the left side of the screen. You can open or close the table of contents using the hamburger icon (horizontal bars) at the top of the document. Additionally, there are other icons at the top of the document for searching within the text, and for adjusting the size, font, or color scheme of the page. The document will be updated (unpredictably) throughout the semester. Each module corresponds to a week’s worth of material. Most modules are dedicated to improving a specific skill or at the very least dedicated to a specific theme. Within each module, you will find a range of resources including embedded videos, slides, activities, labs, and tutorials. The skills developed in each module are designed to build upon those you’ve learned in previous modules. Eventually, this class will have more modules available than weeks in a semester, so that you – the reader – can choose your own adventure (err… module) you’d like to start with. Although these notes have some textbook-like features, they are neither comprehensive nor completely original. The main purpose is to give you all a set of common materials to draw upon during the course. In class, we will sometimes do things outside the notes. The idea here is that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. Status of course In terms of timing, I will have each module completed by the start of the week. Given that the class meets on Tuesday and Thursday, the start of the “week” will be Monday at 12 p.m. EST.. I may get ahead of this deadline. You can see the current status of the course below. Although you are welcome to work ahead, be aware that I will be making changes to modules that haven’t officially started yet. In addition, I may add optional materials to previous modules that might be helpful. The table below shows the current status of the course, listing proportions of specific components by module. Overall completion: 93.606% "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples’ materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major Attributions Jenny Bryan’s (jennybryan.org) STAT 545 and Happy Git with R; Joe Rodgers’s PSY 8751 Exploratory and Graphical Data Analysis Course Mine Çetinkaya-Rundel’s Data Science in a Box. Additional Attributions Academic.io’s AWESOME DATA SCIENCE Julia Fukuyama’s EXPLORATORY DATA ANALYSIS Benjamin Soltoff’s Computing for the Social Sciences Grant McDermott’s course materials on environmental economics and data science Zachary M. Smith’s crash course in rmarkdown Thomas E. Love Karl Broman EMILY SUZANNE CLARK’s Rubric for Unessays Ariel Muldoon’s tutorial on simulations "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This information is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["sitemap.html", "Sitemap", " Sitemap #plfmiwxhwd table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #plfmiwxhwd thead, #plfmiwxhwd tbody, #plfmiwxhwd tfoot, #plfmiwxhwd tr, #plfmiwxhwd td, #plfmiwxhwd th { border-style: none; } #plfmiwxhwd p { margin: 0; padding: 0; } #plfmiwxhwd .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #plfmiwxhwd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #plfmiwxhwd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #plfmiwxhwd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #plfmiwxhwd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #plfmiwxhwd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #plfmiwxhwd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #plfmiwxhwd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #plfmiwxhwd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #plfmiwxhwd .gt_column_spanner_outer:first-child { padding-left: 0; } #plfmiwxhwd .gt_column_spanner_outer:last-child { padding-right: 0; } #plfmiwxhwd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #plfmiwxhwd .gt_spanner_row { border-bottom-style: hidden; } #plfmiwxhwd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #plfmiwxhwd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #plfmiwxhwd .gt_from_md > :first-child { margin-top: 0; } #plfmiwxhwd .gt_from_md > :last-child { margin-bottom: 0; } #plfmiwxhwd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #plfmiwxhwd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #plfmiwxhwd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #plfmiwxhwd .gt_row_group_first td { border-top-width: 2px; } #plfmiwxhwd .gt_row_group_first th { border-top-width: 2px; } #plfmiwxhwd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #plfmiwxhwd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #plfmiwxhwd .gt_first_summary_row.thick { border-top-width: 2px; } #plfmiwxhwd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #plfmiwxhwd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #plfmiwxhwd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #plfmiwxhwd .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #plfmiwxhwd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #plfmiwxhwd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #plfmiwxhwd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #plfmiwxhwd .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #plfmiwxhwd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #plfmiwxhwd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #plfmiwxhwd .gt_left { text-align: left; } #plfmiwxhwd .gt_center { text-align: center; } #plfmiwxhwd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #plfmiwxhwd .gt_font_normal { font-weight: normal; } #plfmiwxhwd .gt_font_bold { font-weight: bold; } #plfmiwxhwd .gt_font_italic { font-style: italic; } #plfmiwxhwd .gt_super { font-size: 65%; } #plfmiwxhwd .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #plfmiwxhwd .gt_asterisk { font-size: 100%; vertical-align: 0; } #plfmiwxhwd .gt_indent_1 { text-indent: 5px; } #plfmiwxhwd .gt_indent_2 { text-indent: 10px; } #plfmiwxhwd .gt_indent_3 { text-indent: 15px; } #plfmiwxhwd .gt_indent_4 { text-indent: 20px; } #plfmiwxhwd .gt_indent_5 { text-indent: 25px; } #plfmiwxhwd .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #plfmiwxhwd div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } title link 404 URL Activity Oh My Git Version Control Challenge URL Ae01 URL Analyzing Each Sample By Summarizing Its Main Attributes URL Api Wrappers URL Attribution URL Automation URL Basic Syntax URL Basiccare URL Bechdal URL Bias URL Bootstrapping URL Child Documents URL Colophon URL Communicating Data Science Results Effectively URL Creating Datasets With Quantiative And Categorical Variables URL Creating Datasets With Quantitative And Categorical Variables 1 URL Cross Validation URL Data Science And Ethics URL Data Types And Recoding URL Data Usually Finds Me URL Data Simulations URL Deeper Diving Into Ggplot2 URL Define Mean And Covariance Matrix URL Designing Effective Visualizations URL Diy Web Data URL Dont Miss Module 00 URL Dont Miss The Last Module URL Dplyr Intro URL Efficient Workflow With R Projects And R Markdown URL Enhancing The Function Towards The Perfectly Formed Rear View Mirror URL Exploratory Data Analysis URL Featurenotes URL Fitting And Interpreting Models URL Functions 1 URL Functions Part1 URL Functions Practicum URL Generate Character Vectors With Rep URL Getting Started With Simulating Data In R URL Good Resources URL Grammar Of Data Wrangling URL Guidance URL Hands On With Openais Api URL Handson URL Import Export URL Importing Data URL Index URL Lab00 URL Lab01 URL Lab01aloha URL Lab01execercise URL Lab02 URL Lab03 URL Lab04 URL Lab05 URL Lab06 URL Lab07 URL Lab08 URL Lab09 URL Lab10 URL Lab11 URL Lab12 URL Labmoney URL Language Of Models URL Lecture Applications Of Large Language Models In Data Science 1 URL Lecture Applications Of Large Language Models In Data Science URL Lecture Cross Validation URL Lecture Ethical Considerations Of Llms URL Lecture Getting Started With Simulating Data In R URL Lecture Grammar Of Data Wrangling URL Lecture Meet Our Toolbox URL Lecture Overfitting URL Lecture Scraping The Web URL Lecture Thoughtful Workflow URL Lecture Tidy Data URL Lecture What Are Large Language Models URL Lesson 4 Yaml Headers URL Lesson 5 Code Chunks And Inline Code URL License URL Media Without A Home Yet URL Meet Our Toolbox URL Merges URL Mod06 URL Modeling Non Linear Relationships URL Modeling With Multiple Predictors URL Natural Language Processing URL Neural Networks URL Notes On Hypothesis Testing URL Notes On Logistic Regression URL Notes On Simulations URL Odd Data Transformations And Tukeys Ladder Of Powers URL Odd Design Choices In Data Visualization URL Odd Notes On Cross Validation URL Overfitting URL Parameterized Reports URL Plots Behaving Badly Lessons In Data Misrepresentation URL Plots Behaving Badly URL Practical Advice From The Data Professor URL Public Health URL Quantifying Uncertainty URL Quickhappygit URL R Commands URL R Basics URL References URL Repeatedly Simulate Data With Replicate URL Rmd Creation URL Rshiny Overview URL Rshiny URL Save Figs URL Scientific Studies And Confounding URL Scraping The Web URL Secrets URL Shiny Overview URL Shiny Resources URL Shorthappygit URL Simulate 5 Different Samples Of 20 Colonists Each From Our Original Pool URL Sitemap URL Society And Ai URL Special Topics Machine Learn URL Special Topics Reproducible Reports URL Star Wars Activity URL Test On Unexpected Inputs URL Thoughtful Workflow URL Thoughts From Hadley Wickham On Tidyverse URL Tidy Data URL Topic 1 URL Visualization Examples URL Visualizing Categorical Data URL Visualizing Data With Ggplot2 URL Visualizing Numerical Data URL Welcome To Base R And Simulating Data URL Welcome To Data And Ethics URL Welcome To Data And Visualization URL Welcome To Data Diving With Types URL Welcome To Data Science URL Welcome To Functions And Automation URL Welcome To Large Language Models In Data Science URL Welcome To Large Language Models URL Welcome To Modeling The Tidy Way URL Welcome To Overfitting And Cross Validation URL Welcome To Quantifying Uncertainty URL Welcome To Reproducible Reports URL Welcome To Simulating Data URL Welcome To The Tidyverse URL Welcome To Tips For Effective Data Visualization URL Welcome To Web Scraping URL What Is Data Science URL Whats The Next Step URL Working With Multiple Data Frames URL D00 Slide URL D00 Aps URL D00 Teaser URL D00 Closers URL D00 Code URL D00 Content URL D00 Interactive URL D00 Openers URL D00 Slide URL D00 Visuals URL D01.1 Ai URL D01 Welcome URL D02 Toolkit URL D03 Dataviz URL D04 Ggplot2 URL D05 Viznum URL D06 Vizcat URL D07 Tidy URL D08 Grammar URL D09 Wrangle URL D10 Dfs URL D11 Types URL D12 Import URL D13 Goodviz URL D13b Moreggplot URL D14 Confound URL D15 Goodtalk URL D16 Webscraping URL D17 Functions URL D18 Ethics URL D19 Bias URL D20 Language URL D21 Fitting URL D04 Ggplot2 URL D22 Cloud URL D22 Nonlinear URL D23 Multiple URL Widget Plotly Plot URL D24 Overfitting URL D25 Crossvalidation URL D26 Quantify URL D27 Bootstrap URL D28 Interactive URL D29 Machine URL D30 Simulations URL D31 Llmintro URL D32 Llmapplications URL Index URL Setup URL "],["colophon.html", "Colophon", " Colophon These notes was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from github. The book style was designed by Desirée De Leon. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.4.1 (2024-06-14 ucrt) #&gt; os Windows 11 x64 (build 26100) #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.utf8 #&gt; ctype English_United States.utf8 #&gt; tz America/New_York #&gt; date 2025-02-17 #&gt; pandoc 3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown) Along with these packages: "],["dont-miss-module-00.html", "Don’t Miss Module 00 0.1 Big Ideas 0.2 Course Modality 0.3 Knowledge is Power 0.4 Meet Prof. Mason 0.5 Website Tour", " Don’t Miss Module 00 This overview is designed to orient you to the class. Please watch the videos from this playlist and work your way through the notes. Although the module-level playlists are embedded in the course, you can find the full-course video playlist here. In addition, you can find the slides for this module here. Currently, there are seven videos in this playlist. The average video length is 12 minutes, 27 seconds. The total length of the playlist is 1 hour, 27 minutes, 10 seconds. Data Science for Psychologists (DS4P) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. This class links those principles to psychological methods and open science practices by emphasizing exploratory analyses and description rather than confirmatory analyses and prediction. Through the semester, we will work our way through Wickham and Grolemund’s R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. DS4P is a practical skilled-based class and should be helpful to students aiming for academia and those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g., eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., valence analysis). 0.1 Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. 0.2 Course Modality I have taught this class in practically every modality available. Technically, this course is designated as a blended course. This designation is because this course’s modality changes based on the level of COVID-19 transmission in the community. When COVID-19 community transmission is low or medium, this class will be in-person and masking will be required. However, if COVID-19 transmission is high in the community, this course may be moved entirely online in order to protect the health and safety of students and instructor. In the case that we move online, the in-person sessions will become synchronous online sessions. Any changes will be clearly and promptly communicated via email. Pragmatically, the mask-to-mask portions of the class are – well – mask-to-mask. Or that was the idea anyway… however, during the first semester I taught this course, a few members of the class were on the other side of the planet. Accordingly, I pivoted all the planned in-class activities and labs so that the entire class could complete their degrees on-time. So obviously this last-minute pivot was a little messy, but I think it turned out ok… So again, technically, this class was blended, but effectively, it can be completed from anywhere at any time. It had to be. 0.2.1 Successful Asynchronous Learning I’ve created a video highlighting how to be a successful asynchronous learner. Much of this information comes from Northeastern University’s Tips for Taking Online Classes 0.3 Knowledge is Power This brief video is covers the icebreaker I do in all of my classes. I encourage you to watch it. In it, I discuss stereotype threats and statistics anxiety. 0.4 Meet Prof. Mason 0.5 Website Tour "],["guidance.html", "Guidance 0.6 Materials 0.7 Assignment Instructions", " Guidance This document provides some basic assignment instructions and information. It is not the syllabus for students taking this course. Please see the data science syllabus on my syllabus website. And yes, the bookdown theme looks familiar… 0.6 Materials 0.6.1 Hardware This class requires that you have a laptop that can run R. 0.6.2 Required Texts The text is intended to supplement the videos, lecture notes, tutorials, activities, and labs. You probably need to consume all of them in order to be successful in this class. All materials for this course are open source, including the multimedia course notes. Garrison’s Data Science for Psychologists (https://datascience4psych.github.io/DataScience4Psych/) Wickham and Grolemund’s R for Data Science text 0.6.3 Software 0.6.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX, Windows, and MacOS platforms. RStudio is a free integrated development environment (IDE), a powerful user interface for R. 0.6.3.2 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files – called a repository – in a structured way. Think of it like the “Track Changes” features from Microsoft Word. GitHub is a free IDE hosting service for Git. As a Wake Forest student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your GitHub account. You can learn more about how we’ll use GitHub in this class here. If you want to jump ahead, here are some git IDEs that you can use to interact with GitHub. 0.7 Assignment Instructions 0.7.1 Portfolio This description is from my advisor, Joe Rodgers. Data science (and Exploratory Data Analysis) is like basketball. We can watch either being done, and appreciate the art and skill involved in high-level performance. In the hands of Lebron James or Michael Jordan, a basketball is a highly-tuned artistic instrument; in the hands of John Tukey, a graph sings the praises of data in melodies both harmonious and discordant, reflecting model, data, and mood. Part of this course will be devoted to Watching and Studying the master at his work. But basketball is played by thousands of bodies with less than NBA training and ability. Some novice basketball players are just learning their craft, and others will evolve into future LJs and MJs; others have lower aspirations, yet still enjoy participating. So, also, should DS be played? A second part of this course will involve learning to do EDA by Doing It. Each of you will be expected to do several portfolio pieces. These projects can be done during class, as well as during out-of-class effort. The nature of most of the particular projects will be entirely up to you. You will report to your instructor during EDA Labs on what you have been doing and what you plan to do. You will give a 10-minute individual presentation to the class at the end of the course on what you did in one or two of your major projects. Each project will require some data, to which EDA techniques will be applied. You are welcomed (in fact, strongly encouraged) to use data with which you are currently involved; dissertation or thesis data, a research project, the almanac, data from an article, data from textbooks, data you collect from your family or friends, or data provided to you by your instructor are possible sources. If you are in need of an example of what a smaller scale portfolio looks like, you can check out Eric Stone’s portofolio (https://estone65.github.io/portfolios/)[https://estone65.github.io/portfolios/], which has three pieces it in. ### Additional Ground Rules In this class, I actively encourage you to “double-dip.” My aim is for you to incorporate your research into portfolio pieces and use these pieces in other places. However, I Mason have some ground rules. These rules have been implemented for various reasons, but they primarily preserve my sanity by establishing some boundaries. As much as I adore this class, my students (especially you, dear reader!), and everything it stands for – it blurs many lines because I often serve numerous roles while teaching this class. In particular, I may also be your mentor, committee member, collaborator, letter of recommendation writer, colleague, confidant, statistics consultant, friend, or cat caretaker. I do my best to navigate these roles. However, for my sanity, here are my basic rules. For anything that is related to a graduate school milestone (thesis, first-year project, major area paper) or something else that you’re working with your advisor on… you need to actively discuss it with your advisor and get their approval in advance. They may not – and that’s okay. If they’re unsure, I’m very happy to schedule a quick 30-minute chat with all three of us. My role on anything milestone-related is to assist you with the implementation or give you general feedback. I cannot give you advice related to modeling or anything that would merit a discussion about authorship on the final work. I may disagree with something they recommend that you do… such as use a specific software that isn’t R. (I do have many strong opinions about SPSS and AMOS, as well as modeling choices. However, there is nearly always room for debate in these issues. I try to remind folks that my opinion is just that – an opinion.) For all things related to your research at Wake Forest, please defer to your advisor. Everyone in the department are amazing at what they do. Seriously, each is an expert in their research area – just as I am an expert in R (and genetically-informed designs and strange data…). In all likelihood, they know more about the modeling in their specific area. I am happy to share my expert opinions and I have the privilege to facilitate such conversations through the nature of this class. HOWEVER, please defer to your advisor for all design decisions. I will do my best to tell you when something is outside the scope of the class or if we’re approaching a gray area. I will not always be successful at navigating this issue. However, I will do my best to do so because it is worth the extra challenge. 0.7.1.1 Possible Projects Examples of appropriate portfolio pieces are listed below. I hope some or all of these will be worked on by members of the class. You should develop and work on your projects individually, but discussion with the instructor and others class members is encouraged and in fact expected. Draw plots by hand of some data that are of interest to you, and transform the variables in several different ways. Interpret your results. Choose some data from EDA (Exploratory Data Analysis) or VDat (Visualizing Data); table or plot them in a way that Tukey/Cleveland didn’t. Find some population data of interest to you (e.g., North Carolina, Forsyth County, your cat herd, etc.) and do several hand plots like those in Chapter 5 of EDA. Interpret results. Find some data in the World Almanac and plot and/or table them. Use some two-way data, and repeatedly extract the medians like Tukey does in Ch. 10 &amp; 11. Find some time series data, and smooth them in several different ways (see EDA, ch. 7). Data with seasonal patterns are especially interesting (see VDat, pp. 152-172). Write an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA, or other program to portray influence-enhanced scatter plots. Produce scatter plots of several relationships. Write a BASIC, FORTRAN, C, SAS, SPSS, JAVA, or other program to portray scatter plots on a computer. Give the user the option to plot X and/or Y as either raw data, logs, squares, cubes, reciprocals, roots, etc. Write an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA or other program to produce some exotic version of stem-and-leaf diagrams. Write a an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA or other program to plot in three-dimensions with time as one of the dimensions (i.e., a kinostatistical plot). Use R or SAS-Graph or some other dedicated graphical package to plot some interesting data (preferably in color, possibly in 3D, maybe even in higher than 3D). Write an R/SAS routine to do median smoothing by three, and use it on some data. Write an R program, or SAS MACRO or SAS PROC or SAS program to produce some EDA output (but don’t duplicate what PROC UNIVARIATE already does). Find an R program in the R library that does interesting EDA; apply it to some interesting data. Produce a correlation matrix between many variables, and develop a scatter plot matrix from it. Read the literature on graphical data analysis and develop some new graphical techniques. Program your techniques. Apply them to real data. Invent a new EDA graphical application, and apply it to real data. Additional ideas that aren’t thoroughly thought out: Data Cleaning Project Using Lab Data Web Scraping Project Tidy Tuesday Project Data Innovation Recreate A Classic Visualization Interactive Project (Rshiny) Infographic Master’s Thesis / First Year Project Misleading Graph Impossible To Read Colorblind Friendly Visualization That Only Uses X Colors Animated/Video Tutorial Webscraper Data Digital Humanities Project Reproduce Findings From An Article In Your Content Area Machine Learn! Live Dashboard Maps/ Geospatial Things Lie To Me Graphic 0.7.1.2 Documenting Your Project I have provided a template that you are welcome to use. You should keep a log describing all EDA projects you undertake. At the end of the course, two things will happen. First, you will give a 10 minute presentation in which you choose one of your EDA projects to describe to the class. Your description should include the goal of the project, the data you used, and a demonstration (PowerPoint, handout, holdup, computer demo, etc.) of the product. 2nd, you will turn in a Portfolio, which consists of two components: A report describing all your projects. The total number depends on the scope and difficulty of each project (as specified in your contract). There may be projects that you don’t finish. That’s fine; EDA projects are hardly ever completely finished; write them up anyway. The projects should be numbered consecutively (i.e., in the order in which you began them), and should include for each project a description of the goal, the product (computer program, hand graph, computer graph, etc.), the data, and some interpretation. Reports must be reproducible and of high quality in terms of writing, grammar, presentation, etc. A prototypical example of the product of each project (e.g., a graph, computer code, etc.). You may wish to put computer output into binders or appendices, graphs into report folders, etc. Portfolios will not be returned; if you wish to have a copy, make one before you turn it in. Portfolios are due during finals. Project reports will not be accepted late. Please, no exceptions!!! 0.7.1.3 More Information about contract grading Marc Kissel on The UnEssay Emily Suzanne Clark on The Unessay Ryan Cordell on How Not to Teach Digital Humanities "],["welcome-to-data-science.html", "1 Welcome to Data Science 1.1 Module Materials", " 1 Welcome to Data Science This module is designed to introduce you to data science. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the module playlist here. Most of the slides used to make the videos in this module can be found in the slides repo. 1.1 Module Materials Videos Located in the subchapters of this module Slidedecks Welcome Slides Meet the toolkit Suggested Readings All subchapters of this module, including R basics and workflow R4DS Book Introduction Data exploration Introduction Happy Git with R If Happy Git is too much, start here If Short Happy Git is too much, start with Oh My Git Activities UN Voting Covid Data Bechdal Test Oh My Git Lab Hello R 1.1.1 Estimated Video Length No of videos : 8 Average length of video: 12 minutes, 6 seconds Total length of playlist: 1 hour, 36 minutes, 48 seconds "],["what-is-data-science.html", "2 What is Data Science? 2.1 See for yourselves 2.2 Course structure and some other useful things", " 2 What is Data Science? You can follow along with the slides here if they do not appear below. 2.1 See for yourselves I’ve embedded a few examples below. 2.1.1 Shiny App 2.1.2 Hans Rosling The video below is the shorter version. Hans Rosling’s 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. 2.1.3 Social Media Social media contains a ton of great (and terrible) examples of data science in action. These examples range from entire subreddits, such as /r/DataisBeautiful (be sure to check out the highest voted posts) to celebrity tweets about data scientists. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones  (@Lesdoggg) January 5, 2021 Good reasons to not be a Data Scientist:- It is a lot of work- Literally nobody will know what you&#39;re talking about- In the end, your computer will be your only real friend&mdash; Kareem Carr, Statistics Person (@kareem_carr) January 22, 2021 2.1.4 Read for yourselves Link Preview What is Data Science @ O’reilly Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdiscplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: “here’s a lot of data, what can you make from it?” What is Data Science @ Quora Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyze it, and find innovative solutions to difficult problems. Basically Data Science is all about Analyzing data and driving for business growth by finding creative ways. The sexiest job of 21st century Data scientists today are akin to Wall Street “quants” of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed master’s programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs. Wikipedia Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. How to Become a Data Scientist Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientist’s role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations. a very short history of #datascience The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young one–computer science. The term “Data Science” has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term “Data Science” and its use, attempts to define it, and related terms. 2.2 Course structure and some other useful things You can follow along with the slides here if they do not appear below. "],["ae01.html", "3 Activity: UN voting 3.1 UN Voting 3.2 COVID Data", " 3 Activity: UN voting You can do either activity. The choice is yours. Activities UN Voting Covid Data 3.1 UN Voting You can find the materials for the UN activity here. The compiled version should look something like the following… 3.2 COVID Data You can find the materials for the Covid version of this activity here. The compiled version should look something like the following… "],["lecture-meet-our-toolbox.html", "4 Lecture: Meet our toolbox 4.1 Reproducible data analysis 4.2 Toolkit for Reproducible Data Analysis 4.3 R and RStudio", " 4 Lecture: Meet our toolbox You can follow along with the slides here if they do not appear below. I recommend installing R, Rstudio, git, and github before starting activity 02 4.1 Reproducible data analysis What does it mean for a data analysis to be “reproducible”? This concept is at the heart of scientific integrity and a cornerstone of data science practice. It means that the results of your analysis can be recreated by others, and that your work is transparent and verifiable. Being able to reproduce the results of an analysis ensures that the conclusions drawn are reliable and can be built upon by future work. 4.1.1 Reproducibility checklist To gauge the reproducibility of a data analysis, consider the following near-term and long-term goals: Near-term goals: Are the tables and figures reproducible from the code and data? Everything we claim should be directly reproducible by running the code we’ve written. Does the code actually do what you think it does? Verify that your code executes correctly and achieves its intended purpose. Mid-term goals: Is the code well-organized and documented? Structured and well-documented code is easier to understand and maintain. Can someone else understand what you did? The code should be clear and well-documented so that others can understand and build upon it. Can someone else understand why you did what you did? Understanding the rationale behind decisions, such as parameter settings, enhances clarity and reproducibility. Long-term goals: Can the code be used for other data? Flexibility in code for reuse with different datasets broadens the applicability of your work. Can you extend the code to do other things? Structure your projects so that your codebase can be easily expanded for additional analyses or functionalities. Being able to build upon existing code without starting from scratch each time accelerates progress. 4.2 Toolkit for Reproducible Data Analysis A successful data analyst in psychology relies on a core set of tools: Scriptability using R, enabling direct manipulation of data. Literate programming with R Markdown, which integrates code, narrative, and output. Version control through Git/GitHub, documenting project evolution and facilitating collaboration. The toolkit empowers us to achieve our reproducibility goals and supports a structured and efficient workflow: 4.3 R and RStudio 4.3.1 Install R and RStudio Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page linked above! Install RStudio’s IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 4.3.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you haven’t written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, you’ve succeeded in installing R and RStudio. 4.3.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage 4.3.4 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudio’s leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["bechdal.html", "5 Activity: Bechdel", " 5 Activity: Bechdel Activities Bechdal Test Oh My Git You can find the materials for the Bechdel activity here. The compiled version should look something like the following… "],["activity-oh-my-git-version-control-challenge.html", "6 Activity: Oh My Git! Version Control Challenge", " 6 Activity: Oh My Git! Version Control Challenge In this activity, you will: Visit Oh My Git! and download/play the introductory levels. Learn the basics of Git through the game’s puzzles (commit, branch, merge, resolve conflicts). (Optional) Create a branch in your own Bechdel test repository (or any class dataset repo) and apply the same concepts you learned. This approach lets you practice Git in a fun setting first, then apply those skills to data analysis tasks. "],["lecture-thoughtful-workflow.html", "7 Lecture: Thoughtful Workflow 7.1 R Markdown 7.2 Git and Github 7.3 Getting Help with R", " 7 Lecture: Thoughtful Workflow At this point, I recommend you pause and think about your workflow. I give you permission to spend some time and energy sorting this out! It can be as or more important than learning a new R function or package. The experts don’t talk about this much, because they’ve already got a workflow; it’s something they do almost without thinking. Working through subsequent material in R Markdown documents, possibly using Git and GitHub to track and share your progress, is a great idea and will leave you more prepared for your future data analysis projects. Typing individual lines of R code is but a small part of data analysis and it pays off to think holistically about your workflow. If you want a lot more detail on workflows, you can wander over to the optional bit on r basics and workflow. 7.1 R Markdown If you’d rather just get on with it, here’s the abridged version. But, if you are in the mood to be entertained, start the video from the beginning. You can follow along with the slides here if they do not appear below. R Markdown is an accessible way to create computational documents that combine prose and tables and figures produced by R code. An introductory R Markdown workflow, including how it intersects with Git, GitHub, and RStudio, is now maintained within the Happy Git site: Test drive R Markdown 7.2 Git and Github XKCD on Git First, it’s important to realize that Git and GitHub are distinct things. GitHub is an online hosting platform that provides an array of services built on top of the Git system. (Similar platforms include Bitbucket and GitLab.) Just like we don’t need Rstudio to run R code, we don’t need GitHub to use Git… But, it will make our lives so much easier. I recommend checking out Jenny Bryan’s instructions around installation, setup, and early Git usage with her book Happy Git with R. I have a recommended deep dive in a later chapter](#shorthappygit). You can follow along with the slides here if they do not appear below. 7.2.1 What is Github? 7.2.2 Git Git is a distributed Version Control System (VCS). It is a useful tool for easily tracking changes to your code, collaborating, and sharing. (Wait, what?) Okay, try this: Imagine if Dropbox and the “Track changes” feature in MS Word had a baby. Git would be that baby. In fact, it’s even better than that because Git is optimized for the things that social scientists and data scientists spend a lot of time working on (e.g. code). The learning curve is worth it – I promise you! With Git, you can track the changes you make to your project so you always have a record of what you’ve worked on and can easily revert back to an older version if need be. It also makes working with others easier -— groups of people can work together on the same project and merge their changes into one final source! GitHub is a way to use the same power of Git all online with an easy-to-use interface. It’s used across the software world and beyond to collaborate and maintain the history of projects. There’s a high probability that your favorite app, program or package is built using Git-based tools. (RStudio is a case in point.) Scientists and academic researchers are starting to use it as well. Benefits of version control and collaboration tools aside, Git(Hub) helps to operationalize the ideals of open science and reproducibility. Journals have increasingly strict requirements regarding reproducibility and data access. GH makes this easy (DOI integration, off-the-shelf licenses, etc.). I run my entire lab on GH; this entire course is running on github; these lecture notes are hosted on github… 7.3 Getting Help with R R and its community offer extensive support resources. Utilizing forums, help files, and documentation can significantly accelerate problem-solving and learning. You can follow along with the slides here if they do not appear below. Check out this helpful post on asking good R questions… "],["r_basics.html", "8 Notes: R basics and workflows 8.1 Working with RStudio and the R Console 8.2 Workspace and working directory 8.3 RStudio projects 8.4 Tradition", " 8 Notes: R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 There is an implicit contract between computers and scripting languages. The computer will do tedious tasks for you. In return, you must be explicit in your instructions. The computer does not have the ability to extrapolate. So we have to work within the range of what it does understand. And that is where this entire course comes into play. We are learning how to communicate in a way that it does understand. So let’s begin! The 8 year old is learning Python, and after a dealing with a syntax bug she asks: “If the computer knows I’m missing a semicolon here, why won’t it add it itself?”I don’t know. I really don’t know.&mdash; Joe Magerramov (@_joemag_) February 8, 2022 8.1 Working with RStudio and the R Console 8.1.1 Initial Setup in RStudio Upon launching RStudio, note the following default panes: Console (entire left) Environment/History (upper right, tabbed) Files/Plots/Packages/Help (lower right, tabbed) You can customize the layout via Customizing RStudio. 8.1.2 Basic Commands and Assignments You can interact with R in realtime using the Console. Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 # Assigning a product to &#39;x&#39; x # Displaying the value of &#39;x&#39; #&gt; [1] 12 All R statements where you create objects – “assignments” – have this form: objectName &lt;- value # Example: &#39;x gets 12&#39; #&gt; Error: object &#39;value&#39; not found and in my head I hear, e.g., “x gets 12”. You will make lots of assignments. Even thought, the operator &lt;- is a bit more cumbersome to type, resist the urge to use =. Technically, it would work. But it will be really confusing later. Instead, try RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio auto-magically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Well-spaced code is less painful to read. RStudio offers a lot of reference cards, including one for keyboard shortcuts. You can even summon a keyboard shortcut reference card with Alt+Shift+K. 8.1.3 Object names Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this assignment, try out RStudio’s completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2^3 Let’s try to inspect: masonrocks #&gt; Error: object &#39;masonrocks&#39; not found masn_rocks #&gt; Error: object &#39;masn_rocks&#39; not found Here’s where that implicit contract comes in. The computer (and R) will do amazing things, if we can ask it to do those things in a way it understands. Typos matter. Case matters. Precision matters. We have to work within it’s narrow range of ability. The computer isn’t accommodating, unfortunately. However, if you struggle with precise typing, it might be worth your time to teach your computer to be accommodating. And as a result, you might discover that you have a real knack for programming… and that knack will lead you toward quantitative psychology… and teaching data science for psychologists… In other words, I have some typing macros and tricks that may help you. If there’s interest, I can carve out some time in-class to discuss them. 8.1.4 Functions R has a huge collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Let’s try using seq() which makes regular sequences of numbers and, while we’re at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a function’s arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Because we didn’t specify step size, the default value of by in the function definition is used. In this case, the default is 1. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you don’t get to see the value. Then you’re tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses. These combined action causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Mon Feb 17 19:25:05 2025&quot; Now look at your workspace – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_funyoutube&quot; #&gt; [3] &quot;ds4p_urls&quot; &quot;ebirdkey&quot; #&gt; [5] &quot;embed_youtube_alt&quot; &quot;install_quietly&quot; #&gt; [7] &quot;mason_rocks&quot; &quot;pretty_install&quot; #&gt; [9] &quot;sample_no_surprises&quot; &quot;shhh_check&quot; #&gt; [11] &quot;slide_url&quot; &quot;this_is_a_really_long_name&quot; #&gt; [13] &quot;try_include_tweet&quot; &quot;x&quot; #&gt; [15] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_funyoutube&quot; #&gt; [3] &quot;ds4p_urls&quot; &quot;ebirdkey&quot; #&gt; [5] &quot;embed_youtube_alt&quot; &quot;install_quietly&quot; #&gt; [7] &quot;mason_rocks&quot; &quot;pretty_install&quot; #&gt; [9] &quot;sample_no_surprises&quot; &quot;shhh_check&quot; #&gt; [11] &quot;slide_url&quot; &quot;this_is_a_really_long_name&quot; #&gt; [13] &quot;try_include_tweet&quot; &quot;x&quot; #&gt; [15] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 8.2 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. will you save it as your lasting record of what happened? Where does your analysis “live”? 8.2.1 Workspace, .RData As a beginning R user, it’s OK to consider your workspace “real”. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as “real”. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as “real” (saving and reloading all the time), if you need to redo analysis … you’re going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your “good” R code in a script for future reuse. Because it can be useful sometimes, note the commands you’ve recently run appear in the History pane. But you don’t have to choose right now and the two strategies are not incompatible. Let’s demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. You’ll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click “Save”. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and you’ll see the same objects as before. In the History tab of the same pane, you should also see your command history. You’re back in business. This way of starting and stopping analytical work will not serve you well for long but it’s a start. 8.2.2 Working directory Any process running on your computer has a notion of its “working directory”. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, it’s OK let your home directory or any other weird directory on your computer be R’s working directory. Very soon, I urge you to evolve to the next level, where you organize your projects into directories and, when working on project A, set R’s working directory to the associated directory. Although I do not recommend it, in case you’re curious, you can set R’s working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudio’s Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (You’ll see even more options there). Or within the Files pane, choose “More” and “Set As Working Directory”. But there’s a better way. A way that also puts you on the path to managing your R work like an expert. 8.3 RStudio projects Keeping all the files associated with a project organized together – input data, R scripts, results, figures – is such a wise and common practice that RStudio has built-in support for this via its projects. Let’s make one to use for the rest of this class. Do this: File &gt; New Project…. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the “home” directory for your project is the working directory of our current R process: getwd() I can’t print my output here because this document itself does not reside in the RStudio Project we just created. Let’s enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.561 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Let’s say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click “To Source”. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line … whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click “Run” in the upper right corner of editor pane). Source the entire document – equivalent to entering source('toy-line.r') in the Console – by keyboard shortcut (Shift+Command+S) or mouse (click “Source” in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 8.4 Tradition It is tradition to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like you’ve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code you’re saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (we’ll soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (I’m one of them), never save or consult the history. Once you get to that point, there are options available in RStudio to deactivate the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This method is rarely necessary when using RStudio projects, and can be challenging for collaborators. "],["quickhappygit.html", "9 RDD: Quick Starting with Github 9.1 The Basics of GitHub and Git 9.2 Getting Started with GitHub 9.3 Half the battle 9.4  Resources", " 9 RDD: Quick Starting with Github One of the goals for this course is to familiarize yourself with Git and GitHub. This section is a concise version of a more detailed document you’ll find later in the class, combining insights from GitHub Classroom’s github-starter-course and Jenny Bryan’s happygitwithr. I strongly encourage you to check out the unabridged version of happygit as it has so much more detail. Alternatively, this video by Jess Chan of Coder Coder is an excellent resource for beginners. Following along with @SMasonGarrison&#39;s open source data science for psychologists course has helped me finally understand @github which has improved my work and my life so much &mdash; Jen Traver (@jmtraver1) October 17, 2021 9.1 The Basics of GitHub and Git 9.1.1 What is Git? Git is a distributed Version Control System (VCS), which means it is a useful tool for easily tracking changes to your code, collaborating, and sharing. With Git you can track the changes you make to your project so you always have a record of what you’ve worked on and can easily revert back to an older version if need be. It also makes working with others easier—groups of people can work together on the same project and merge their changes into one final source! 9.1.2 What is GitHub? GitHub extends Git’s capabilities by providing a web-based interface for collaboration. It’s widely used for code development, allowing multiple people to work together on projects. GitHub hosts your repositories (project folders) and provides tools for managing changes, reviewing code, and collaborating with others. 9.2 Getting Started with GitHub 9.2.1 Create a GitHub Account Sign up at github.com. Choose a thoughtful username as it represents you in the GitHub community. You will be able to upgrade to a paid level of service, apply discounts, join organizations, etc., in the future. 9.2.1.1 Username advice You will be able to upgrade to a paid level of service, apply discounts, join organizations, etc. in the future, so don’t fret about any of that now. Except your username. You might want to give that some thought. A few tips, which sadly tend to contradict each other: Incorporate your actual name! People like to know who they’re dealing with. Also makes your username easier for people to guess or remember. Reuse your username from other contexts, e.g., Twitter or Slack. But, of course, someone with no GitHub activity will probably be squatting on that. Pick a username you will be comfortable revealing to your future boss. Shorter is better than longer. Be as unique as possible in as few characters as possible. In some settings GitHub auto-completes or suggests usernames. Make it timeless. Don’t highlight your current university, employer, or place of residence, e.g. JennyFromTheBlock. Avoid words laden with special meaning in programming. In Jenny’s first inept efforts to script around the GitHub API, she assigned lots of issues to the guy with username NA because my vector of GitHub usernames contained missing values. A variant of Little Bobby Tables. Avoid the use of upper vs. lower case to separate words. We highly recommend all lowercase. GitHub treats usernames in a case insensitive way, but using all lowercase is kinder to people doing downstream regular expression work with usernames, in various languages. A better strategy for word separation is to use a hyphen - or underscore _. You can change your username later, but better to get this right the first time. https://help.github.com/articles/changing-your-github-username/ https://help.github.com/articles/what-happens-when-i-change-my-username/ 9.2.2 Install Git and a Git client Install Git: Essential for interacting with GitHub. Installation instructions are available for Windows, macOS, and Linux/Unix. Install a Git client: GitHub offers a free Git(Hub) client, GitHub Desktop, for Windows and macOS. GitHub Desktop is aimed at beginners who want the most useful features of Git front and center. The flipside is that it may not support some of the more advanced workflows exposed by the clients above. At present, this client is what I, Mason, mostly use. 9.3 Half the battle Getting all the necessary software installed, configured, and playing nicely together is honestly half the battle when first adopting Git. Brace yourself for some pain. The upside is that you can give yourself a pat on the back once you get through this. And you WILL get through this. You will find far more resources for how to use Git than for installation and configuration. Why? The experts … Have been doing this for years. It’s simply not hard for them anymore. Probably use some flavor of Unix. They may secretly (or not so secretly) take pride in neither using nor knowing Windows. Get more satisfaction and reward for thinking and writing about Git concepts and workflows than Git installation. In their defense, it’s hard to write installation instructions. Failures can be specific to an individual OS or even individual computer. 9.3.1 What is a Git client? Why would you want one? “Git” is really just a collection of individual commands you execute in the shell. This interface is not appealing for everyone. Some may prefer to do Git operations via a client with a graphical interface. Git and your Git client are not the same thing, just like R and RStudio are not the same thing. A Git client and an integrated development environment, such as RStudio, are not necessary to use Git or R, respectively. But they make the experience more pleasant because they reduce the amount of command line bullshittery and provide a richer visual representation of the current state. RStudio offers a very basic Git client via its Git pane. I use this often for simple operations, but you probably want another, more powerful one as well. Install a Git client: GitHub offers a free Git(Hub) client, GitHub Desktop, for Windows and macOS. GitHub Desktop is aimed at beginners who want the most useful features of Git front and center. The flipside is that it may not support some of the more advanced workflows exposed by the clients above. At present, this client is what I, Mason, mostly use. 9.4  Resources A short video explaining what GitHub is Git and GitHub learning resources Understanding the GitHub flow How to use GitHub branches Interactive Git training materials GitHub’s Learning Lab Education community forum GitHub community forum 9.4.1 Oh My Git Oh My Git! is a game that “visualizes the internal structures of Git repositories in realtime”. It’s a great way to learn how Git works and to debug problems with your repositories. "],["lab01.html", "10 Lab: Hello R! About The Hello R Lab Lab Goals", " 10 Lab: Hello R! Many of the labs for this course have been adapted from a series of Rstudio tutorials. These tutorials were initially created by Mine Çetinkaya-Rundel. Mine is fantastic; her work is fantastic; and she’s just a badass! I have adapted these tutorials for two reasons: I think it useful to see other people working with R; and Pragmatically, adapting Mine’s lab materials means that I can spend more time on other aspects of the course – like the website, course notes, videos, feedback, learning how to embed tweets… That&#39;s so wonderful to hear, thank you!&mdash; Mine Çetinkaya-Rundel (@minebocek) January 22, 2021 Seriously, you’d never know it, but every hour of finished video takes between 6 and 8 hours to make. (3 hours of writing, 1.5 hours of filming, and 3.5 hours for video editing). About The Hello R Lab This lab, collectively named the Hello R lab, is divided into two parts, A and Z. The first part, Aloha R, is focused on setting up GitHub, RStudio, and YAML. It also introduces some basics of R Markdown. The second part, Zdravo R, is the heart of the lab excercises, with a series of data visualization and analysis tasks, introducing ggplot2, dplyr, and related tools. This is the hardest lab of the semester, but it’s also the most important. If you can get through this, you can get through anything. Please complete the lab in order, and don’t skip ahead. You’ll need to know the basics before you can do the more advanced stuff. Lab Goals Recall: R is the name of the programming language itself, and RStudio is a convenient interface. The primary goal of this lab is to introduce you to R and RStudio, tools we will use throughout the course: * to learn the statistical concepts discussed in the course, and * to analyze real data and come to informed conclusions. Recall: git is a version control system (like “Track Changes” features from Microsoft Word on steroids), and GitHub is the home of your Git-based projects on the Internet (like DropBox but much, much better). The second goal is to introduce you to Git and GitHub, the collaboration and version control system that we will use throughout the course. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today, we’ll start with the foundational building blocks of R and RStudio: the interface, reading in data, and basic commands. To make versioning simpler, this lab is a solo lab. I want to make sure everyone gets a substantial amount of time at the steering wheel, working directly with R. "],["lab01aloha.html", "11 Aloha R! Getting started Using GitHub Desktop Introduction to R and RStudio", " 11 Aloha R! The first part of the lab, Aloha R, focuses on setting up GitHub, RStudio, and YAML while introducing the basics of R Markdown. This section supports the second goal of the Hello R lab. The goal is to get you comfortable with the tools you’ll be using throughout the course. Getting started Each of your assignments will begin with these initial steps. This lab provides a detailed overview of the process, but future labs will offer less detail. You can always refer back to this lab for a detailed list of the steps involved in getting started with an assignment. You can find the assignment link for this lab here. This GitHub repository (“repo”) serves as a template for the assignment. You can build on it to complete your assignment. First, log into your GitHub account. If you don’t have one, you can create one here, using your university email address. Next, you need to get a copy of the lab repo on your own GitHub account. This is called “cloning” the repo. You can do this by clicking on the green “Use this template” button. On GitHub, click on the Use this template button. This option initiates the creation of a repo on your account, based on the lab template. You should now provide a name to your new repository. Lab 01 or something to that effect should work nicely. You can also provide a description if you like. It’s not necessary, but it can be helpful to future you. I recommend making the repo public, but private is fine if you already have GitHub pro set up. Once you fill out all the required information, click on the Create Repository from Template button. Now you should have your own copy of the lab! Now we need to connect your new git repo with your computer. Notice that are a few ways to interface between git and R. Clone Open Download Using GitHub Desktop For this lab, we’ll use GitHub Desktop to manage your repository. It’s a simple, beginner-friendly tool for interacting with Git. Yoo Ri Hwang wrote up a helpful demonstration of using GitHub Desktop for a lab. Here’s how to get started: If you don’t already have GitHub Desktop installed, download it from desktop.github.com and install it. Open GitHub on your browser and log in with your GitHub account. Go to your repositories tab and click “New.” Click “Add” and select a repository template In this case, the template is DataScience4Psych/lab-01-plastic-waste You can create this repository. Then, go to the repository that you’ve just created. Click code -&gt; open with the GitHub Desktop. Click Open GitHub Desktop You can change the local path by clicking “choose”. And if you click the “clone” button, the lab02 plastic waste file would be in your local path. Your changes should be automatically reflected in the app. Write something in there (where I wrote “write something”) and click the “Commit to main” button. Click the “Push Origin” button, and tada~ Hit OK, and you’re good to go! Option 2: Use RStudio Option 2 From your repo, select Code. Use RStudio to clone the repo. Now, select Use HTTPS (this might already be selected by default, and if it is, you’ll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio. Select from the menu, new project. Using the new project wizard, select Version Control Select the option for Git Copy and paste the URL of your assignment repo into the dialog box. Fill out the information for what folder you’d like to store your lab in. Hit OK, and you’re good to go! Introduction to R and RStudio Before we introduce the data, let’s warm up with some simple exercises. FYI: The top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document. YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, save your changes, and then knit the document. Committing changes Then go to the Git pane in your RStudio. If you have made saved changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If you’re happy with these changes, write “Update author name” in the Commit message box and hit Commit. You do not have to commit after every change, indeed doing so would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments, I’ll suggest exactly when to commit and in some cases, what commit message to use. As the semester progresses, you make these decisions. Pushing changes Now that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, depending on your privacy settings, we mean the just you OR perhaps the world… In order to push your changes to GitHub, click on Push. This will prompt a dialog box where you first need to enter your user name, and then your password. This might feel cumbersome. Soon – you will learn how to save your password so you don’t have to enter it every time. But for this one assignment you’ll have to manually enter each time you push in order to gain some experience with it. "],["lab01execercise.html", "12 Zdravo R! Packages Data Exercises", " 12 Zdravo R! The 2nd part of the lab is called Zdravo R. Zdravo is the Slovenian word for “hello”. This is the second half of the Hellow R lab. You should have just finished the first part of the lab (Aloha R), which was focused on getting you set up with R and RStudio. This part of the lab is focused on actually using R to do some data analysis. Packages In this lab, we will work with two packages: datasauRus and tidyverse. datasauRus contains the dataset we’ll be using; tidyverse is a collection of packages for doing data analysis in a “tidy” way. Install these packages by running the following commands in the console. install.packages(&quot;tidyverse&quot;) install.packages(&quot;datasauRus&quot;) Now that the necessary packages are installed, you should be able to Knit your document and see the results. If you’d like to run your code in the Console as well you’ll also need to load the packages there. To do so, run the following in the console. library(tidyverse) library(datasauRus) #&gt; Warning: package &#39;datasauRus&#39; was built under R version 4.4.2 Note that the packages are also loaded with the same commands in your R Markdown document. Data Fun fact: If it’s confusing that the data frame is called datasaurus_dozen when it contains 13 datasets, you’re not alone! Have you heard of a baker’s dozen? The data frame we will be working with today is called datasaurus_dozen and it’s in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable. To find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark (or two, depending on the version) before the name of an object will always bring up its help file. This command must be run in the Console. Exercises Based on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. Let’s take a look at what these datasets are. To do so we can make a frequency table of the dataset variable: datasaurus_dozen %&gt;% count(dataset) %&gt;% print() #&gt; # A tibble: 13 × 2 #&gt; dataset n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 away 142 #&gt; 2 bullseye 142 #&gt; 3 circle 142 #&gt; 4 dino 142 #&gt; 5 dots 142 #&gt; 6 h_lines 142 #&gt; 7 high_lines 142 #&gt; 8 slant_down 142 #&gt; 9 slant_up 142 #&gt; 10 star 142 #&gt; 11 v_lines 142 #&gt; 12 wide_lines 142 #&gt; 13 x_shape 142 Fun fact: Matejka, Justin, and George Fitzmaurice. “Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017. The original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets that have the same summary statistics as the Datasaurus but have very different distributions.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with the commit message “Added answer for Ex 1”. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset. Below is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results. Start with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data. dino_data &lt;- datasaurus_dozen %&gt;% filter(dataset == &quot;dino&quot;) Because a lot going on here – let’s slow down and unpack it a bit. First, the pipe operator: %&gt;%, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\". Second, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame. Next, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data you’re visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case, we want these to be points, hence geom_point. ggplot(data = dino_data, mapping = aes(x = x, y = y)) + geom_point() If this seems like a lot, it is. You will learn about the philosophy of layering data visualizations in the next module. For now, follow along with the code that is provided. For the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. Recall: Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This nonlinear relationships are exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial! But, for illustrative purposes, let’s calculate the correlation coefficient between x and y. Tip: Start with dino_data and calculate a summary statistic that we will call r as the correlation between x and y. dino_data %&gt;% summarize(r = cor(x, y)) #&gt; # A tibble: 1 × 1 #&gt; r #&gt; &lt;dbl&gt; #&gt; 1 -0.0645  ✅ ⬆️ Knit, commit, and push your changes to GitHub with the commit message “Added answer for Ex 2”. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino? # code goes here  ✅ ⬆️ This is another good place to pause, knit, commit changes with the commit message “Added answer for Ex 3”, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?  ✅ ⬆️ You should pause again, commit changes with the commit message “Added answer for Ex 4”, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Finally, let’s plot all datasets at once. In order to do this, we will make use of faceting. Hint: Facet by the dataset variable, placing the plots in a 3 column grid. ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) + geom_point() + facet_wrap(~dataset, ncol = 3) + theme(legend.position = &quot;none&quot;) And we can use the group_by function to generate all the summary correlation coefficients. datasaurus_dozen %&gt;% group_by(dataset) %&gt;% summarize(r = cor(x, y)) %&gt;% print(13) You’re done with the data analysis exercises, but I’d like you to do two more things: Resize your figures: Click on the gear icon near the top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialog box, go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML. You can also use different figure sizes for different figures. To do so, click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well. Change the look of your report: Once again, click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialog box, try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look. Pro Tip: Not sure how to use emojis on your computer? Maybe a teammate can help?  ✅ ⬆️ Yay, you’re done! Commit all remaining changes, use the commit message “Done with Lab 1! “, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo. Click this text for Bonus Tips by Yoo Ri Here are some helpful tips  filter() is for extracting rows group_by() is for grouping datasets by assigned column ungroup() cancels the grouping summarize() is often used with group_by(). This function can print the output according to group_by(). facet_grid(y~x,…) creates a grid with variable y as a row, variable x as a column facet_wrap(x,… ) is useful when there is only one variable "],["welcome-to-data-and-visualization.html", "13 Welcome to Data and Visualization 13.1 Module Materials", " 13 Welcome to Data and Visualization This module is designed to introduce you to exploratory and graphical data analysis. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. The slides used to make the videos in this module can be found in the slides repo. 13.1 Module Materials Slides from Lectures Data and visualization Visualizing data with ggplot2 Visualizing numerical data Visualizing categorical data Suggested Readings All subchapters of this module, including Basic care and feeding of data in R R4DS Data Exploratation Section, including Data visualization Exploratory Data Analysis Activities Star Wars! Lab Plastic waste 13.1.1 Estimated Video Length No of videos : 9 Average length of video : 13 minutes, 34 seconds Total length of playlist : 2 hours, 2 minutes, 11 seconds "],["exploratory-data-analysis.html", "14 Exploratory Data Analysis 14.1 What is in a dataset?", " 14 Exploratory Data Analysis You can follow along with the slides here if they do not appear below. 14.1 What is in a dataset? 14.1.1 Why do we visualize? "],["visualizing-data-with-ggplot2.html", "15 Visualizing data with ggplot2 15.1 ggplot2 and aesthetics", " 15 Visualizing data with ggplot2 Does anyone else feel that their affinity for an artistic skill helps them with statistics? Just me?&mdash; Valerie Polad (@valeriepolad) May 2, 2021 Your data visualizations suck.In the beginning, at least. See, that’s the secret no one tells you. We all sucked when we got started. Make bad visualizations and share them. A lot of them. Then people will help you and they’ll get better. That’s how you learn.&mdash; Oh, Friend. (@oh__friend) December 10, 2021 You can follow along with the slides here if they do not appear below. 15.1 ggplot2 and aesthetics "],["visualizing-numerical-data.html", "16 Visualizing numerical data 16.1 Looking at Data 16.2 More on visualizing numerical data", " 16 Visualizing numerical data You can follow along with the slides here if they do not appear below. 16.1 Looking at Data 16.2 More on visualizing numerical data Fun fact, when you screen capture with f.lux running in the background, f.lux is captured too. "],["visualizing-categorical-data.html", "17 Visualizing categorical data", " 17 Visualizing categorical data You can follow along with the slides here if they do not appear below. "],["star-wars-activity.html", "18 Star Wars Activity", " 18 Star Wars Activity You can find the materials for the Star Wars activity here. The compiled version should look something like the following… "],["basiccare.html", "19 Basic care and feeding of data in R 19.1 Buckle your seatbelt 19.2 Data frames are awesome 19.3 Get the Gapminder data 19.4 Meet the gapminder data frame or “tibble” 19.5 Look at the variables inside a data frame 19.6 Recap", " 19 Basic care and feeding of data in R Feel free to ignore this extra scaffolding if you don’t need this bonus support. However, I strongly encourage you to take a look, even if you think you know what you’re doing. Bestie, don’t feel bad about ANY data set up you have  LuLaRoe is a billion dollar company using a GOOGLE SPREADSHEET THAT MULTIPLE PEOPLE HAD EDIT ACCESS TO  pic.twitter.com/DbD7X9thUk&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) September 17, 2021 19.1 Buckle your seatbelt Now is the time to make sure you are working in an appropriate directory on your computer, probably through the use of an RStudio project. Enter getwd() in the Console to see current working directory or, in RStudio, this is displayed in the bar at the top of Console. You should clean out your workspace. In RStudio, click on the “Clear” broom icon from the Environment tab or use Session &gt; Clear Workspace. You can also enter rm(list = ls()) in the Console to accomplish same. Now restart R. Restarting will ensure you don’t have any packages loaded from previous calls to library(). In RStudio, use Session &gt; Restart R. Otherwise, quit R with q() and re-launch it. Why do we do this? So that the code you write is complete and re-runnable. If you return to a clean slate often, you will root out hidden dependencies where one snippet of code only works because it relies on objects created by code saved elsewhere or, much worse, never saved at all. Similarly, an aggressive clean slate approach will expose any usage of packages that have not been explicitly loaded. Finally, open a new R script and develop and run your code from there. In RStudio, use File &gt; New File &gt; R Script. Save this script with a name ending in .r or .R, containing no spaces or other funny stuff, and that evokes whatever it is we’re doing today. Example: cm004_data-care-feeding.r. Another great idea is to do this in an R Markdown document. See Test drive R Markdown for a refresher. 19.2 Data frames are awesome Whenever you have rectangular, spreadsheet-y data, your default data receptacle in R is a data frame. Do not depart from this practice without good reason. Data frames are awesome because… Data frames package related variables neatly together, keeping them in sync vis-a-vis row order applying any filtering of observations uniformly Most functions for inference, modeling, and graphing are happy to be passed a data frame via a data = argument. This has been true in base R for a long time. The set of packages known as the tidyverse takes this one step further and explicitly prioritizes the processing of data frames. This includes popular packages like dplyr and ggplot2. In fact the tidyverse prioritizes a special flavor of data frame, called a “tibble”. Data frames – unlike general arrays or, specifically, matrices in R – can hold variables of different flavors, such as character data (subject ID or name), quantitative data (white blood cell count), and categorical information (treated vs. untreated). If you use homogeneous structures, like matrices, for data analysis, you are likely to make the terrible mistake of spreading a dataset out over multiple, unlinked objects. Why? Because you can’t put character data, such as subject name, into the numeric matrix that holds white blood cell count. This fragmentation is a Bad Idea. 19.3 Get the Gapminder data We will work with some of the data from the Gapminder project. Jenny Bryan has released this data as an R package, so we can install it from CRAN like so: install.packages(&quot;gapminder&quot;) Now load the package: library(gapminder) 19.4 Meet the gapminder data frame or “tibble” By loading the gapminder package, we now have access to a data frame by the same name. Get an overview of this with str(), which displays the structure of an object. str(gapminder) #&gt; tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just str() some of the recently created objects to get some ideas about what to do next. We could print the gapminder object itself to screen. However, if you’ve used R before, you might be reluctant to do this, because large datasets just fill up your Console and provide very little insight. This is the first big win for tibbles. The tidyverse offers a special case of R’s default data frame: the “tibble”, which is a nod to the actual class of these objects, tbl_df. If you have not already done so, install the tidyverse meta-package now: install.packages(&quot;tidyverse&quot;) Now load it: library(tidyverse) Now we can boldly print gapminder to screen! It is a tibble (and also a regular data frame) and the tidyverse provides a nice print method that shows the most important stuff and doesn’t fill up your Console. ## see? it&#39;s still a regular data frame, but also a tibble class(gapminder) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; gapminder #&gt; # A tibble: 1,704 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ℹ 1,694 more rows If you are dealing with plain vanilla data frames, you can rein in data frame printing explicitly with head() and tail(). Or turn it into a tibble with as_tibble()! head(gapminder) #&gt; # A tibble: 6 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder) #&gt; # A tibble: 6 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zimbabwe Africa 1982 60.4 7636524 789. #&gt; 2 Zimbabwe Africa 1987 62.4 9216418 706. #&gt; 3 Zimbabwe Africa 1992 60.4 10704340 693. #&gt; 4 Zimbabwe Africa 1997 46.8 11404948 792. #&gt; 5 Zimbabwe Africa 2002 40.0 11926563 672. #&gt; 6 Zimbabwe Africa 2007 43.5 12311143 470. as_tibble(iris) #&gt; # A tibble: 150 × 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # ℹ 140 more rows More ways to query basic info on a data frame: names(gapminder) #&gt; [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; ncol(gapminder) #&gt; [1] 6 length(gapminder) #&gt; [1] 6 dim(gapminder) #&gt; [1] 1704 6 nrow(gapminder) #&gt; [1] 1704 A statistical overview can be obtained with summary(): summary(gapminder) #&gt; country continent year lifeExp #&gt; Afghanistan: 12 Africa :624 Min. :1952 Min. :23.6 #&gt; Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.2 #&gt; Algeria : 12 Asia :396 Median :1980 Median :60.7 #&gt; Angola : 12 Europe :360 Mean :1980 Mean :59.5 #&gt; Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.8 #&gt; Australia : 12 Max. :2007 Max. :82.6 #&gt; (Other) :1632 #&gt; pop gdpPercap #&gt; Min. :6.00e+04 Min. : 241 #&gt; 1st Qu.:2.79e+06 1st Qu.: 1202 #&gt; Median :7.02e+06 Median : 3532 #&gt; Mean :2.96e+07 Mean : 7215 #&gt; 3rd Qu.:1.96e+07 3rd Qu.: 9325 #&gt; Max. :1.32e+09 Max. :113523 #&gt; Although we haven’t begun our formal coverage of visualization yet, it’s so important for smell-testing dataset that we will make a few figures anyway. Here, we use only base R graphics, which are very basic. plot(lifeExp ~ year, gapminder) plot(lifeExp ~ gdpPercap, gapminder) plot(lifeExp ~ log(gdpPercap), gapminder) Let’s go back to the result of str() to talk about what a data frame is. str(gapminder) #&gt; tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... A data frame is a special case of a list, which is used in R to hold just about anything. Data frames are a special case where the length of each list component is the same. Data frames are superior to matrices in R because they can hold vectors of different flavors, e.g. numeric, character, and categorical data can be stored together. This comes up a lot! 19.5 Look at the variables inside a data frame To specify a single variable from a data frame, use the dollar sign $. Let’s explore the numeric variable for life expectancy. head(gapminder$lifeExp) #&gt; [1] 28.8 30.3 32.0 34.0 36.1 38.4 summary(gapminder$lifeExp) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 23.6 48.2 60.7 59.5 70.8 82.6 hist(gapminder$lifeExp) The year variable is an integer variable, but since there are so few unique values it also functions a bit like a categorical variable. summary(gapminder$year) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1952 1966 1980 1980 1993 2007 table(gapminder$year) #&gt; #&gt; 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 #&gt; 142 142 142 142 142 142 142 142 142 142 142 142 The variables for country and continent hold truly categorical information, which is stored as a factor in R. class(gapminder$continent) #&gt; [1] &quot;factor&quot; summary(gapminder$continent) #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 levels(gapminder$continent) #&gt; [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) #&gt; [1] 5 The levels of the factor continent are “Africa”, “Americas”, etc. and this is what’s usually presented to your eyeballs by R. In general, the levels are friendly human-readable character strings, like “male/female” and “control/treated”. But never ever ever forget that, under the hood, R is really storing integer codes 1, 2, 3, etc. Look at the result from str(gapminder$continent) if you are skeptical. str(gapminder$continent) #&gt; Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... This Janus-like nature of factors means they are rich with booby traps for the unsuspecting but they are a necessary evil. I recommend you resolve to learn how to properly care and feed for factors. The pros far outweigh the cons. Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Here we count how many observations are associated with each continent and, as usual, try to portray that info visually. This makes it much easier to quickly see that African countries are well represented in this dataset. table(gapminder$continent) #&gt; #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 barplot(table(gapminder$continent)) In the figures below, we see how factors can be put to work in figures. The continent factor is easily mapped into “facets” or colors and a legend by the ggplot2 package. Making figures with ggplot2 is covered a bit later. So feel free to just sit back and enjoy these plots or mindlessly copy/paste. ## we exploit the fact that ggplot2 was installed and loaded via the tidyverse p &lt;- ggplot( filter(gapminder, continent != &quot;Oceania&quot;), aes(x = gdpPercap, y = lifeExp) ) # just initializes p &lt;- p + scale_x_log10() # log the x axis the right way p + geom_point() # scatterplot p + geom_point(aes(color = continent)) # map continent to color p + geom_point(alpha = (1 / 3), size = 3) + geom_smooth(lwd = 3, se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; p + geom_point(alpha = (1 / 3), size = 3) + facet_wrap(~continent) + geom_smooth(lwd = 1.5, se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 19.6 Recap Use data frames!!! Use the tidyverse!!! This will provide a special type of data frame called a “tibble” that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Always understand the basic extent of your data frames: number of rows and columns. Understand what flavor the variables are. Use factors!!! But with intention and care. Do basic statistical and visual sanity checking of each variable. Refer to variables by name, e.g., gapminder$lifeExp, not by column number. Your code will be more hardy and readable. "],["shorthappygit.html", "20 RDD: More on GITing Started with Github 20.1 The Basics of GitHub and Git 20.2 Understanding the GitHub flow 20.3  GitHub terms to know 20.4 Half the battle 20.5 Install Git 20.6 Windows 20.7 Introduce yourself to Git 20.8 Install a Git client 20.9  Resources 20.10  Optional next steps", " 20 RDD: More on GITing Started with Github One of the goals for this course is to familiarize yourself with Git and GitHub. This section is a fusion of GitHub Classroom’s github-starter-course and Jenny Bryan’s happygitwithr. I strongly encourage you to check out the unabridged version of happygit as it has so much more detail. Alternatively, if you’d rather not read, this video by Jess Chan of Coder Coder is extremely helpful in getting you started. I just got a recruiter message as a comment on a Github commit... now that&#39;s a first. pic.twitter.com/ON9yBzs9xt&mdash; Dylan Garcia (@_dylanga) February 11, 2022 20.1 The Basics of GitHub and Git 20.1.1 What is Git? Git is a distributed Version Control System (VCS), which means it is a useful tool for easily tracking changes to your code, collaborating, and sharing. With Git you can track the changes you make to your project so you always have a record of what you’ve worked on and can easily revert back to an older version if need be. It also makes working with others easier—groups of people can work together on the same project and merge their changes into one final source! 20.1.2 What is GitHub? GitHub extends Git’s capabilities by providing a web-based interface for collaboration. It’s widely used for code development, allowing multiple people to work together on projects. GitHub hosts your repositories (project folders) and provides tools for managing changes, reviewing code, and collaborating with others. 20.2 Understanding the GitHub flow The GitHub flow is a lightweight workflow that allows you to experiment and collaborate on your projects easily, without the risk of losing your previous work. 20.2.1 Key Terms Repository: Your project’s digital directory on GitHub. Clone: Copying a repository to your computer. Commit: Recording changes to your files in the repository. Push: Uploading local repository changes to GitHub. 20.2.1.1 Repositories A repository is where your project work happens–think of it as your project folder. It contains all of your project’s files and revision history. You can work within a repository alone or invite others to collaborate with you on those files. 20.2.1.2 Cloning When a repository is created with GitHub, it’s stored remotely in the ☁️. You can clone a repository to create a local copy on your computer and then use Git to sync the two. This makes it easier to fix issues, add or remove files, and push larger commits. You can also use the editing tool of your choice as opposed to the GitHub UI. Cloning a repository also pulls down all the repository data that GitHub has at that point in time, including all versions of every file and folder for the project! This can be helpful if you experiment with your project and then realize you liked a previous version more. To learn more about cloning, read “Cloning a Repository”. 20.2.1.3 Committing and pushing Committing and pushing are how you can add the changes you made on your local machine to the remote repository in GitHub. That way your instructor or teammates can see your latest work when you’re ready to share it. You can make a commit when you have made changes to your project that you want to “checkpoint.” You can also add a helpful commit message to remind yourself or your teammates what work you did (e.g. “Added a README with information about our project”). Once you have a commit or multiple commits that you’re ready to add to your repository, you can use the push command to add those changes to your remote repository. Committing and pushing may feel new at first, but we promise you’ll get used to it  20.3  GitHub terms to know 20.3.1 Repositories We mentioned repositories already, they are where your project work happens, but let’s talk a bit more about the details of them! As you work more on GitHub you will have many repositories which may feel confusing at first. Fortunately, your “GitHub dashboard” helps to easily navigate to your repositories and see useful information about them. Make sure you’re logged in to see it! Repositories also contain READMEs. You can add a README file to your repository to tell other people why your project is useful, what they can do with your project, and how they can use it. We are using this README to communicate how to learn Git and GitHub with you.  To learn more about repositories read “Creating, Cloning, and Archiving Repositories and “About README’s”. 20.3.2 Branches You can use branches on GitHub to isolate work that you do not want merged into your final project just yet. Branches allow you to develop features, fix bugs, or safely experiment with new ideas in a contained area of your repository. Typically, you might create a new branch from the default branch of your repository—main. This makes a new working copy of your repository for you to experiment with. Once your new changes have been reviewed by a teammate, or you are satisfied with them, you can merge your changes into the default branch of your repository. To learn more about branching, read “About Branches”. 20.3.3 Forks A fork is another way to copy a repository, but is usually used when you want to contribute to someone else’s project. Forking a repository allows you to freely experiment with changes without affecting the original project and is very popular when contributing to open source software projects! To learn more about forking, read “Fork a repo” 20.3.4 Pull requests When working with branches, you can use a pull request to tell others about the changes you want to make and ask for their feedback. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add more changes if need be. You can add specific people as reviewers of your pull request which shows you want their feedback on your changes! Once a pull request is ready-to-go, it can be merged into your main branch. To learn more about pull requests, read “About Pull Requests”. 20.3.5 Issues Issues are a way to track enhancements, tasks, or bugs for your work on GitHub. Issues are a great way to keep track of all the tasks you want to work on for your project and let others know what you plan to work on. You can also use issues to tell a favorite open source project about a bug you found or a feature you think would be great to add! For larger projects, you can keep track of many issues on a project board. GitHub Projects help you organize and prioritize your work and you can read more about them in this “About Project boards document. You likely won’t need a project board for your assignments, but once you move on to even bigger projects, they’re a great way to organize your team’s work! You can also link together pull requests and issues to show that a fix is in progress and to automatically close the issue when someone merges the pull request. To learn more about issues and linking them to your pull requests, read “About Issues”. 20.3.6 Your user profile Your profile page tells people the story of your work through the repositories you’re interested in, the contributions you’ve made, and the conversations you’ve had. You can also give the world a unique view into who you are with your profile README. You can use your profile to let future employers know all about you! To learn more about your user profile and adding and updating your profile README, read “Managing Your Profile README”. 20.3.7 Using markdown on GitHub You might have noticed already, but you can add some fun styling to your issues, pull requests, and files. “Markdown” is an easy way to style your issues, pull requests, and files with some simple syntax. This can be helpful to organize your information and make it easier for others to read. You can also drop in gifs and images to help convey your point! To learn more about using GitHub’s flavor of markdown, read “Basic Writing and Formatting Syntax”. 20.3.8 Engaging with the GitHub community The GitHub community is vast. There are many types of people who use GitHub in their day to day—students like you, professional developers, hobbyists working on open source projects, and explorers who are just jumping into the world of software development on their own. There are many ways you can interact with the larger GitHub community, but here are three places where you can start. 20.3.8.1 Starring repositories If you find a repository interesting or you want to keep track of it, star it! When you star a repository it’s also used as a signal to surface better recommendations on github.com/explore. If you’d like to get back to your starred repositories you can do so via your user profile. To learn more about starring repositories, read “Saving Repositories with Stars”. 20.3.8.2 Following users You can follow people on GitHub to receive notifications about their activity and discover projects in their communities. When you follow a user, their public GitHub activity will show up on your dashboard so you can see all the cool things they are working on. To learn more about following users, read “Following People”. 20.3.8.3 Browsing GitHub Explore GitHub Explore is a great place to do just that … explore :smile: You can find new projects, events, and developers to interact with. You can check out the GitHub Explore website at github.com/explore. The more you interact with GitHub the more tailored your Explore view will be. 20.4 Half the battle Getting all the necessary software installed, configured, and playing nicely together is honestly half the battle when first adopting Git. Brace yourself for some pain. The upside is that you can give yourself a pat on the back once you get through this. And you WILL get through this. You will find far more resources for how to use Git than for installation and configuration. Why? The experts … Have been doing this for years. It’s simply not hard for them anymore. Probably use some flavor of Unix. They may secretly (or not so secretly) take pride in neither using nor knowing Windows. Get more satisfaction and reward for thinking and writing about Git concepts and workflows than Git installation. In their defense, it’s hard to write installation instructions. Failures can be specific to an individual OS or even individual computer. 20.4.1 Free private repos GitHub offers free unlimited private repositories for all users. These free private repositories support up to three external collaborators, making them a perfect place for your personal projects, for job applications, and testing things out before making your project open source. Go ahead and register your free account NOW and then pursue any special offer that applies to you: Students, faculty, and educational/research staff: GitHub Education. GitHub “Organizations” can be extremely useful for courses or research/lab groups, where you need some coordination across a set of repos and users. Official nonprofit organizations and charities: GitHub for Good 20.5 Install Git You need Git, so you can use it at the command line and so RStudio can call it. If there’s any chance it’s installed already, verify that, rejoice, and skip this step. Otherwise, find installation instructions below for your operating system. Install Git: Essential for interacting with GitHub. Installation instructions are available for Windows, macOS, and Linux/Unix. 20.5.1 Git already installed? Go to the shell (More info on shell from Jenny Bryan). Enter which git to request the path to your Git executable: which git and git --version to see its version: git --version If you are successful, that’s great! You have Git already. No need to install! Move on. If, instead, you see something more like git: command not found, keep reading. macOS users might get an immediate offer to install command line developer tools. Yes, you should accept! Click “Install” and read more below. 20.6 Windows Option 1 (highly recommended): Install Git for Windows, also known as msysgit or “Git Bash”, to get Git in addition to some other useful tools, such as the Bash shell. Yes, all those names are totally confusing, but you might encounter them elsewhere and I want you to be well-informed. We like this because Git for Windows leaves the Git executable in a conventional location, which will help you and other programs, e.g. RStudio, find it and use it. This also supports a transition to more expert use, because the “Git Bash” shell will be useful as you venture outside of R/RStudio. NOTE: When asked about “Adjusting your PATH environment”, make sure to select “Git from the command line and also from 3rd-party software”. Otherwise, we believe it is good to accept the defaults. Note that RStudio for Windows prefers for Git to be installed below C:/Program Files and this appears to be the default. This implies, for example, that the Git executable on my Windows system is found at C:/Program Files/Git/bin/git.exe. Unless you have specific reasons to otherwise, follow this convention. This also leaves you with a Git client, though not a very good one. So check out Git clients the Jenny recommends FYI, this appears to be equivalent to what you would download from here: https://git-scm.com/download/. Additional approaches for Windows can be found here 20.6.1 macOS Although I (Mason) have limited knowledge about the inner workings of mac, I do know of quantitative psychologists who use macs with R, including Bill Revelle – author of the psych package and Full Professor at Northwestern. Option 1 (highly recommended): Install the Xcode command line tools (not all of Xcode), which includes Git. Go to the shell and enter one of these commands to elicit an offer to install developer command line tools: git --version git config Accept the offer! Click on “Install”. Here’s another way to request this installation, more directly: xcode-select --install We just happen to find this Git-based trigger apropos. Note also that, after upgrading macOS, you might need to re-do the above and/or re-agree to the Xcode license agreement. We have seen this cause the RStudio Git pane to disappear on a system where it was previously working. Use commands like those above to tickle Xcode into prompting you for what it needs, then restart RStudio. Option 2 (recommended): Install Git from here: http://git-scm.com/downloads. This option arguably sets you up the best for the future. It will certainly get you the latest version of Git of all approaches described here. The GitHub home for the macOS installer is here: https://github.com/timcharper/git_osx_installer. At that link, you can find more info if something goes wrong or you are working on an old version of macOS. Additional approaches for macOS can be found here 20.7 Introduce yourself to Git In the shell (More info on shell from Jenny Bryan): git config --global user.name &#39;Jane Doe&#39; git config --global user.email &#39;jane@example.com&#39; git config --global --list substituting your name and the email associated with your GitHub account. The usethis package offers an alternative approach. You can set your Git user name and email from within R: ## install if needed (do this exactly once): ## install.packages(&quot;usethis&quot;) library(usethis) use_git_config( user.name = &quot;Jane Doe&quot;, user.email = &quot;jane@example.org&quot; ) 20.7.1 More about git config An easy way to get into a shell from RStudio is Tools &gt; Terminal or Tools &gt; Shell. (More info on shell from Jenny Bryan). Special Windows gotchas: If you are struggling on Windows, consider there are different types of shell and you might be in the wrong one. You want to be in a “Git Bash” shell, as opposed to Power Shell or the legacy cmd.exe command prompt. This might also be a reason to do this configuration via the usethis package in R. What user name should you give to Git? This name does not have to be your GitHub user name, although it can be. Another good option is your actual first name and last name. If you commit from different machines, sometimes people work that info into the user name. Your commits will be labeled with this user name, so make it informative to potential collaborators and future you. What email should you give to Git? This email must be the email associated with your GitHub account. These commands return nothing. You can check that Git understood what you typed by looking at the output of git config --global --list. 20.7.2 Configure the Git editor Another Git option that many people eventually configure is the editor. At some point, you will fail to give Git what it wants in terms of a commit message and it will kick you into an editor. This can be distressing, if it’s not your editor of choice and you don’t even know how to save and quit. You can enforce your will with something along these lines: git config --global core.editor &quot;emacs&quot; Substitute your preferred editor for \"emacs\" here. Software Carpentry’s Git lesson has a comprehensive listing of the exact git config command needed for many combinations of OS and editor. 20.8 Install a Git client Although having a git client is optional, I highly recommend it for the same reasons as I recommend having Rstudio. Learning to use version control can be rough at first. I found the use of a GUI – as opposed to the command line – extremely helpful when I was getting started. I call this sort of helper application a Git client. It’s really a Git(Hub) client because it also helps you interact with GitHub or other remotes. 20.8.1 What is a Git client? Why would you want one? “Git” is really just a collection of individual commands you execute in the shell. This interface is not appealing for everyone. Some may prefer to do Git operations via a client with a graphical interface. Git and your Git client are not the same thing, just like R and RStudio are not the same thing. A Git client and an integrated development environment, such as RStudio, are not necessary to use Git or R, respectively. But they make the experience more pleasant because they reduce the amount of command line bullshittery and provide a richer visual representation of the current state. RStudio offers a very basic Git client via its Git pane. I use this often for simple operations, but you probably want another, more powerful one as well. Fair warning: for some tasks, you must use the command line. But the more powerful your Git client is, the less often this happens. The visual overview given by your Git client can also be invaluable for understanding the current state of things, even when preparing calls to command line Git. Fantastic news: because all of the clients are just forming and executing Git commands on your behalf, you don’t have to pick one. You can literally do one operation from the command line, do another from RStudio, and another from SourceTree, one after the other, and it just works. Very rarely, both clients will scan the repo at the same time and you’ll get an error message about .git/index.lock. Try the operation again at least once before doing any further troubleshooting. 20.8.2 A picture is worth a thousand words Here’s a screenshot of SourceTree (see below) open to the repository for this site. You get a nice graphical overview of the recent commit history, branches, and diffs, as well as a GUI that facilitates the most common Git operations. SourceTree screenshot In contrast, here’s a shell session where I’ve used command line Git to access some of the same information. Command line Git Which do you prefer? 20.8.3 No one is giving out Git Nerd merit badges Work with Git in whatever way makes you most effective. Feel free to revisit your approach over time or to use different approaches for different tasks. No one can tell whether you use the command line or a GUI when they look at your Git history or your GitHub repo. If your Git life happens on your own computer, there is no reason to deny yourself a GUI if that’s what you like. If you prefer working in the shell or if you frequently log into a remote server, then it makes sense to prioritize building Git skills at the command line. Do whatever works for you, but don’t do anything for the sake of purity or heroism. 20.8.4 Recommended Git clients Install a Git client: GitHub offers a free Git(Hub) client, GitHub Desktop, for Windows and macOS. GitHub Desktop is aimed at beginners who want the most useful features of Git front and center. The flipside is that it may not support some of the more advanced workflows exposed by the clients above. At present, this client is what I, Mason, mostly use. GitKraken is a free, powerful Git(Hub) client that is Jenny Bryan’s current favorite. It’s especially exciting because it works on Windows, macOS, and Linux. This is great news, especially for long-suffering Linux users who have previously had very few options. SourceTree is another free client that Jenny highly recommends, at least on Windows 1. It was her first and most beloved Git client, but she eventually had to give it up on macOS, due to a long-standing bug re: leaking file handles that they will clearly never fix. Jenny Bryan still uses SourceTree on Windows. Others that I have heard positive reviews for: magit, for Emacs nerds GitUp SmartGit git-cola Browse even more Git(Hub) clients. 20.9  Resources A short video explaining what GitHub is Git and GitHub learning resources Understanding the GitHub flow How to use GitHub branches Interactive Git training materials GitHub’s Learning Lab Education community forum GitHub community forum 20.10  Optional next steps Create a new markdown file in this repository. Let them know what you learned and what you are still confused about! Experiment with different styles! Create your profile README. Let the world know a little bit more about you! What are you interested in learning? What are you working on? What’s your favorite hobby? Learn more about creating your profile README in the document, “Managing Your Profile README”. Go to your user dashboard and create a new repository. Experiment with the features within that repository to familiarize yourself with them. Let github know what you liked or didn’t like about the content of this course. What would you like to see more of? What would be interesting or helpful to your learning journey? During installation and registration, you’ll need to create a free Atlassian Bitbucket account and link that to a free Atlassian Bitbucket Cloud account. Also, feel free to uncheck the checkbox about installing Mercurial (another version control system), unless you feel you need it.↩︎ "],["lab02.html", "21 Lab: Global plastic waste Learning goals Getting started Warm up Exercises Wrapping up", " 21 Lab: Global plastic waste Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab, we focus on data from 2010. Additionally, National Geographic ran a data visualization communication contest on plastic waste as seen here. Learning goals Visualizing numerical and categorical data and interpreting visualizations Recreating visualizations Getting more practice using with R, RStudio, Git, and GitHub Getting started Go to the GitHub course organization and locate the assignment repo template, which should be named lab-02-plastic-waste. If you’re in the right place, it should look like the following. Fork or use the template to make your own repo, and then clone it in RStudio. First, open the R Markdown document lab-02.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. Packages Like always, we’ll use the tidyverse package for this analysis. Run the following code in the Console to load this package. library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 Data The dataset for this assignment can be found as a csv file in the data folder of your repository. You can read it in using the following. plastic_waste &lt;- read.csv(&quot;data/plastic-waste.csv&quot;) The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder Warm up Recall that RStudio is divided into four panes. Without looking, can you name them all and briefly describe their purpose? Verify that the dataset has loaded into the Environment. How many observations are in the dataset? Clicking on the dataset in the Environment will allow you to inspect it more carefully. Alternatively, you can type View(plastic_waste) into the Console to do this. Hint: If you’re not sure, run the command ?NA which will lead you to the documentation. Have a quick look at the data and notice that there are cells taking the value NA – what does this mean? Exercises Let’s start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_histogram(binwidth = 0.2) ## Warning: Removed 51 rows containing non-finite outside the scale range ## (`stat_bin()`). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. plastic_waste %&gt;% filter(plastic_waste_per_cap &gt; 3.5) ## code entity continent year gdp_per_cap plastic_waste_per_cap ## 1 TTO Trinidad and Tobago North America 2010 31260.91 3.6 ## mismanaged_plastic_waste_per_cap mismanaged_plastic_waste coastal_pop ## 1 0.19 94066 1358433 ## total_pop ## 1 1341465 Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. 1.1. Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? NOTE: From this point onwards, the plots and the output of the code are not displayed in the lab instructions, but you can and should run the code and view the results yourself. Another way of visualizing numerical data is using density plots. ggplot( data = plastic_waste, aes(x = plastic_waste_per_cap) ) + geom_density() And compare distributions across continents by coloring density curves by continent. ggplot( data = plastic_waste, mapping = aes( x = plastic_waste_per_cap, color = continent ) ) + geom_density() The resulting plot may be a little difficult to read, so let’s also fill the curves in with colors as well. ggplot( data = plastic_waste, mapping = aes( x = plastic_waste_per_cap, color = continent, fill = continent ) ) + geom_density() The overlapping colors make it difficult to tell what’s happening with the distributions. The first plotted in continents get covered by continents plotted over them. We can change the transparency level of the fill color to help with this problem. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so you just need to try a few. ggplot( data = plastic_waste, mapping = aes( x = plastic_waste_per_cap, color = continent, fill = continent ) ) + geom_density(alpha = 0.7) This plot still doesn’t look great… 2.1. Recreate the density plots above using a different (lower) alpha level that works better for displaying the density curves for all continents. 2.2. Explain why we defined the color and fill of the curves by mapping aesthetics of the plot but we defined the alpha level as a characteristic of the plotting geom.  ✅ ⬆️ Now is a good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. And yet another way to visualize this relationship is using side-by-side box plots. ggplot( data = plastic_waste, mapping = aes( x = continent, y = plastic_waste_per_cap ) ) + geom_boxplot() 3.1. Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? Remember: We use geom_point() to make scatterplots. 4.1. Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. 4.2. Color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? 4.3. Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?  ✅ ⬆️ Now is another good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Wrapping up If you still have some time left, move on to the remaining exercises below. Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the data are not only represented with points on the plot but also a smooth curve. The term “smooth” should help you pick which geom to use. 5.1. Recreate the following plot, and interpret what you see in context of the data.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work. Once you’re done, check to make sure your latest changes are on GitHub and readable.2 (In previous versions of this lab, there was an automated check to see if for your R Markdown document knitted properly. It required using github actions, which made this lab a tad too complicated.)↩︎ "],["welcome-to-the-tidyverse.html", "22 Welcome to the tidyverse! 22.1 Module Materials 22.2 Estimated Video Length", " 22 Welcome to the tidyverse! Welcome to the world of data wrangling and the tidyverse! This module is designed to guide you through the essential process of transforming and reshaping raw data into a more usable format. As you’ll soon discover, data wrangling is a crucial step in the data pipeline and plays a vital role in ensuring that your data are of high quality and ready for analysis. You’re in for an exciting journey as you learn about your data and discover how to avoid biases and misunderstandings due to poor data quality. The best part is, getting started is easy! Simply watch the instructional videos and follow along with the accompanying notes. The video playlist for this module can be found here, and the slides used in the videos are available in the slides repo. To start watching the videos, simply turn to the next page and dive in! 22.1 Module Materials Slides from Lectures Tidy data Grammar of data wrangling Hands on Data Wrangling Working with Multiple Data Frames Suggested Readings All subchapters of this module, including Introduction to dplyr Merges on Github R4DS Data Wrangling, including Tidy Data Pipes Activities Hotels! Lab Nobel Laureates 22.2 Estimated Video Length No of videos : 8 Average length of video : 14 minutes, 11 seconds Total length of playlist : 1 hour, 53 minutes, 34 seconds "],["lecture-tidy-data.html", "23 Lecture: Tidy data 23.1 Data structures in R", " 23 Lecture: Tidy data You can follow along with the slides here if they do not appear below. 23.1 Data structures in R "],["lecture-grammar-of-data-wrangling.html", "24 Lecture: Grammar of data wrangling 24.1 Piping", " 24 Lecture: Grammar of data wrangling You can follow along with the slides here if they do not appear below. 24.1 Piping "],["dplyr_intro.html", "25 Introduction to dplyr 25.1 Think before you create excerpts of your data 25.2 Use filter() to subset data row-wise 25.3 Meet the new pipe operator 25.4 Use select() to subset the data on variables or columns 25.5 Revel in the convenience 25.6 Pure, predictable, pipeable", " 25 Introduction to dplyr dplyr is a package for data manipulation, developed by Hadley Wickham and Romain Francois. It is built to be fast, highly expressive, and open-minded about how your data is stored. It is installed as part of the tidyverse meta-package and, as a core package, it is among those loaded via library(tidyverse). dplyr’s roots are in an earlier package called plyr, which implements the “split-apply-combine” strategy for data analysis (Hadley Wickham 2011b). Where plyr covers a diverse set of inputs and outputs (e.g., arrays, data frames, lists), dplyr has a laser-like focus on data frames or, in the tidyverse, “tibbles”. dplyr is a package-level treatment of the ddply() function from plyr, because “data frame in, data frame out” proved to be so incredibly important. Have no idea what I’m talking about? Not sure if you care? If you use these base R functions: subset(), apply(), [sl]apply(), tapply(), aggregate(), split(), do.call(), with(), within(), then you should keep reading. Also, if you use for() loops a lot, you might enjoy learning other ways to iterate over rows or groups of rows or variables in a data frame. 25.0.1 Load dplyr and gapminder I choose to load the tidyverse, which will load dplyr, among other packages we’ll use incidentally below. library(tidyverse) Also load gapminder. library(gapminder) 25.0.2 Say hello to the gapminder tibble The gapminder data frame is a special kind of data frame: a tibble. gapminder #&gt; # A tibble: 1,704 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ℹ 1,694 more rows It’s tibble-ness is why we get nice compact printing. For a reminder of the problems with base data frame printing, go type iris in the R Console or, better yet, print a data frame to screen that has lots of columns. Note how gapminder’s class() includes tbl_df; the “tibble” terminology is a nod to this. class(gapminder) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Some functions, like print(), know about tibbles and do something special. However, other functions do not, like summary(). In those cases, the tibble will be treated the same as a regular data frame because every tibble is also a regular data frame. To turn any data frame into a tibble, use as_tibble(): as_tibble(iris) #&gt; # A tibble: 150 × 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # ℹ 140 more rows 25.1 Think before you create excerpts of your data If you feel the urge to store a little snippet of your data: (canada &lt;- gapminder[241:252, ]) #&gt; # A tibble: 12 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Canada Americas 1952 68.8 14785584 11367. #&gt; 2 Canada Americas 1957 70.0 17010154 12490. #&gt; 3 Canada Americas 1962 71.3 18985849 13462. #&gt; 4 Canada Americas 1967 72.1 20819767 16077. #&gt; 5 Canada Americas 1972 72.9 22284500 18971. #&gt; 6 Canada Americas 1977 74.2 23796400 22091. #&gt; 7 Canada Americas 1982 75.8 25201900 22899. #&gt; 8 Canada Americas 1987 76.9 26549700 26627. #&gt; 9 Canada Americas 1992 78.0 28523502 26343. #&gt; 10 Canada Americas 1997 78.6 30305843 28955. #&gt; 11 Canada Americas 2002 79.8 31902268 33329. #&gt; 12 Canada Americas 2007 80.7 33390141 36319. Stop and ask yourself … Do I want to create mini datasets for each level of some factor (or unique combination of several factors) … in order to compute or graph something? If YES, use proper data aggregation techniques or faceting in ggplot2 – don’t subset the data. Or, more realistic, only subset the data as a temporary measure while you develop your elegant code for computing on or visualizing these data subsets. If NO, then maybe you really do need to store a copy of a subset of the data. But seriously consider whether you can achieve your goals by simply using the subset = argument of, e.g., the lm() function, to limit computation to your excerpt of choice. Lots of functions offer a subset = argument! Copies and excerpts of your data clutter your workspace, invite mistakes, and sow general confusion. Avoid whenever possible. Reality can also lie somewhere in between. You will find the workflows presented below can help you accomplish your goals with minimal creation of temporary, intermediate objects. 25.2 Use filter() to subset data row-wise filter() takes logical expressions and returns the rows for which all are TRUE. filter(gapminder, lifeExp &lt; 29) #&gt; # A tibble: 2 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Rwanda Africa 1992 23.6 7290203 737. filter(gapminder, country == &quot;Rwanda&quot;, year &gt; 1979) #&gt; # A tibble: 6 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Rwanda Africa 1982 46.2 5507565 882. #&gt; 2 Rwanda Africa 1987 44.0 6349365 848. #&gt; 3 Rwanda Africa 1992 23.6 7290203 737. #&gt; 4 Rwanda Africa 1997 36.1 7212583 590. #&gt; 5 Rwanda Africa 2002 43.4 7852401 786. #&gt; 6 Rwanda Africa 2007 46.2 8860588 863. filter(gapminder, country %in% c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) #&gt; # A tibble: 24 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ℹ 14 more rows Compare with some base R code to accomplish the same things: gapminder[gapminder$lifeExp &lt; 29, ] ## repeat `gapminder`, [i, j] indexing is distracting subset(gapminder, country == &quot;Rwanda&quot;) ## almost same as filter; quite nice actually Under no circumstances should you subset your data the way I did at first: excerpt &lt;- gapminder[241:252, ] Why is this approach a terrible idea? It is not self-documenting. What is so special about rows 241 through 252? It is fragile. This line of code will produce different results if someone changes the row order of gapminder, e.g. sorts the data earlier in the script. filter(gapminder, country == &quot;Canada&quot;) This call explains itself and is fairly robust. 25.3 Meet the new pipe operator Before we go any further, we should exploit the new pipe operator that the tidyverse imports from the magrittr package by Stefan Bache. This is going to change your data analytical life. You no longer need to enact multi-operation commands by nesting them inside each other, like so many Russian nesting dolls. This new syntax leads to code that is much easier to write and to read. Here’s what it looks like: %&gt;%. The RStudio keyboard shortcut: Ctrl+Shift+M (Windows), Cmd+Shift+M (Mac). Let’s demo, then I’ll explain. gapminder %&gt;% head() #&gt; # A tibble: 6 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. This code is equivalent to head(gapminder). The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side – literally, drops it in as the first argument. Never fear, you can still specify other arguments to this function! To see the first 3 rows of gapminder, we could say head(gapminder, 3) or this: gapminder %&gt;% head(3) #&gt; # A tibble: 3 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. I’ve advised you to think “gets” whenever you see the assignment operator, &lt;-. Similarly, you should think “then” whenever you see the pipe operator, %&gt;%. You are probably not impressed yet, but the magic will soon happen. 25.4 Use select() to subset the data on variables or columns Back to dplyr…. Use select() to subset the data on variables or columns. Here’s a conventional call: select(gapminder, year, lifeExp) #&gt; # A tibble: 1,704 × 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 #&gt; 2 1957 30.3 #&gt; 3 1962 32.0 #&gt; 4 1967 34.0 #&gt; 5 1972 36.1 #&gt; 6 1977 38.4 #&gt; 7 1982 39.9 #&gt; 8 1987 40.8 #&gt; 9 1992 41.7 #&gt; 10 1997 41.8 #&gt; # ℹ 1,694 more rows And here’s the same operation, but written with the pipe operator and piped through head(): gapminder %&gt;% select(year, lifeExp) %&gt;% head(4) #&gt; # A tibble: 4 × 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 #&gt; 2 1957 30.3 #&gt; 3 1962 32.0 #&gt; 4 1967 34.0 Think: “Take gapminder, then select the variables year and lifeExp, then show the first 4 rows.” 25.5 Revel in the convenience Here’s the data for Cambodia, but only certain variables: gapminder %&gt;% filter(country == &quot;Cambodia&quot;) %&gt;% select(year, lifeExp) #&gt; # A tibble: 12 × 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 39.4 #&gt; 2 1957 41.4 #&gt; 3 1962 43.4 #&gt; 4 1967 45.4 #&gt; 5 1972 40.3 #&gt; 6 1977 31.2 #&gt; 7 1982 51.0 #&gt; 8 1987 53.9 #&gt; 9 1992 55.8 #&gt; 10 1997 56.5 #&gt; 11 2002 56.8 #&gt; 12 2007 59.7 and what a typical base R call would look like: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] #&gt; # A tibble: 12 × 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 39.4 #&gt; 2 1957 41.4 #&gt; 3 1962 43.4 #&gt; 4 1967 45.4 #&gt; 5 1972 40.3 #&gt; 6 1977 31.2 #&gt; 7 1982 51.0 #&gt; 8 1987 53.9 #&gt; 9 1992 55.8 #&gt; 10 1997 56.5 #&gt; 11 2002 56.8 #&gt; 12 2007 59.7 25.6 Pure, predictable, pipeable We’ve barely scratched the surface of dplyr but I want to point out key principles you may start to appreciate. If you’re new to R or “programming with data”, feel free skip this section and move on. dplyr’s verbs, such as filter() and select(), are what’s called pure functions. To quote from Wickham’s Advanced R Programming book (2015): The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return. In fact, these verbs are a special case of pure functions: they take the same flavor of object as input and output. Namely, a data frame or one of the other data receptacles dplyr supports. And finally, the data is always the very first argument of the verb functions. These design choices are deliberate. When combined with the new pipe operator, the result is a highly effective, low friction domain-specific language for data analysis. Furthermore, cheatsheets are really great resources to learn functions. Click the link to download it! Go to the next section, for more dplyr! "],["handson.html", "26 Hands on Data Wrangling 26.1 Working with a single data frame 26.2 Activity 04: Hotels 26.3 ODD: Single table dplyr functions", " 26 Hands on Data Wrangling 26.1 Working with a single data frame You can follow along with the slides here) if they do not appear below. 26.2 Activity 04: Hotels You can find the materials for the Hotels activity here. The compiled version should look something like the following… 26.3 ODD: Single table dplyr functions This optional deep dive covers more detail on dplyr. Previously, on Introduction to dplyr, we used two very important verbs and an operator: filter() for subsetting data with row logic select() for subsetting data variable- or column-wise the pipe operator %&gt;%, which feeds the LHS as the first argument to the expression on the RHS We also discussed dplyr’s role inside the tidyverse and tibbles: dplyr is a core package in the tidyverse meta-package. Because we often make incidental usage of the others, we will load dplyr and the others via library(tidyverse). The tidyverse embraces a special flavor of data frame, called a tibble. The gapminder dataset is stored as a tibble. This time, we’re going to dive a bit deeper into dplyr. 26.3.1 Load dplyr and gapminder I choose to load the tidyverse, which will load dplyr, among other packages we use incidentally below. library(tidyverse) Also load gapminder. library(gapminder) 26.3.2 Create a copy of gapminder We’re going to make changes to the gapminder tibble. To eliminate any fear that you’re damaging the data that comes with the package, we create an explicit copy of gapminder for our experiments. (my_gap &lt;- gapminder) #&gt; # A tibble: 1,704 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ℹ 1,694 more rows Pay close attention to when we evaluate statements but let the output just print to screen: ## let output print to screen, but do not store my_gap %&gt;% filter(country == &quot;Canada&quot;) … versus when we assign the output to an object, possibly overwriting an existing object. ## store the output as an R object my_precious &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) 26.3.3 Use mutate() to add new variables Imagine we wanted to recover each country’s GDP. After all, the Gapminder data has a variable for population and GDP per capita. Let’s multiply them together. mutate() is a function that defines and inserts new variables into a tibble. You can refer to existing variables by name. my_gap %&gt;% mutate(gdp = pop * gdpPercap) #&gt; # A tibble: 1,704 × 7 #&gt; country continent year lifeExp pop gdpPercap gdp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. #&gt; # ℹ 1,694 more rows Hmmmm … those GDP numbers are almost uselessly large and abstract. Consider the advice of Randall Munroe of xkcd: One thing that bothers me is large numbers presented without context… ‘If I added a zero to this number, would the sentence containing it mean something different to me?’ If the answer is ‘no,’ maybe the number has no business being in the sentence in the first place.” Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. But what if I reported GDP per capita, relative to some benchmark country. Since Canada is my adopted home, I’ll go with that. I need to create a new variable that is gdpPercap divided by Canadian gdpPercap, taking care that I always divide two numbers that pertain to the same year. How I achieve this: Filter down to the rows for Canada. Create a new temporary variable in my_gap: Extract the gdpPercap variable from the Canadian data. Replicate it once per country in the dataset, so it has the right length. Divide raw gdpPercap by this Canadian figure. Discard the temporary variable of replicated Canadian gdpPercap. ctib &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) ## this is a semi-dangerous way to add this variable ## I&#39;d prefer to join on year, but we haven&#39;t covered joins yet my_gap &lt;- my_gap %&gt;% mutate( tmp = rep(ctib$gdpPercap, nlevels(country)), gdpPercapRel = gdpPercap / tmp, tmp = NULL ) Note that, mutate() builds new variables sequentially so you can reference earlier ones (like tmp) when defining later ones (like gdpPercapRel). Also, you can get rid of a variable by setting it to NULL. How could we sanity check that this worked? The Canadian values for gdpPercapRel better all be 1! my_gap %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(country, year, gdpPercapRel) #&gt; # A tibble: 12 × 3 #&gt; country year gdpPercapRel #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Canada 1952 1 #&gt; 2 Canada 1957 1 #&gt; 3 Canada 1962 1 #&gt; 4 Canada 1967 1 #&gt; 5 Canada 1972 1 #&gt; 6 Canada 1977 1 #&gt; 7 Canada 1982 1 #&gt; 8 Canada 1987 1 #&gt; 9 Canada 1992 1 #&gt; 10 Canada 1997 1 #&gt; 11 Canada 2002 1 #&gt; 12 Canada 2007 1 I perceive Canada to be a “high GDP” country, so I predict that the distribution of gdpPercapRel is located below 1, possibly even well below. Check your intuition! summary(my_gap$gdpPercapRel) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.01 0.06 0.17 0.33 0.45 9.53 The relative GDP per capita numbers are, in general, well below 1. We see that most of the countries covered by this dataset have substantially lower GDP per capita, relative to Canada, across the entire time period. Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that you’ve done what meant to. Prepare to be horrified. 26.3.4 Use arrange() to row-order data in a principled way arrange() reorders the rows in a data frame. Imagine you wanted this data ordered by year then country, as opposed to by country then year. my_gap %&gt;% arrange(year, country) #&gt; # A tibble: 1,704 × 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 #&gt; 2 Albania Europe 1952 55.2 1282697 1601. 0.141 #&gt; 3 Algeria Africa 1952 43.1 9279525 2449. 0.215 #&gt; 4 Angola Africa 1952 30.0 4232095 3521. 0.310 #&gt; 5 Argentina Americas 1952 62.5 17876956 5911. 0.520 #&gt; 6 Australia Oceania 1952 69.1 8691212 10040. 0.883 #&gt; 7 Austria Europe 1952 66.8 6927772 6137. 0.540 #&gt; 8 Bahrain Asia 1952 50.9 120447 9867. 0.868 #&gt; 9 Bangladesh Asia 1952 37.5 46886859 684. 0.0602 #&gt; 10 Belgium Europe 1952 68 8730405 8343. 0.734 #&gt; # ℹ 1,694 more rows Or maybe you want just the data from 2007, sorted on life expectancy? my_gap %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) #&gt; # A tibble: 142 × 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Swaziland Africa 2007 39.6 1.13e6 4513. 0.124 #&gt; 2 Mozambique Africa 2007 42.1 2.00e7 824. 0.0227 #&gt; 3 Zambia Africa 2007 42.4 1.17e7 1271. 0.0350 #&gt; 4 Sierra Leone Africa 2007 42.6 6.14e6 863. 0.0237 #&gt; 5 Lesotho Africa 2007 42.6 2.01e6 1569. 0.0432 #&gt; 6 Angola Africa 2007 42.7 1.24e7 4797. 0.132 #&gt; 7 Zimbabwe Africa 2007 43.5 1.23e7 470. 0.0129 #&gt; 8 Afghanistan Asia 2007 43.8 3.19e7 975. 0.0268 #&gt; 9 Central African Republ… Africa 2007 44.7 4.37e6 706. 0.0194 #&gt; 10 Liberia Africa 2007 45.7 3.19e6 415. 0.0114 #&gt; # ℹ 132 more rows Oh, you’d like to sort on life expectancy in descending order? Then use desc(). my_gap %&gt;% filter(year == 2007) %&gt;% arrange(desc(lifeExp)) #&gt; # A tibble: 142 × 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Japan Asia 2007 82.6 127467972 31656. 0.872 #&gt; 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 1.09 #&gt; 3 Iceland Europe 2007 81.8 301931 36181. 0.996 #&gt; 4 Switzerland Europe 2007 81.7 7554661 37506. 1.03 #&gt; 5 Australia Oceania 2007 81.2 20434176 34435. 0.948 #&gt; 6 Spain Europe 2007 80.9 40448191 28821. 0.794 #&gt; 7 Sweden Europe 2007 80.9 9031088 33860. 0.932 #&gt; 8 Israel Asia 2007 80.7 6426679 25523. 0.703 #&gt; 9 France Europe 2007 80.7 61083916 30470. 0.839 #&gt; 10 Canada Americas 2007 80.7 33390141 36319. 1 #&gt; # ℹ 132 more rows I advise that your analyses NEVER rely on rows or variables being in a specific order. But it’s still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order. 26.3.5 Use rename() to rename variables When I first cleaned this Gapminder excerpt, I was a camelCase person, but now I’m all about snake_case. So I am vexed by the variable names I chose when I cleaned this data years ago. Let’s rename some variables! my_gap %&gt;% rename( life_exp = lifeExp, gdp_percap = gdpPercap, gdp_percap_rel = gdpPercapRel ) #&gt; # A tibble: 1,704 × 7 #&gt; country continent year life_exp pop gdp_percap gdp_percap_rel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. 0.0657 #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. 0.0634 #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. 0.0520 #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. 0.0390 #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. 0.0356 #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. 0.0427 #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. 0.0320 #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. 0.0246 #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. 0.0219 #&gt; # ℹ 1,694 more rows I did NOT assign the post-rename object back to my_gap because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to my_gap, in a data preparation script, and proceed with the new variable names. 26.3.6 select() can rename and reposition variables You’ve seen simple use of select(). There are two tricks you might enjoy: select() can rename the variables you request to keep. select() can be used with everything() to hoist a variable up to the front of the tibble. my_gap %&gt;% filter(country == &quot;Burundi&quot;, year &gt; 1996) %&gt;% select(yr = year, lifeExp, gdpPercap) %&gt;% select(gdpPercap, everything()) #&gt; # A tibble: 3 × 3 #&gt; gdpPercap yr lifeExp #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 463. 1997 45.3 #&gt; 2 446. 2002 47.4 #&gt; 3 430. 2007 49.6 everything() is one of several helpers for variable selection. Read its help to see the rest. 26.3.7 group_by() is a mighty weapon I have found friends and family collaborators love to ask seemingly innocuous questions like, “which country experienced the sharpest 5-year drop in life expectancy?”. In fact, that is a totally natural question to ask. But if you are using a language that doesn’t know about data, it’s an incredibly annoying question to answer. dplyr offers powerful tools to solve this class of problem: group_by() adds extra structure to your dataset – grouping information – which lays the groundwork for computations within the groups. summarize() takes a dataset with \\(n\\) observations, computes requested summaries, and returns a dataset with 1 observation. Window functions take a dataset with \\(n\\) observations and return a dataset with \\(n\\) observations. mutate() and summarize() will honor groups. You can also do very general computations on your groups with do(), though elsewhere in this course, I advocate for other approaches that I find more intuitive, using the purrr package. Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease. 26.3.7.1 Counting things up Let’s start with simple counting. How many observations do we have per continent? my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n()) #&gt; # A tibble: 5 × 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 Let us pause here to think about the tidyverse. You could get these same frequencies using table() from base R. table(gapminder$continent) #&gt; #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 str(table(gapminder$continent)) #&gt; &#39;table&#39; int [1:5(1d)] 624 300 396 360 24 #&gt; - attr(*, &quot;dimnames&quot;)=List of 1 #&gt; ..$ : chr [1:5] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; ... But the object of class table that is returned makes downstream computation a bit fiddlier than you’d like. For example, it’s too bad the continent levels come back only as names and not as a proper factor, with the original set of levels. This is an example of how the tidyverse smooths transitions where you want the output of step i to become the input of step i + 1. The tally() function is a convenience function that knows to count rows. It honors groups. my_gap %&gt;% group_by(continent) %&gt;% tally() #&gt; # A tibble: 5 × 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 The count() function is an even more convenient function that does both grouping and counting. my_gap %&gt;% count(continent) #&gt; # A tibble: 5 × 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 What if we wanted to add the number of unique countries for each continent? You can compute multiple summaries inside summarize(). Use the n_distinct() function to count the number of distinct countries within each continent. my_gap %&gt;% group_by(continent) %&gt;% summarize( n = n(), n_countries = n_distinct(country) ) #&gt; # A tibble: 5 × 3 #&gt; continent n n_countries #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Africa 624 52 #&gt; 2 Americas 300 25 #&gt; 3 Asia 396 33 #&gt; 4 Europe 360 30 #&gt; 5 Oceania 24 2 26.3.7.2 General summarization The functions you’ll apply within summarize() include classical statistical summaries, like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). Remember they are functions that take \\(n\\) inputs and distill them down into 1 output. Although this may be statistically ill-advised, let’s compute the average life expectancy by continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(avg_lifeExp = mean(lifeExp)) #&gt; # A tibble: 5 × 2 #&gt; continent avg_lifeExp #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Africa 48.9 #&gt; 2 Americas 64.7 #&gt; 3 Asia 60.1 #&gt; 4 Europe 71.9 #&gt; 5 Oceania 74.3 summarize_at() applies the same summary function(s) to multiple variables. Let’s compute average and median life expectancy and GDP per capita by continent by year…but only for 1952 and 2007. my_gap %&gt;% filter(year %in% c(1952, 2007)) %&gt;% group_by(continent, year) %&gt;% summarize_at(vars(lifeExp, gdpPercap), list(~ mean(.), ~ median(.))) #&gt; # A tibble: 10 × 6 #&gt; # Groups: continent [5] #&gt; continent year lifeExp_mean gdpPercap_mean lifeExp_median gdpPercap_median #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa 1952 39.1 1253. 38.8 987. #&gt; 2 Africa 2007 54.8 3089. 52.9 1452. #&gt; 3 Americas 1952 53.3 4079. 54.7 3048. #&gt; 4 Americas 2007 73.6 11003. 72.9 8948. #&gt; 5 Asia 1952 46.3 5195. 44.9 1207. #&gt; 6 Asia 2007 70.7 12473. 72.4 4471. #&gt; 7 Europe 1952 64.4 5661. 65.9 5142. #&gt; 8 Europe 2007 77.6 25054. 78.6 28054. #&gt; 9 Oceania 1952 69.3 10298. 69.3 10298. #&gt; 10 Oceania 2007 80.7 29810. 80.7 29810. Let’s focus just on Asia. What are the minimum and maximum life expectancies seen by year? my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize( min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp) ) #&gt; # A tibble: 12 × 3 #&gt; year min_lifeExp max_lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 65.4 #&gt; 2 1957 30.3 67.8 #&gt; 3 1962 32.0 69.4 #&gt; 4 1967 34.0 71.4 #&gt; 5 1972 36.1 73.4 #&gt; 6 1977 31.2 75.4 #&gt; 7 1982 39.9 77.1 #&gt; 8 1987 40.8 78.7 #&gt; 9 1992 41.7 79.4 #&gt; 10 1997 41.8 80.7 #&gt; 11 2002 42.1 82 #&gt; 12 2007 43.8 82.6 Of course it would be much more interesting to see which country contributed these extreme observations. Is the minimum (maximum) always coming from the same country? We tackle that with window functions shortly. 26.3.8 Grouped mutate Sometimes you don’t want to collapse the \\(n\\) rows for each group into one row. You want to keep your groups, but compute within them. 26.3.8.1 Computing with group-wise summaries Let’s make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use mutate() to make a new variable. The first() function extracts the first value from a vector. Notice that first() is operating on the vector of life expectancies within each country group. my_gap %&gt;% group_by(country) %&gt;% select(country, year, lifeExp) %&gt;% mutate(lifeExp_gain = lifeExp - first(lifeExp)) %&gt;% filter(year &lt; 1963) #&gt; # A tibble: 426 × 4 #&gt; # Groups: country [142] #&gt; country year lifeExp lifeExp_gain #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 1952 28.8 0 #&gt; 2 Afghanistan 1957 30.3 1.53 #&gt; 3 Afghanistan 1962 32.0 3.20 #&gt; 4 Albania 1952 55.2 0 #&gt; 5 Albania 1957 59.3 4.05 #&gt; 6 Albania 1962 64.8 9.59 #&gt; 7 Algeria 1952 43.1 0 #&gt; 8 Algeria 1957 45.7 2.61 #&gt; 9 Algeria 1962 48.3 5.23 #&gt; 10 Angola 1952 30.0 0 #&gt; # ℹ 416 more rows Within country, we take the difference between life expectancy in year \\(i\\) and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers. 26.3.8.2 Window functions Window functions take \\(n\\) inputs and give back \\(n\\) outputs. Furthermore, the output depends on all the values. So rank() is a window function but log() is not. Here we use window functions based on ranks and offsets. Let’s revisit the worst and best life expectancies in Asia over time, but retaining info about which country contributes these extreme values. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) %&gt;% filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) %&gt;% arrange(year) %&gt;% print(n = Inf) #&gt; # A tibble: 24 × 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1952 Israel 65.4 #&gt; 3 1957 Afghanistan 30.3 #&gt; 4 1957 Israel 67.8 #&gt; 5 1962 Afghanistan 32.0 #&gt; 6 1962 Israel 69.4 #&gt; 7 1967 Afghanistan 34.0 #&gt; 8 1967 Japan 71.4 #&gt; 9 1972 Afghanistan 36.1 #&gt; 10 1972 Japan 73.4 #&gt; 11 1977 Cambodia 31.2 #&gt; 12 1977 Japan 75.4 #&gt; 13 1982 Afghanistan 39.9 #&gt; 14 1982 Japan 77.1 #&gt; 15 1987 Afghanistan 40.8 #&gt; 16 1987 Japan 78.7 #&gt; 17 1992 Afghanistan 41.7 #&gt; 18 1992 Japan 79.4 #&gt; 19 1997 Afghanistan 41.8 #&gt; 20 1997 Japan 80.7 #&gt; 21 2002 Afghanistan 42.1 #&gt; 22 2002 Japan 82 #&gt; 23 2007 Afghanistan 43.8 #&gt; 24 2007 Japan 82.6 We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldn’t it be nice to have one row per year? How did that actually work? First, I store and view a partial that leaves off the filter() statement. All of these operations should be familiar. asia &lt;- my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) asia #&gt; # A tibble: 396 × 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1957 Afghanistan 30.3 #&gt; 3 1962 Afghanistan 32.0 #&gt; 4 1967 Afghanistan 34.0 #&gt; 5 1972 Afghanistan 36.1 #&gt; 6 1977 Afghanistan 38.4 #&gt; 7 1982 Afghanistan 39.9 #&gt; 8 1987 Afghanistan 40.8 #&gt; 9 1992 Afghanistan 41.7 #&gt; 10 1997 Afghanistan 41.8 #&gt; # ℹ 386 more rows Now we apply a window function – min_rank(). Since asia is grouped by year, min_rank() operates within mini-datasets, each for a specific year. Applied to the variable lifeExp, min_rank() returns the rank of each country’s observed life expectancy. FYI, the min part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order. For concreteness, I use mutate() to actually create these variables, even though I dropped this in the solution above. Let’s look at a bit of that. asia %&gt;% mutate( le_rank = min_rank(lifeExp), le_desc_rank = min_rank(desc(lifeExp)) ) %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Japan&quot;, &quot;Thailand&quot;), year &gt; 1995) #&gt; # A tibble: 9 × 5 #&gt; # Groups: year [3] #&gt; year country lifeExp le_rank le_desc_rank #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1997 Afghanistan 41.8 1 33 #&gt; 2 2002 Afghanistan 42.1 1 33 #&gt; 3 2007 Afghanistan 43.8 1 33 #&gt; 4 1997 Japan 80.7 33 1 #&gt; 5 2002 Japan 82 33 1 #&gt; 6 2007 Japan 82.6 33 1 #&gt; 7 1997 Thailand 67.5 12 22 #&gt; 8 2002 Thailand 68.6 12 22 #&gt; 9 2007 Thailand 70.6 12 22 Afghanistan tends to present 1’s in the le_rank variable, Japan tends to present 1’s in the le_desc_rank variable and other countries, like Thailand, present less extreme ranks. You can understand the original filter() statement now: filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) These two sets of ranks are formed on-the-fly, within year group, and filter() retains rows with rank less than 2, which means … the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max. If we had wanted just the min OR the max, an alternative approach using top_n() would have worked. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% arrange(year) %&gt;% group_by(year) %&gt;% # top_n(1, wt = lifeExp) ## gets the min top_n(1, wt = desc(lifeExp)) ## gets the max #&gt; # A tibble: 12 × 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1957 Afghanistan 30.3 #&gt; 3 1962 Afghanistan 32.0 #&gt; 4 1967 Afghanistan 34.0 #&gt; 5 1972 Afghanistan 36.1 #&gt; 6 1977 Cambodia 31.2 #&gt; 7 1982 Afghanistan 39.9 #&gt; 8 1987 Afghanistan 40.8 #&gt; 9 1992 Afghanistan 41.7 #&gt; 10 1997 Afghanistan 41.8 #&gt; 11 2002 Afghanistan 42.1 #&gt; 12 2007 Afghanistan 43.8 26.3.9 Grand Finale So let’s answer that “simple” question: which country experienced the sharpest 5-year drop in life expectancy? Recall that this excerpt of the Gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints. At this point, that’s just too easy, so let’s do it by continent while we’re at it. my_gap %&gt;% select(country, year, continent, lifeExp) %&gt;% group_by(continent, country) %&gt;% ## within country, take (lifeExp in year i) - (lifeExp in year i - 1) ## positive means lifeExp went up, negative means it went down mutate(le_delta = lifeExp - lag(lifeExp)) %&gt;% ## within country, retain the worst lifeExp change = smallest or most negative summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %&gt;% ## within continent, retain the row with the lowest worst_le_delta top_n(-1, wt = worst_le_delta) %&gt;% arrange(worst_le_delta) #&gt; `summarise()` has grouped output by &#39;continent&#39;. You can override using the #&gt; `.groups` argument. #&gt; # A tibble: 5 × 3 #&gt; # Groups: continent [5] #&gt; continent country worst_le_delta #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Africa Rwanda -20.4 #&gt; 2 Asia Cambodia -9.10 #&gt; 3 Americas El Salvador -1.51 #&gt; 4 Europe Montenegro -1.46 #&gt; 5 Oceania Australia 0.170 Ponder that for a while. The subject matter and the code. Mostly you’re seeing what genocide looks like in dry statistics on average life expectancy. Break the code into pieces, starting at the top, and inspect the intermediate results. That’s certainly how I was able to write such a thing. These commands do not leap fully formed out of anyone’s forehead – they are built up gradually, with lots of errors and refinements along the way. I’m not even sure it’s a great idea to do so much manipulation in one fell swoop. Is the statement above really hard for you to read? If yes, then by all means break it into pieces and make some intermediate objects. Your code should be easy to write and read when you’re done. In later tutorials, we’ll explore more of dplyr, such as operations based on two datasets. 26.3.10 Resources dplyr official stuff: Package home on CRAN. Note there are several vignettes, with the introduction being the most relevant right now. The one on window functions will also be interesting to you now. Development home on GitHub. RStudio Data Transformation Cheat Sheet, covering dplyr. Remember you can get to these via Help &gt; Cheatsheets. Data transformation chapter of R for Data Science (Hadley Wickham and Grolemund 2016). Excellent slides on pipelines and dplyr by TJ Mahr, talk given to the Madison R Users Group. Blog post Hands-on dplyr tutorial for faster data manipulation in R by Data School, that includes a link to an R Markdown document and links to videos. "],["working-with-multiple-data-frames.html", "27 Working with multiple data frames 27.1 Case Studies in Joining", " 27 Working with multiple data frames You can follow along with the slides here if they do not appear below. 27.1 Case Studies in Joining You can follow along with the slides here if they do not appear below. "],["merges.html", "28 ODD: Merges and Collaboration 28.1 Learning goal 28.2 Merges and merge conflicts 28.3 Merge conflict activity 28.4 Tips for collaborating via GitHub", " 28 ODD: Merges and Collaboration This optional deep dive is about merge conflicts. You are welcome to try it with your classmates. 28.1 Learning goal Collaborating on GitHub and resolving merge conflicts 28.2 Merges and merge conflicts We’re going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes you’ll run into merge conflicts. So our first task today is to walk you through a merge conflict! Pushing to a repo replaces the code on GitHub with the code you have on your computer. If a collaborator has made a change to your repo on GitHub that you haven’t incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborator’s work! So you need to explicitly “merge” your collaborator’s work before you can push. If your and your collaborator’s changes are in different files or in different parts of the same file, git merges the work for you automatically when you *pull*. If you both changed the same part of a file, git will produce a **merge conflict** because it doesn’t know how which change you want to keep and which change you want to overwrite. Git will put conflict markers in your code that look like: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD See also: [dplyr documentation](https://dplyr.tidyverse.org/) ======= See also [ggplot2 documentation](https://ggplot2.tidyverse.org/) &gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4 The ===s separate your changes (top) from their changes (bottom). Note that on top you see the word HEAD, which indicates that these are your changes. And at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f). This is the hash (a unique identifier) of the commit your collaborator made with the conflicting change. Your job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then you can stage and commit the result. 28.3 Merge conflict activity 28.3.1 Setup Clone the repo and open the .Rmd file. Assign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers. 28.3.2 Let’s cause a merge conflict Our goal is to see two different types of merges: first we’ll see a type of merge that git can’t figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention. Doing this will require some tight choreography, so pay attention! Take turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you won’t be able to resist the urge to touch your computer when it’s not your turn, we recommend putting your hands in your pockets or sitting on them! Before starting: everyone should have the repo cloned and know which role number(s) they are. Role 1: Change the team name to your actual team name. Knit, commit, push.  Make sure the previous role has finished before moving on to the next step. Role 2: Change the team name to some other word. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by editing the document to choose the correct/preferred change. Knit. Click the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes. Commit and push.  Make sure the previous role has finished before moving on to the next step. Role 3: Change the a label of the first code chunk Knit, commit, push. You should get an error. Pull. No merge conflicts should occur, but you should see a message about merging. Now push.  Make sure the previous role has finished before moving on to the next step. Role 4: Change the label of the first code chunk to something other than previous role did. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.  Make sure the previous role has finished before moving on to the next step. Everyone: Pull, and observe the changes in your document. 28.4 Tips for collaborating via GitHub Always pull first before you start working. Resolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict. Knit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve. If you find yourself in a situation that is difficult to resolve, ask questions as soon as possible. Don’t let it linger and get bigger. "],["lab03.html", "29 Lab: Nobel laureates Learning goals Lab prep Getting started Exercises But of those US-based Nobel laureates, many were born in other countries Interested in how Buzzfeed made their visualizations?", " 29 Lab: Nobel laureates In January 2017, Buzzfeed published an article titled “These Nobel Prize Winners Show Why Immigration Is So Important for American Science”. The article highlighted that many U.S.-based Nobel laureates in the sciences were born outside the United States, emphasizing the role of immigration in scientific innovation. In this lab, you will analyze Nobel laureate data to replicate and extend some of the visualizations from the article. Learning goals By completing this lab, you will: Practice replicating published results Perform data wrangling and visualization using tidyverse Lab prep Task: Read the Buzzfeed article titled These Nobel Prize Winners Show Why Immigration Is So Important For American Science. We will be replicating this analysis in the lab. So it’s pretty important that you’re familiar with it ahead of time. Getting started To begin this lab, navigate to our course’s GitHub organization and locate the repository named lab-03-nobel-laureates. Follow these steps: Repository Setup: Forking: If you are working individually, fork the repository to create a personal copy under your GitHub account. Template Copying: If you prefer not to fork, you can also use the repository as a template. This option is particularly useful if you’re working as a group. Clone and Open: Cloning: Use GitHub Desktop or the command line to clone the repository to your local machine. Opening the Project: Open the cloned directory in RStudio by clicking on the lab-03-nobel-laureates.Rproj file. Open and Knit the R Markdown File: Open the lab-03.Rmd file in RStudio. Knit the document to ensure all code chunks run correctly and the markdown file (lab-03.md) is generated without errors. Update YAML Header: Modify the YAML header in the R Markdown document to include your name as the author and ensure other settings like date and output format are correctly specified. Knit the document again to see the updates in the output. Commit and Push Changes: Commit your changes with a meaningful message using Git, such as “Updated YAML header with author details.” Push these changes to your GitHub repository to ensure they are properly uploaded and visible online. Verify Changes on GitHub: Visit your GitHub repository to confirm that your changes appear correctly in both the .Rmd and .md files. This ensures that your setup is correct and that you are ready to proceed with the lab exercises. Packages We’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console: library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 Data The dataset for this assignment can be found as a csv (comma separated values) file in the data folder of your repository. You can read it in using the following. nobel &lt;- read_csv(&quot;data/nobel.csv&quot;) The variable descriptions are as follows: id: ID number firstname: First name of laureate surname: Surname of laureate year: Year prize won category: Category of prize affiliation: Affiliation of laureate city: City of laureate in prize year country: Country of laureate in prize year born_date: Birth date of laureate died_date: Death date of laureate gender: Gender of laureate born_city: City where laureate was born born_country: Country where laureate was born born_country_code: Code of country where laureate was born died_city: City where laureate died died_country: Country where laureate died died_country_code: Code of country where laureate died overall_motivation: Overall motivation for recognition share: Number of other winners award is shared with motivation: Motivation for recognition In a few cases, the name of the city/country changed after laureate was given (e.g., in 1975, Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`. born_country_original: Original country where laureate was born born_city_original: Original city where laureate was born died_country_original: Original country where laureate died died_city_original: Original city where laureate died city_original: Original city where laureate lived at the time of winning the award country_original: Original country where laureate lived at the time of winning the award Exercises Get to know your data How many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent? There are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results. Create a new data frame called nobel_living that filters for laureates who are still alive (their died_date is NA), as well as laureates for whom country is available laureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender) Confirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Most living Nobel laureates were based in the US when they won their prizes … says the Buzzfeed article. Let’s see if that’s true. First, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\". Note: We can achieve the same result using the fct_other() function we’ve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R. nobel_living &lt;- nobel_living %&gt;% mutate( country_us = if_else(country == &quot;USA&quot;, &quot;USA&quot;, &quot;Other&quot;) ) Next, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics. Note: Technically, the Nobel Prize in Economics is a memorial prize. It was established in 1968. If you want to annoy an economist, point that distinction out to them… More info here if you want to learn some more. nobel_living_science &lt;- nobel_living %&gt;% filter(category %in% c(&quot;Physics&quot;, &quot;Medicine&quot;, &quot;Chemistry&quot;, &quot;Economics&quot;)) For the next exercise, work with the nobel_living_science data frame you created above. You’ll need to define this data frame in your R Markdown document, even though the next exercise doesn’t explicitly ask you to do so. Create a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data. Your visualization should be faceted by category. For each facet you should have two bars, one for winners in the US and one for Other. Flip the coordinates so the bars are horizontal, not vertical.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. But of those US-based Nobel laureates, many were born in other countries Hint: You should be able to cheat borrow from code you used earlier to create the country_us variable. Create a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US? Add a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences. Your final visualization should contain a facet for each category. Within each facet, there should be a bar for whether the laureate won the award in the US or not. Each bar should have segments for whether the laureate was born in the US or not.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Here’s where those immigrant Nobelists were born Note: Your bar plot won’t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published. In a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work. Now go back through your write up to make sure you’ve answered all questions and all of your R chunks are properly labeled. Interested in how Buzzfeed made their visualizations? The plots in the Buzzfeed article are called waffle plots. You can find the code used for making these plots in Buzzfeed’s GitHub repo (yes, they have one!) here. You’re not expected to recreate them as part of your assignment, but you’re welcomed to do so for fun! "],["welcome-to-data-diving-with-types.html", "30 Welcome to Data Diving with Types 30.1 Module Materials 30.2 Estimated Video Length", " 30 Welcome to Data Diving with Types Welcome to the exciting world of data types and data importing! This module is your guide to understanding the basics of these important topics. Get ready to dive deep into the world of data and learn everything you need to know about data types and data importing. To get started, simply watch the videos and follow along with the notes. The video playlist for this module can be found here and the slides used in the videos are available in the slides repo. So, grab a notebook and pen and turn to the next page to start learning! With each video, you’ll gain a deeper understanding of data types and data importing, and by the end of this module, you’ll be a pro. Let’s get started! 30.1 Module Materials Slides from Lectures Data types and recoding Importing data Suggested Readings All subchapters of this module R4DS Wrangle Data Transformation Data Importing Activities Hotels, again! Data Import Lab Visualizing spatial data 30.2 Estimated Video Length No of videos : 9 Average length of video : 8 minutes, 28 seconds Total length of playlist : 1 hour, 16 minutes, 15 seconds "],["data-types-and-recoding.html", "31 Data types and recoding 31.1 Why should you care about data types? 31.2 Data types 31.3 Special Values 31.4 Data classes 31.5 Working with factors 31.6 Working with Dates 31.7 Working with Dates", " 31 Data types and recoding You can follow along with the slides here if they do not appear below. That’s when you realize you can educate those you work with to give you better data, better questions to answer, etc., so that you can give them better insights. You have more power than you think!&mdash; Isabella R. Ghement (@IsabellaGhement) December 22, 2021 Wonderful visual illustration of data types in #rstats! Image credits:https://t.co/ESrtYBwtNmFor further reading, see:https://t.co/nrR59hLniV#DataScience pic.twitter.com/iO0key2mnh&mdash; Indrajeet Patil (इंद्रजीत पाटील) (@patilindrajeets) December 29, 2021 31.1 Why should you care about data types? 31.2 Data types 31.2.1 Another Hotels Activity You can find the materials for the Hotels activity here. The compiled version should look something like the following… 31.3 Special Values 31.4 Data classes 31.5 Working with factors 31.5.1 (An) Another Hotels Activity You can find the materials for the Hotels activity here. The compiled version should look something like the following… 31.6 Working with Dates 31.7 Working with Dates "],["importing-data.html", "32 Importing data 32.1 Importing data! 32.2 Importing and Variable Types 32.3 Vroom", " 32 Importing data 32.1 Importing data! You can follow along with the slides here if they do not appear below. 32.2 Importing and Variable Types You can follow along with the slides here if they do not appear below. 32.2.1 More Activity You can find the materials for the Nobels and sales activities here. 32.3 Vroom You can follow along with the slides [here][#d12_import] if they do not appear below. "],["import-export.html", "33 Writing and reading files 33.1 File I/O overview 33.2 Let’s Begin 33.3 Compute something worthy of export 33.4 Clean up 33.5 Resources", " 33 Writing and reading files This deep dive has been adapted from Jenny Bryan’s Stat545. 33.1 File I/O overview For the most part, we’ve been working with (p)reprocessed data, like the Gapminder data from the gapminder data package or data from any of the labs. In other words, we haven’t been explicitly writing any data or derived results to file. In real life (and in this class), you’ll have to bring rectangular data into and out of R. Sometimes you’ll need to do same for non-rectangular objects. How do you do this? What issues should you think about? 33.1.1 Data import mindset Data import generally feels one of two ways: “Surprise me!” You probably have to adopt this attitude when you first get a dataset. You are just happy to import without an error. You start to explore. You discover flaws in the data and/or the import. You address them. Lather, rinse, repeat. “Another day in paradise.” This attitude is when you bring in a tidy dataset you have maniacally cleaned in one or more cleaning scripts. There should be no surprises. You should express your expectations about the data in formal assertions at the very start of these downstream scripts. In the second case, and as the first cases progresses, you actually know a lot about how the data is / should be. My main import advice: use the arguments of your import function to get as far as you can, as fast as possible. Novice code often has a great deal of unnecessary post import fussing around. Read the docs for the import functions and take maximum advantage of the arguments to control the import. 33.1.2 Data export mindset There will be many occasions when you need to write data from R. Two main examples: a tidy ready-to-analyze dataset that you heroically created from messy data a numerical result from data aggregation or modelling or statistical inference First tip: today’s outputs are tomorrow’s inputs. Think back on all the pain you have suffered importing data and don’t inflict such pain on yourself! Second tip: don’t be too cute or clever. A plain text file that is readable by a human being in a text editor should be your default until you have actual proof that this will not work. Reading and writing to exotic or proprietary formats will be the first thing to break in the future or on a different computer. It also creates barriers for anyone who has a different toolkit than you do. Be software-agnostic. Aim for future-proof and forgetfu-proof. How does this approach fit with our emphasis on dynamic reporting via R Markdown? There is a time and place for everything. There are projects and documents where the scope and personnel will allow you to geek out with knitr and R Markdown. But there are lots of good reasons why (parts of) an analysis should not (only) be embedded in a dynamic report. Maybe you are just doing data cleaning to produce a valid input dataset. Maybe you are making a small but crucial contribution to a giant multi-author paper. Etc. Also remember there are other tools and workflows for making something reproducible. I’m looking at you, make. 33.2 Let’s Begin 33.2.1 Load the tidyverse The main package we will be using is readr, which provides drop-in substitute functions for read.table() and friends. However, to make some points about data export and import, it is nice to reorder factor levels. For that, we will use the forcats package, which is also included in the tidyverse package. library(tidyverse) 33.2.2 Locate the Gapminder data We could load the data from the package as usual, but instead we will load it from tab delimited file. The gapminder package includes the data normally found in the gapminder data frame as a .tsv. So let’s get the path to that file on your system using the fs package. library(fs) (gap_tsv &lt;- path_package(&quot;gapminder&quot;, &quot;extdata&quot;, &quot;gapminder.tsv&quot;)) #&gt; C:/Users/smaso/AppData/Local/R/win-library/4.4/gapminder/extdata/gapminder.tsv 33.2.3 Bring rectangular data in The workhorse data import function of readr is read_delim(). Here we’ll use a variant, read_tsv(), that anticipates tab-delimited data: gapminder &lt;- read_tsv(gap_tsv) #&gt; Rows: 1704 Columns: 6 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;\\t&quot; #&gt; chr (2): country, continent #&gt; dbl (4): year, lifeExp, pop, gdpPercap #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. str(gapminder, give.attr = FALSE) #&gt; spc_tbl_ [1,704 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ country : chr [1:1704] &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghani&quot;.. #&gt; $ continent: chr [1:1704] &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; ... #&gt; $ year : num [1:1704] 1952 1957 1962 1967 1972 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : num [1:1704] 8425333 9240934 10267083 11537966 13079460 ... #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... For full flexibility re: specifying the delimiter, you can always use readr::read_delim(). There’s a similar convenience wrapper for comma-separated values, read_csv(). The most noticeable difference between the readr functions and base is that readr does NOT convert strings to factors by default. In the grand scheme of things, this default behavior is better, although we go ahead and convert them to factor here. Do not be deceived – in general, you will do less post-import fussing if you use readr. gapminder &lt;- gapminder %&gt;% mutate( country = factor(country), continent = factor(continent) ) str(gapminder) #&gt; tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : num [1:1704] 1952 1957 1962 1967 1972 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : num [1:1704] 8425333 9240934 10267083 11537966 13079460 ... #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... 33.2.3.1 Bring rectangular data in – summary Default to readr::read_delim() and friends. Use the arguments! The Gapminder data is too clean and simple to show off the great features of readr, so I encourage you to check out the part of the introduction vignette on column types. There are many variable types that you will be able to parse correctly upon import, thereby eliminating a great deal of post-import fussing. 33.3 Compute something worthy of export We need compute something worth writing to file. Let’s create a country-level summary of maximum life expectancy. gap_life_exp &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% summarize(life_exp = max(lifeExp)) %&gt;% ungroup() #&gt; `summarise()` has grouped output by &#39;country&#39;. You can override using the #&gt; `.groups` argument. gap_life_exp #&gt; # A tibble: 142 × 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 #&gt; 7 Austria Europe 79.8 #&gt; 8 Bahrain Asia 75.6 #&gt; 9 Bangladesh Asia 64.1 #&gt; 10 Belgium Europe 79.4 #&gt; # ℹ 132 more rows The gap_life_exp data frame is an example of an intermediate result that we want to store for the future and for downstream analyses or visualizations. 33.3.1 Write rectangular data out The workhorse export function for rectangular data in readr is write_delim() and friends. Let’s use write_csv() to get a comma-delimited file. write_csv(gap_life_exp, &quot;gap_life_exp.csv&quot;) Let’s look at the first few lines of gap_life_exp.csv. If you’re following along, you should be able to open this file or, in a shell, use head() on it. country,continent,life_exp Afghanistan,Asia,43.828 Albania,Europe,76.423 Algeria,Africa,72.301 Angola,Africa,42.731 Argentina,Americas,75.32 This output is pretty decent looking, though there is no visible alignment or separation into columns. Had we used the base function read.csv(), we would be seeing rownames and lots of quotes, unless we had explicitly shut that down. Nicer default behavior is the main reason we are using readr::write_csv() over write.csv(). It’s not really fair to complain about the lack of visible alignment. Remember we are “writing data for computers”. If you really want to browse around the file, use View() in RStudio or open it in Microsoft Excel (!) but don’t succumb to the temptation to start doing artisanal data manipulations there … get back to R and construct commands that you can re-run the next 15 times you import/clean/aggregate/export the same dataset. Trust me, it will happen. If I had one thing to tell biologists learning bioinformatics, it would be &quot;write code for humans, write data for computers&quot;.&mdash; Vince Buffalo (@vsbuffalo) July 20, 2013 33.3.2 Invertibility It turns out these self-imposed rules are often in conflict with one another: Write to plain text files Break analysis into pieces: the output of script i is an input for script i + 1 Be the boss of factors: order the levels in a meaningful, usually non-alphabetical way Avoid duplication of code and data Example: after performing the country-level summarization, we reorder the levels of the country factor, based on life expectancy. This reordering operation is conceptually important and must be embodied in R commands stored in a script. However, as soon as we write gap_life_exp to a plain text file, that meta-information about the countries is lost. Upon re-import with read_delim() and friends, we are back to alphabetically ordered factor levels. Any measure we take to avoid this loss immediately breaks another one of our rules. So what do I do? I must admit I save (and re-load) R-specific binary files. Right after I save the plain text file. Belt and suspenders! I have toyed with the idea of writing import helper functions for a specific project, that would re-order factor levels in principled ways. They could be defined in one file and called from many. This would also have a very natural implementation within a workflow where each analytic project is an R package. But so far it has seemed too much like yak shaving. I’m intrigued by a recent discussion of putting such information in YAML frontmatter (see Martin Fenner’s blog post Using YAML frontmatter with CSV). 33.3.3 Reordering the levels of the country factor I reorder the country factor levels according to the life expectancy summary we’ve already computed. head(levels(gap_life_exp$country)) # alphabetical order #&gt; [1] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Angola&quot; &quot;Argentina&quot; #&gt; [6] &quot;Australia&quot; gap_life_exp &lt;- gap_life_exp %&gt;% mutate(country = fct_reorder(country, life_exp)) head(levels(gap_life_exp$country)) # in increasing order of maximum life expectancy #&gt; [1] &quot;Sierra Leone&quot; &quot;Angola&quot; &quot;Afghanistan&quot; &quot;Liberia&quot; &quot;Rwanda&quot; #&gt; [6] &quot;Mozambique&quot; head(gap_life_exp) #&gt; # A tibble: 6 × 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 Note that the row order of gap_life_exp has not changed. I could choose to reorder the rows of the data frame if, for example, I was about to prepare a table to present to people. But I’m not, so I won’t. 33.3.4 saveRDS() and readRDS() If you have a data frame AND you have exerted yourself to rationalize the factor levels, you have my blessing to save it to file in a way that will preserve this hard work upon re-import. Use saveRDS(). saveRDS(gap_life_exp, &quot;gap_life_exp.rds&quot;) saveRDS() serializes an R object to a binary file. It’s not a file you will able to open in an editor, diff nicely with Git(Hub), or share with non-R friends. It’s a special purpose, limited use function that I use in specific situations. The opposite of saveRDS() is readRDS(). You must assign the return value to an object. I highly recommend you assign back to the same name as before. Why confuse yourself?!? rm(gap_life_exp) gap_life_exp #&gt; Error: object &#39;gap_life_exp&#39; not found gap_life_exp &lt;- readRDS(&quot;gap_life_exp.rds&quot;) gap_life_exp #&gt; # A tibble: 142 × 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 #&gt; 7 Austria Europe 79.8 #&gt; 8 Bahrain Asia 75.6 #&gt; 9 Bangladesh Asia 64.1 #&gt; 10 Belgium Europe 79.4 #&gt; # ℹ 132 more rows saveRDS() has more arguments, in particular compress for controlling compression, so read the help for more advanced usage. It is also very handy for saving non-rectangular objects, like a fitted regression model, that took a nontrivial amount of time to compute. You will eventually hear about save() + load() and even save.image(). You may even see them in documentation and tutorials, but don’t be tempted. Just say no. These functions encourage unsafe practices, like storing multiple objects together and even entire workspaces. There are legitimate uses of these functions, but not in your typical data analysis. 33.3.5 Retaining factor levels upon re-import Here is a concrete demonstration of how non-alphabetical factor level order is lost with write_delim() / read_delim() workflows but maintained with saveRDS() / readRDS(). (country_levels &lt;- tibble(original = head(levels(gap_life_exp$country)))) #&gt; # A tibble: 6 × 1 #&gt; original #&gt; &lt;chr&gt; #&gt; 1 Sierra Leone #&gt; 2 Angola #&gt; 3 Afghanistan #&gt; 4 Liberia #&gt; 5 Rwanda #&gt; 6 Mozambique write_csv(gap_life_exp, &quot;gap_life_exp.csv&quot;) saveRDS(gap_life_exp, &quot;gap_life_exp.rds&quot;) rm(gap_life_exp) head(gap_life_exp) # will cause error! proving gap_life_exp is really gone #&gt; Error: object &#39;gap_life_exp&#39; not found gap_via_csv &lt;- read_csv(&quot;gap_life_exp.csv&quot;) %&gt;% mutate(country = factor(country)) #&gt; Rows: 142 Columns: 3 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (2): country, continent #&gt; dbl (1): life_exp #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. gap_via_rds &lt;- readRDS(&quot;gap_life_exp.rds&quot;) country_levels &lt;- country_levels %&gt;% mutate( via_csv = head(levels(gap_via_csv$country)), via_rds = head(levels(gap_via_rds$country)) ) country_levels #&gt; # A tibble: 6 × 3 #&gt; original via_csv via_rds #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Sierra Leone Afghanistan Sierra Leone #&gt; 2 Angola Albania Angola #&gt; 3 Afghanistan Algeria Afghanistan #&gt; 4 Liberia Angola Liberia #&gt; 5 Rwanda Argentina Rwanda #&gt; 6 Mozambique Australia Mozambique Note how the original, post-reordering country factor levels are restored using the saveRDS() / readRDS() strategy but revert to alphabetical ordering using write_csv() / read_csv(). 33.3.6 dput() and dget() One last method of saving and restoring data deserves a mention: dput() and dget(). dput() offers this odd combination of features: it creates a plain text representation of an R object which still manages to be quite opaque. If you use the file = argument, dput() can write this representation to file but you won’t be tempted to actually read that thing. dput() creates an R-specific-but-not-binary representation. Let’s try it out. ## first restore gap_life_exp with our desired country factor level order gap_life_exp &lt;- readRDS(&quot;gap_life_exp.rds&quot;) dput(gap_life_exp, &quot;gap_life_exp-dput.txt&quot;) Now let’s look at the first few lines of the file gap_life_exp-dput.txt. structure(list(country = structure(c(3L, 107L, 74L, 2L, 98L, 138L, 128L, 102L, 49L, 125L, 26L, 56L, 96L, 47L, 75L, 85L, 18L, 12L, 37L, 24L, 133L, 13L, 16L, 117L, 84L, 82L, 53L, 9L, 28L, 120L, 22L, 104L, 114L, 109L, 115L, 23L, 73L, 97L, 66L, 71L, 15L, 29L, 20L, 122L, 134L, 40L, 35L, 123L, 38L, 126L, 60L, 25L, 7L, 39L, 59L, 141L, 86L, 140L, 51L, 63L, 64L, 52L, 121L, 135L, 132L, Huh? Don’t worry about it. Remember we are “writing data for computers”. The partner function dget() reads this representation back in. gap_life_exp_dget &lt;- dget(&quot;gap_life_exp-dput.txt&quot;) country_levels &lt;- country_levels %&gt;% mutate(via_dput = head(levels(gap_life_exp_dget$country))) country_levels #&gt; # A tibble: 6 × 4 #&gt; original via_csv via_rds via_dput #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Sierra Leone Afghanistan Sierra Leone Sierra Leone #&gt; 2 Angola Albania Angola Angola #&gt; 3 Afghanistan Algeria Afghanistan Afghanistan #&gt; 4 Liberia Angola Liberia Liberia #&gt; 5 Rwanda Argentina Rwanda Rwanda #&gt; 6 Mozambique Australia Mozambique Mozambique Note how the original, post-reordering country factor levels are restored using the dput() / dget() strategy. But why on earth would you ever do this? The main application of this is the creation of highly portable, self-contained minimal examples. For example, if you want to pose a question on a forum or directly to an expert, it might be required or just plain courteous to NOT attach any data files. You will need a monolithic, plain text blob that defines any necessary objects and has the necessary code. dput() can be helpful for producing the piece of code that defines the object. If you dput() without specifying a file, you can copy the return value from Console and paste into a script. Or you can write to file and copy from there or add R commands below. 33.3.7 Other types of objects to use dput() or saveRDS() on My special dispensation to abandon human-readable, plain text files is even broader than I’ve let on. Above, I give my blessing to store data.frames via dput() and/or saveRDS(), when you’ve done some rational factor level re-ordering. The same advice and mechanics apply a bit more broadly: you’re also allowed to use R-specific file formats to save vital non-rectangular objects, such as a fitted nonlinear mixed effects model or a classification and regression tree. 33.4 Clean up We’ve written several files in this tutorial. Some of them are not of lasting value or have confusing filenames. I choose to delete them, while demonstrating some of the many functions R offers for interacting with the filesystem. It’s up to you whether you want to submit this command or not. file.remove(list.files(pattern = &quot;^gap_life_exp&quot;)) #&gt; [1] TRUE TRUE TRUE 33.4.1 Pitfalls of delimited files If a delimited file contains fields where a human being has typed, be proactive! Expect the unexpected. People do all sorts of things. Especially people who aren’t in the business of programming and have never had to compute on text. Claim: a person’s regular expression skill is inversely proportional to the skill required to handle the files they create. Implication: if someone has never heard of regular expressions, prepare for lots of pain working with their files. When the header fields (often, but not always, the variable names) or actual data contain the delimiter, it can lead to parsing and import failures. Two popular delimiters are the comma , and the TAB \\t and humans tend to use these when typing. If you can design this problem away during data capture, such as by using a drop down menu on an input form, by all means do so. Sometimes this is impossible or undesirable and you must deal with fairly free form text. That’s a good time to allow/force text to be protected with quotes, because it will make parsing the delimited file go more smoothly. Sometimes, instead of rigid tab-delimiting, whitespace is used as the delimiter. That is, in fact, the default for both read.table() and write.table(). Assuming you will write/read variable names from the first line (a.k.a. the header in write.table() and read.table()), they must be valid R variable names … or they will be coerced into something valid. So, for these two reasons, it is good practice to use “one word” variable names whenever possible. If you need to evoke multiple words, use snake_case or camelCase to cope. Example: the header entry for the field holding the subject’s last name should be last_name or lastName NOT last name. With the readr package, “column names are left as is, not munged into valid R identifiers (i.e. there is no check.names = TRUE)”. So you can get away with whitespace in variable names and yet I recommend that you do not. 33.5 Resources Data import chapter of R for Data Science by Hadley Wickham and Garrett Grolemund (2016). White et al.’s “Nine simple ways to make it easier to (re)use your data” (2013). First appeared in PeerJ Preprints Published in Ideas in Ecology and Evolution in 2013 Section 4 “Use Standard Data Formats” is especially good reading. Wickham’s paper on tidy data in the Journal of Statistical Software (2014). Available as a PDF here Data Manipulation in R by Phil Spector (2008). Available via SpringerLink Author’s webpage GoogleBooks search 33.5.1 Data Import Activity You can find the materials for the Nobels and sales activities here. "],["odd-data-transformations-and-tukeys-ladder-of-powers.html", "34 ODD: Data Transformations and Tukey’s Ladder of Powers 34.1 Transforming Data: Tukey’s Ladder of Powers 34.2 Introduction to Tukey’s Ladder of Powers 34.3 Vectorizing a function 34.4 Box Cox Transformation", " 34 ODD: Data Transformations and Tukey’s Ladder of Powers This optional deep dive explores data transformations, focusing on Tukey’s Ladder of Powers—an groundbreaking approach for reshaping data distributions and relationships between variables. Our journey through this topic is inspired by Fox (2016) (ch. 4, pp. 28 - 80) 34.1 Transforming Data: Tukey’s Ladder of Powers John Tukey introduced a methodological toolkit, likened to a set of drill bits of varying sizes, for modifying the shapes of distributions and relationships between variables. This section delves into these transformations, particularly focusing on the mathematical formulations that underpin this approach. 34.1.1 Dataset Preparation and Visualization We’ll be using data from Fox (2016)’s book. You may need to download the data manually and save it in the “data” directory within your current working directory. The dataset is available here. You can use this r code to download the data. download.file(&quot;http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/UnitedNations.txt&quot;, &quot;data/UnitedNations.txt&quot;) #if (!require(&quot;p3d&quot;)) install.packages(&quot;p3d&quot;, repos = &quot;http://R-Forge.R-project.org&quot;) library(p3d) #&gt; Warning: package &#39;rgl&#39; was built under R version 4.4.2 #&gt; Warning: package &#39;nlme&#39; was built under R version 4.4.2 library(car) library(latticeExtra) library(gridExtra) # read data un &lt;- read.table(&quot;data/UnitedNations.txt&quot;, header = TRUE)%&gt;% mutate(country = rownames(.)) # Assigning country names Let’s visualize an attempt to make our regression behave. Below are the original and log-transformed relationships with ggplot2. You’ve likely used a log-transformation before, but we’ll be exploring the more general family of transformations. The log transformation has straightened out the data, allowing us to fit a more useful regression line. However, we’re not limited to just a log transformation. We can use an entire family of transformations called Tukey’s Ladder of Powers. 34.2 Introduction to Tukey’s Ladder of Powers John Tukey suggested this simple toolkit, like a set drill bits of varying sizes, to modify the shape of distributions and the shape of relationships between variables. The basic idea stems from the fact that functions of the form \\(y&#39; = y^p, \\quad y &gt; 0\\) have a graph that is concave up if \\(p &gt;1\\), and concave down if \\(0&lt;p&lt;1\\) or \\(p&lt;0\\). Here are some examples to illustrate these concepts: Concave Up (\\(y= x^2\\)): This plot shows a parabolic curve opening upwards, illustrating the concept of a concave up graph where the slope of the tangent line increases as \\(x\\) increases. Concave Down (\\(y= -x^2\\)): Conversely, this plot shows a parabolic curve opening downwards, illustrating a concave down graph where the slope of the tangent line decreases as \\(x\\) increases. We can use this information to modify the shape of a distribution or the shape of a relationship between variables by using a power transformation (\\(p\\)) on our data. 34.2.1 Mathematical Formulation of Tukey’s Ladder of Powers To understand Tukey’s Ladder of Powers, let’s start with the general form of the transformation: \\[y&#39; = \\frac{y^p - 1}{p}\\] where \\(p\\) is the power parameter. This formula helps us adjust the shape of the data distribution. When \\(p\\) is positive, the transformation can help reduce right skewness. When \\(p\\) is negative, it can help reduce left skewness. Special Case: \\(p = 0\\) When \\(p = 0\\), the transformation simplifies to a logarithmic transformation. This is because as \\(p\\) approaches 0, the formula \\(\\frac{y^p - 1}{p}\\) approaches \\(\\ln y\\). This can be understood using a concept from calculus called l’Hôpital’s rule, which is used to evaluate limits of indeterminate forms. For our purposes, we can state that: \\[\\lim_{p \\to 0} \\frac{y^p - 1}{p}= \\lim_{p \\to 0}\\, e^{\\, p \\ln y} \\ln y = \\ln y\\] To make this concept more intuitive, imagine \\(y^p\\) as a function that gets closer and closer to 1 as \\(p\\) gets closer to 0. The subtraction of 1 and division by \\(p\\) in the formula adjust this function to reflect the logarithmic behavior. Negative Powers For negative values of \\(p\\), the formula \\(y = -y^p\\) is used. This formula is concave down. The negative power causes the transformation to be concave down. This helps transform the data in a way that handles extreme values more effectively. 34.2.2 Defining the Transformation Function in R We define a function that produces this transformation. The easy way to define it is: pow &lt;- function(y, p) { if (p == 0) { log(y) } else { (y^p - 1) / p } } # Testing the transformation with a sequence of x values x &lt;- seq(-1, 3, by = .5) # Applying the transformation for different powers of p x_transformed &lt;- tibble( x = x, `p=2` = pow(x, 2), `p=0` = pow(x, 0), `p=-1` = pow(x, -1) ) #&gt; Warning in log(y): NaNs produced # Visualizing the transformed data x_transformed_long &lt;- pivot_longer(x_transformed, cols = starts_with(&quot;p&quot;), names_to = &quot;Power&quot;, values_to = &quot;Transformed&quot;) ggplot(x_transformed_long, aes(x = x, y = Transformed, color = Power)) + geom_line() + labs(title = &quot;Visualization of Power Transformations&quot;, x = &quot;Original x&quot;, y = &quot;Transformed x&quot;) + theme_minimal() #&gt; Warning: Removed 2 rows containing missing values or values outside the scale range #&gt; (`geom_line()`). # test: x &lt;- seq(-1, 3,by = .5) x # note that these transformations are really intended for y &gt; 0 #&gt; [1] -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 pow(x, 2) #&gt; [1] 0.000 -0.375 -0.500 -0.375 0.000 0.625 1.500 2.625 4.000 #pow(x, 0) %&gt;% # name(x) #pow(x, -1) %&gt;% # name(x) %&gt;% # cbind() plot(exp) # easy plotting of a function plot(function(x) pow(x, p = 2)) # anonymous function or &#39;lambda&#39; plot(function(x) pow(x, p = .5), xlim = c(0, 3)) However, this approach works correctly only for a single value of \\(p\\) because the statement if(p == 0) only tests the first element of p. 34.3 Vectorizing a function Most operators in R are vectorized so they work element-wise when their arguments are vectors. When the arguments have incompatible lengths, the shorter argument is recycled to have the same length as the longer one. That is why the following produces sensible results: z &lt;- c(3, 5, 9) z + c(1, 1, 1) #&gt; [1] 4 6 10 z + 1 # 1 is recycled so the result is equivalent to the previous line #&gt; [1] 4 6 10 z + c(1, 2, 3) #&gt; [1] 4 7 12 z + c(1, 2) # recycles but gives a warning #&gt; Warning in z + c(1, 2): longer object length is not a multiple of shorter #&gt; object length #&gt; [1] 4 7 10 z + z #&gt; [1] 6 10 18 z^2 #&gt; [1] 9 25 81 z^z #&gt; [1] 2.70e+01 3.12e+03 3.87e+08 We can use ifelse which works on a vector instead of a single value. pow &lt;- function(y, p) { p &lt;- rep(p, length.out = length(y)) y &lt;- rep(y, length.out = length(p)) ifelse(p == 0, log(y), (y^p - 1) / p) } # To apply the function over vectors of y and p, ensuring vectorized operations: vectorized_pow &lt;- function(y, p) { map2(y, p, pow) } # test: pow(-1:4, c(2, 0, -1, 1, 3)) #&gt; Warning in log(y): NaNs produced #&gt; [1] 0.00 -Inf 0.00 1.00 8.67 7.50 pow(-1:4, 2) #&gt; [1] 0.0 -0.5 0.0 1.5 4.0 7.5 With a bit more work, we can avoid unnecessary evaluations: pow &lt;- function(y, p) { p &lt;- rep(p, length.out = length(y)) y &lt;- rep(y, length.out = length(p)) y[p == 0] &lt;- log(y[p == 0]) y[p != 0] &lt;- (y[p != 0]^p[p != 0] - 1) / p[p != 0] y } # Test: pow(1:10, 0) == log(1:10) #&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE pow(1:10, -1) #&gt; [1] 0.000 0.500 0.667 0.750 0.800 0.833 0.857 0.875 0.889 0.900 pow(1:10, .5) #&gt; [1] 0.000 0.828 1.464 2.000 2.472 2.899 3.292 3.657 4.000 4.325 pow(1:10, -1:8) #&gt; [1] 0.00e+00 6.93e-01 2.00e+00 7.50e+00 4.13e+01 3.24e+02 3.36e+03 4.37e+04 #&gt; [9] 6.83e+05 1.25e+07 Let’s plot this transformation for a range of values of \\(p\\). The value of expand.grid is a data frame whose rows consist of the Cartesian product (i.e. all possible combinations) of its arguments. expand.grid(a = c(&quot;A&quot;, &quot;B&quot;), x = 1:3) #&gt; a x #&gt; 1 A 1 #&gt; 2 B 1 #&gt; 3 A 2 #&gt; 4 B 2 #&gt; 5 A 3 #&gt; 6 B 3 dd &lt;- expand.grid(y = seq(.01, 3, .01), p = c(-2, -1, -.5, 0, .5, 1, 2, 3)) dim(dd) #&gt; [1] 2400 2 head(dd) #&gt; y p #&gt; 1 0.01 -2 #&gt; 2 0.02 -2 #&gt; 3 0.03 -2 #&gt; 4 0.04 -2 #&gt; 5 0.05 -2 #&gt; 6 0.06 -2 some(dd) # 10 rows at random #&gt; y p #&gt; 61 0.61 -2.0 #&gt; 161 1.61 -2.0 #&gt; 412 1.12 -1.0 #&gt; 513 2.13 -1.0 #&gt; 801 2.01 -0.5 #&gt; 1430 2.30 0.5 #&gt; 1625 1.25 1.0 #&gt; 1676 1.76 1.0 #&gt; 2056 2.56 2.0 #&gt; 2100 3.00 2.0 dd$yval &lt;- with(dd, pow(y, p)) xyplot(yval ~ y | factor(p), dd, type = &quot;l&quot;) xyplot(yval ~ y | factor(p), dd, type = &quot;l&quot;, ylim = c(-2, max(dd$yval))) xyplot(yval ~ y, dd, groups = p, type = &quot;l&quot;, ylim = c(-2, max(dd$yval))) xyplot(yval ~ y, dd, groups = p, type = &quot;l&quot;, xlim = c(0, 3), ylim = c(-2, max(dd$yval)) ) gd(8, lwd = 2) # number of colours needed #&gt; Error in gd(8, lwd = 2): could not find function &quot;gd&quot; xyplot(yval ~ y, dd, groups = p, type = &quot;l&quot;, xlim = c(0, 3), ylim = c(-2, max(dd$yval)) ) xyplot(yval ~ y, dd, groups = p, type = &quot;l&quot;, auto.key = list(space = &quot;right&quot;, lines = T, points = F), xlim = c(0, 3), ylim = c(-2, max(dd$yval)) ) It’s much better to have the legend in the same order as the lines in the graph. We can turn p into a factor and reverse its order. dd$po &lt;- factor(dd$p) dd$po &lt;- reorder(dd$po, -dd$p) xyplot(yval ~ y, dd, groups = po, type = &quot;l&quot;, auto.key = list(space = &quot;right&quot;, lines = T, points = F, title = &quot;power&quot;), xlim = c(0, 3), ylim = c(-2, max(dd$yval)) ) From quantile plots: Uniform quantiles… xqplot(un) #&gt; Error in xqplot(un): could not find function &quot;xqplot&quot; Normal quantiles xqplot(un, ptype = &quot;normal&quot;) #&gt; Error in xqplot(un, ptype = &quot;normal&quot;): could not find function &quot;xqplot&quot; We see that none of the numeric variables have normal distributions. ‘age’ is somewhat platykurtic compared with a normal ‘compositeHourlyWages’ has both a categorical (0) and a continuous component ‘education’ is also platykurtic ‘working’ is dichotomous ‘familyIncome’ is skewed to the right Note that the fact that \\(x\\) or \\(y\\) variables are not normal does not mean that the conditional distribution of \\(y\\) given \\(x\\) is not normal. Let’s explore wages of working women as a function of education. library(latticeExtra) un %&gt;% xyplot(infantMortality ~ GDPperCapita, .) + layer(panel.loess(..., lwd = 2)) # Scatterplot showing curvature in relationship trellis.focus() panel.identify(labels = rownames(un)) #&gt; integer(0) trellis.unfocus() un %&gt;% xyplot(log(infantMortality) ~ GDPperCapita | region, .) + layer(panel.loess(..., lwd = 2)) un %&gt;% subset(country %in% c(&quot;United.States&quot;, &quot;Canada&quot;)) #&gt; region tfr contraception educationMale educationFemale lifeMale #&gt; Canada America 1.61 66 17.2 17.8 76.1 #&gt; United.States America 1.96 71 15.4 16.2 73.4 #&gt; lifeFemale infantMortality GDPperCapita economicActivityMale #&gt; Canada 81.8 6 18943 72.4 #&gt; United.States 80.1 7 26037 74.9 #&gt; economicActivityFemale illiteracyMale illiteracyFemale #&gt; Canada 57.6 NA NA #&gt; United.States 59.3 2.24 2.23 #&gt; country #&gt; Canada Canada #&gt; United.States United.States between wage and education, and heteroskedasticity in wage as a function of education. # library(p3d) # slid %&gt;% # xyplot(sqrt(wage) ~ education, .) + # layer(panel.loess(...)) # Init3d() # Plot3d(log(infantMortality) ~ GDPperCapita + lifeFemale | region, un) # Id3d() # Id3d(&#39;United.States&#39;) # Id3d(&#39;Canada&#39;) # rownames(un) # names(un) 34.4 Box Cox Transformation This video was made by math et al. I like their channel and found this video to be a good one. 34.4.1 Additional Resources Salvatore S. Mangiafico’s Summary and Analysis of Extension Program Evaluation in R, rcompanion.org/handbook/. Pdf version http://www.unige.ch/ses/sococ/cl//stat/eda/ladder.html https://www.statisticshowto.com/tukey-ladder-of-powers/ http://blackwell.math.yorku.ca/math4939/lectures/transforming_data_tukeys_ladder_of_powers.html https://thomaselove.github.io/431-notes/re-expression-tukeys-ladder-box-cox-plot.html "],["lab04.html", "35 Lab: Visualizing spatial data La Quinta is Spanish for ‘next to Denny’s’, Pt. 1 Getting started The data Exercises", " 35 Lab: Visualizing spatial data La Quinta is Spanish for ‘next to Denny’s’, Pt. 1 Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg has joked that it’s Spanish for next to Denny’s. If you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain. These two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab, we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data. This lab was inspired by John Reiser’s post in his New Jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. In this lab, we focus on visualization and analysis of those data. However, it’s worth noting that the data scraping was also done in R. Later in the course,we will discuss web scraping using R . But for now, we’re focusing on the data that has already been scraped and tidied up for you. Getting started Go to the course GitHub organization and locate the lab repo, which should be named something like lab-04-viz-sp-data. This link should take you to the lab. Either Fork it or use the template. Then clone it in RStudio. First, open the R Markdown document lab-04.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. Packages In this lab, we will use the tidyverse and dsbox packages. The *dsbox** package is not on CRAN yet; instead it is hosted on github. You will have to download and install it yourself. This piece of code should help get you started. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio-education/dsbox&quot;) library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 library(dsbox) If you cannot get dsbox to install, you can instead download the two datasets we will be using manually here and here. githubURL_1 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/laquinta.rda&quot; githubURL_2 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/dennys.rda&quot; load(url(githubURL_1)) load(url(githubURL_2)) Project name Currently your project is called Untitled Project. Update the name of your project to be “Lab 04 - Visualizing spatial data”. Warm up Before we introduce the data, let’s warm up with some simple exercises. YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update team name” in the Commit message box and hit Commit. Click on Push. This action will prompt a dialogue box where you first need to enter your user name, and then your password. The data The datasets we’ll use are called dennys and laquinta from the dsbox package. Note that these data were scraped from here and here, respectively. To help with our analysis we will also use a dataset on US states: states &lt;- read_csv(&quot;data/states.csv&quot;) Each observation in this dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles). Exercises What are the dimensions of the Denny’s dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables? What are the dimensions of the La Quinta’s dataset? What does each row in the dataset represent? What are the variables? We would like to limit our analysis to Denny’s and La Quinta locations in the United States. Take a look at the websites that the data come from (linked above). Are there any La Quinta’s locations outside of the US? If so, which countries? What about Denny’s? Now take a look at the data. What would be some ways of determining whether or not either establishment has any locations outside the US using just the data (and not the websites). Don’t worry about whether you know how to implement this, just brainstorm some ideas. Write down at least one as your answer, but you’re welcomed to write down a few options too. We will determine whether or not the establishment has a location outside the US using the state variable in the dn and lq datasets. We know exactly which states are in the US, and we have this information in the states data frame we loaded. Find the Denny’s locations that are outside the US, if any. To do so, filter the Denny’s locations for observations where state is not in states$abbreviation. The code for this is given below. Note that the %in% operator matches the states listed in the state variable to those listed in states$abbreviation. The ! operator means not. Are there any Denny’s locations outside the US? Hint: Some of the abbreviations may not be familiar to you. Professor Google might be able to help. “Filter for states that are not in states$abbreviation.” dn %&gt;% filter(!(state %in% states$abbreviation)) Add a country variable to the Denny’s dataset and set all observations equal to \"United States\". Remember, you can use the mutate function for adding a variable. Make sure to save the result of this as dn again so that the stored data frame contains the new variable going forward. Comment: We don’t need to tell R how many times to repeat the character string “United States” to fill in the data for all observations, R takes care of that automatically. dn %&gt;% mutate(country = &quot;United States&quot;) Find the La Quinta locations that are outside the US, and figure out which country they are in. This might require some googling. Take notes, you will need to use this information in the next exercise. Add a country variable to the La Quinta dataset. Use the case_when function to populate this variable. You’ll need to refer to your notes from Exercise 7 about which country the non-US locations are in. Here is some starter code to get you going: lq %&gt;% mutate(country = case_when( state %in% state.abb ~ &quot;United States&quot;, state %in% c(&quot;ON&quot;, &quot;BC&quot;) ~ &quot;Canada&quot;, state == &quot;ANT&quot; ~ &quot;Colombia&quot;, ... # fill in the rest )) Going forward we will work with the data from the United States only. All Denny’s locations are in the United States, so we don’t need to worry about them. However we do need to filter the La Quinta dataset for locations in United States. lq &lt;- lq %&gt;% filter(country == &quot;United States&quot;) Which states have the most and fewest Denny’s locations? What about La Quinta? Is this surprising? Why or why not? Next, let’s calculate which states have the most Denny’s locations per thousand square miles. This requires joinining information from the frequency tables you created in the previous set with information from the states data frame. First, we count how many observations are in each state, which will give us a data frame with two variables: state and n. Then, we join this data frame with the states data frame. However note that the variables in the states data frame that has the two-letter abbreviations is called abbreviation. So when we’re joining the two data frames we specify that the state variable from the Denny’s data should be matched by the abbreviation variable from the states data: dn %&gt;% count(state) %&gt;% inner_join(states, by = c(&quot;state&quot; = &quot;abbreviation&quot;)) Before you move on the the next question, run the code above and take a look at the output. In the next exercise, you will need to build on this pipe. Which states have the most Denny’s locations per thousand square miles? What about La Quinta? Next, we put the two datasets together into a single data frame. However before we do so, we need to add an identifier variable. We’ll call this establishment and set the value to \"Denny's\" and \"La Quinta\" for the dn and lq data frames, respectively. dn &lt;- dn %&gt;% mutate(establishment = &quot;Denny&#39;s&quot;) lq &lt;- lq %&gt;% mutate(establishment = &quot;La Quinta&quot;) Because the two data frames have the same columns, we can easily bind them with the bind_rows function: dn_lq &lt;- bind_rows(dn, lq) We can plot the locations of the two establishments using a scatter plot, and color the points by the establishment type. Note that the latitude is plotted on the x-axis and the longitude on the y-axis. ggplot(dn_lq, mapping = aes( x = longitude, y = latitude, color = establishment )) + geom_point() The following two questions ask you to create visualizations. These visualizations should follow best practices you learned in class, such as informative titles, axis labels, etc. See http://ggplot2.tidyverse.org/reference/labs.html for help with the syntax. You can also choose different themes to change the overall look of your plots, see http://ggplot2.tidyverse.org/reference/ggtheme.html for help with these. Filter the data for observations in North Carolina only, and recreate the plot. You should also adjust the transparency of the points, by setting the alpha level, so that it’s easier to see the overplotted ones. Visually, does Mitch Hedberg’s joke appear to hold here? Now filter the data for observations in Texas only, and recreate the plot, with an appropriate alpha level. Visually, does Mitch Hedberg’s joke appear to hold here? That’s it for now! In the next lab, we will take a more quantitative approach to answering these questions. "],["welcome-to-tips-for-effective-data-visualization.html", "36 Welcome to Tips for Effective Data Visualization 36.1 Module Materials 36.2 Estimated Video Length", " 36 Welcome to Tips for Effective Data Visualization This module is designed to introduce ideas related to effective data visualization. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 36.1 Module Materials Slides Tips for effective data visualization Deep Diving into ggplot Activities Brexit Lab More Dennys Suggested Readings A layered grammar of graphics All subchapters of this module r4ds Graphics for communication Data visualisation 36.2 Estimated Video Length No of videos: 7 Average length of video: 12 minutes, 23 seconds Total length of playlist: 1 hour, 26 minutes, 46 seconds "],["designing-effective-visualizations.html", "37 Designing effective visualizations 37.1 Principles for effective visualizations", " 37 Designing effective visualizations You can follow along with the slides here if they do not appear below. 37.1 Principles for effective visualizations You can follow along with the slides here if they do not appear below. A piece of advice that I constantly think about from grad school:Design your tables and figures as though that&#39;s the only thing your reviewers are reading.&mdash; Joshua P. Darr (@joshuadarr) December 1, 2021 "],["deeper-diving-into-ggplot2.html", "38 Deeper Diving into ggplot2 38.1 What are the components of a plot? 38.2 Stats, Geoms, and Positions 38.3 Scales and Coordinates 38.4 How this all works with Minard", " 38 Deeper Diving into ggplot2 You can follow along with the slides here if they do not appear below. 38.1 What are the components of a plot? 38.2 Stats, Geoms, and Positions 38.2.1 Jitter to the rescue now with jitter: pic.twitter.com/XziQHdYN4s&mdash; Mijke Rhemtulla (@mijkenijk) February 28, 2021 38.3 Scales and Coordinates 38.4 How this all works with Minard "],["plots-behaving-badly-lessons-in-data-misrepresentation.html", "39 Plots Behaving Badly: Lessons in Data Misrepresentation 39.1 General Principles 39.2 High correlation does not imply replication 39.3 Barplots for paired data 39.4 Gratuitous 3D 39.5 Ignoring important factors 39.6 Too many significant digits 39.7 Displaying data well 39.8 Some further reading", " 39 Plots Behaving Badly: Lessons in Data Misrepresentation This section explores common pitfalls in data visualization, inspired by Karl W. Broman’s talk on how default plotting choices, like those in Microsoft Excel, can “obscure your data and annoy your readers”. We’ll examine how to intentionally create bad plots to understand good design practices better. Broman’s lecture was inspired by (Howard Wainer’s 1984 paper: How to display data badly. American Statistician 38(2): 137–147)[https://www.jstor.org/stable/2683253]. Wainer was the first to set forth the principles of the bad display of data. But according to Karl “The now widespread use of Microsoft Excel has resulted in remarkable advances in the field.” 39.1 General Principles The aims of good data graphics is to display data accurately and clearly. Some rules for displaying data badly: Display as little information as possible. Obscure what you do show (with chart junk). Use pseudo-3d and color gratuitously. Make a pie chart (preferably in color and 3d). Use a poorly chosen scale. Ignore significant figures. 39.1.1 The Problem with Pie Charts Pie charts are criticized for their inefficiency in conveying information. They do not allow easy comparison of sections as humans are better at comparing lengths, not areas. Here’s an example using browser usage data from an August 2013 poll: 39.1.1.1 Displaying Browser Usage Data with a Pie Chart Consider a poll asking about browser preferences collected in August 2013. The typical display method would be a pie chart: browsers &lt;- c(Opera = 1, Safari = 9, Firefox = 20, IE = 26, Chrome = 44) pie(browsers, main = &quot;Browser Usage (August 2013)&quot;) 39.1.1.2 Why Pie Charts Fail The pie function’s help file itself warns against using pie charts for presentation: Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data. To test this, try determining the exact market shares of the browsers from the pie chart above. It’s challenging to assess precise proportions just by looking at the angles and areas. 39.1.1.3 A Better Alternative: Bar Charts If we need to visualize this data more effectively, a bar chart is more appropriate: barplot(browsers, main = &quot;Browser Usage (August 2013)&quot;) The bar chart aligns the data vertically, allowing for easy comparison across browsers. You can follow a horizontal line across to the x-axis to find precise percentages, avoiding the visual estimation errors common with pie charts. (Do avoid 3-D version as they obfuscate the plot and remove this particular advantage.) Note that even worse that piecharts are donut plots. The reason is that by removing the center we remove one of the visual cues for determining the different areas: the angles. There is no reason to ever use a donut to display data. 39.1.2 Barplots as data summaries Although barplots are useful for showing percentages, they are incorrectly used to display data from two groups begin compared. Specifically, barplots are created with height equal to the group means and an antenna is added at the top to represent standard errors. This plot is simply showing two numbers per groups and the plot adds nothing: Much more informative is to summarizing with a boxplot. If the number of points is small enough, we might as well add them to the plot. When the number of points is too large for us to see them, just showing a boxplot is preferable. # library(&quot;downloader&quot;) # tmpfile &lt;- tempfile() # download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig1.RData&quot;) # load(&quot;data/badgraphfig1.RData&quot;) library(tidyverse) library(rafalib) df_fig1 &lt;- read.csv(&quot;data/bromanfig1.csv&quot;) mypar(1, 1) dat &lt;- list( Treatment = df_fig1$x, Control = df_fig1$y ) dat %&gt;% boxplot( xlab = &quot;Group&quot;, ylab = &quot;Response&quot;, cex = 0 ) dat %&gt;% stripchart( vertical = TRUE, method = &quot;jitter&quot;, pch = 16, add = TRUE, col = 1 ) Note how much more we see here: the center, spread, range and the points themselves while in the barplot we only see the mean and the SE and the SE has more to do with sample size than the spread of the data. This problem is magnified when our data has outliers or very large tails. Note that from this plot there appears to be very large and consistent difference between the two groups: A quick look at the data demonstrates that this difference is mostly driven by just two points. A version showing the data in the log-scale is much more informative. df_fig3 &lt;- read.csv(&quot;data/bromanfig3.csv&quot;) library(rafalib) mypar(1, 2) dat &lt;- list( Treatment = df_fig3$x, Control = df_fig3$y ) dat %&gt;% boxplot( xlab = &quot;Group&quot;, ylab = &quot;Response&quot;, xlab = &quot;Group&quot;, ylab = &quot;Response&quot;, cex = 0 ) #&gt; Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : #&gt; Duplicated arguments xlab = &quot;Group&quot;, ylab = &quot;Response&quot; are disregarded dat %&gt;% stripchart( vertical = TRUE, method = &quot;jitter&quot;, pch = 16, add = TRUE, col = 1 ) dat %&gt;% boxplot( xlab = &quot;Group&quot;, ylab = &quot;Response&quot;, xlab = &quot;Group&quot;, ylab = &quot;Response&quot;, log = &quot;y&quot;, cex = 0 ) #&gt; Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : #&gt; Duplicated arguments xlab = &quot;Group&quot;, ylab = &quot;Response&quot; are disregarded dat %&gt;% stripchart( vertical = TRUE, method = &quot;jitter&quot;, pch = 16, add = TRUE, col = 1 ) 39.1.3 Show the scatterplot The purpose of many statistical analyses is to determine relationships between two variables. Sample correlations are typically reported and sometimes plots are displayed to show this. However, showing just the regression line is one way to display your data baldy as it hides the scatter. Surprisingly plots such as the following are commonly seen: df_fig4 &lt;- read.csv(&quot;data/bromanfig4.csv&quot;) fit &lt;- lm(y ~ x, data = df_fig4 ) rho &lt;- (round( cor( df_fig4$x, df_fig4$y ), 4 )) ggplot( data = df_fig4, aes(x = x, y = y) ) + geom_point(alpha = 0) + geom_abline( intercept = fit$coef[1], slope = fit$coef[2], lwd = 1 ) + geom_text( x = 85, y = 200, label = paste( &quot;y =&quot;, round(fit$coef[1], 3), &quot;+&quot;, round(fit$coef[2], 3), &quot;x&quot; ) ) + geom_text( x = 85, y = 187, label = expression(paste(rho, &quot; = 0.8567&quot;)) ) Showing the data is much more informative: ggplot(data = df_fig4, aes(x = x, y = y)) + geom_point() + geom_abline( intercept = fit$coef[1], slope = fit$coef[2], lwd = 1 ) + geom_text( x = 85, y = 200, label = paste( &quot;y =&quot;, round(fit$coef[1], 3), &quot;+&quot;, round(fit$coef[2], 3), &quot;x&quot; ) ) + geom_text( x = 85, y = 187, label = expression(paste(rho, &quot; = 0.8567&quot;)) ) 39.2 High correlation does not imply replication When new technologies or laboratory techniques are introduced, we are often shown scatter plots and correlations from replicated samples. High correlations are used to demonstrate that the new technique is reproducible. But correlation can be very misleading. Below is a scatter plot showing data from replicated samples run on a high throughput technology. This technology outputs 12,626 simultaneously measurements. In the plot on the left we see the original data which shows very high correlation. But the data follows a distribution with very fat tails. Note that 95% of the data is below the green line. The plot on the right is in the log scale. # if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) # BiocManager::install(&quot;Biobase&quot;) # BiocManager::install(&quot;SpikeInSubset&quot;) library(Biobase) library(SpikeInSubset) data(mas95) mypar(1, 2) df_mas95 &lt;- data.frame( r = exprs(mas95)[, 1], ## original measures were not logged g = exprs(mas95)[, 2] ) f &lt;- function(a, x, y, p = 0.95) { mean(x &lt;= a &amp; y &lt;= a) - p } a95 &lt;- uniroot( f, lower = 2000, upper = 20000, x = df_mas95$r, y = df_mas95$g )$root string_cor &lt;- signif(cor(df_mas95$r, df_mas95$g), 3) ggplot( data = df_mas95, aes(x = r, y = g) ) + geom_point(color = &quot;black&quot;, size = 0.5) + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) + xlab(expression(paste(E[1]))) + ylab(expression(paste(E[2]))) + ggtitle(paste0(&quot;corr=&quot;, string_cor)) + geom_abline(slope = -1, intercept = a95, color = &quot;seagreen&quot;) + geom_text( x = 8500, y = 0, label = &quot;95% of data below this line&quot;, vjust = &quot;inward&quot;, hjust = &quot;inward&quot;, color = &quot;seagreen&quot; ) df_mas95 &lt;- df_mas95 %&gt;% mutate( r = log2(r), g = log2(g) ) string_cor &lt;- signif(cor(df_mas95$r, df_mas95$g), 3) ggplot( data = df_mas95, aes(x = r, y = g) ) + geom_point(color = &quot;black&quot;, size = 0.5) + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) + xlab(expression(paste(log[2], &quot; &quot;, E[1]))) + ylab(expression(paste(log[2], &quot; &quot;, E[2]))) + ggtitle(paste0(&quot;corr=&quot;, string_cor)) Although the correlation is reduced in the log-scale, it is still close to 1. Does this mean these data are reproduced? To examine how well the second vector reproduces the first, we need to study the differences. Therefore, we should instead plot the difference (in the log scale) versus the average: df_mas95 &lt;- df_mas95 %&gt;% mutate( x = (r + g) / 2, y = (r - g) ) string_sd &lt;- signif(sqrt(mean((df_mas95$r - df_mas95$g)^2)), 3) ggplot(data = df_mas95, aes(x = x, y = y)) + geom_point(size = .5, color = &quot;black&quot;) + geom_hline(yintercept = 0, color = &quot;red&quot;) + xlab(expression(paste(&quot;Ave{ &quot;, log[2], &quot; &quot;, E[1], &quot;, &quot;, log[2], &quot; &quot;, E[2], &quot; }&quot;))) + ylab(expression(paste(log[2], &quot; { &quot;, E[1], &quot; / &quot;, E[2], &quot; }&quot;))) + ggtitle(paste0(&quot;SD=&quot;, string_sd)) These are referred to as Bland-Altman plots or MA plots in the genomics literature and will say more later. In this plot we see that the typical difference in the log (base 2) scale between two replicated measures is about 1. This means that when measurements should be the same we will, on average, observe 2 fold difference. We can now compare this variability to the differences we want to detect and decide if this technology is precise enough for our purposes. 39.3 Barplots for paired data A common task in data analysis is the comparison of two groups. When the dataset is small and data are paired, for example outcomes before and after a treatment, an unfortunate display that is used is the barplot with two colors: There are various better ways of showing these data to illustrate there is an increase after treatment. One is to simply make a scatterplot and which shows that most points are above the identity line. Another alternative is plot the differences against the before values. set.seed(12201970) before &lt;- runif(6, 5, 8) after &lt;- rnorm(6, before * 1.05, 2) li &lt;- range(c(before, after)) ymx &lt;- max(abs(after - before)) mypar(1, 2) plot( before, after, xlab = &quot;Before&quot;, ylab = &quot;After&quot;, ylim = li, xlim = li ) abline(0, 1, lty = 2, col = 1) plot( before, after - before, xlab = &quot;Before&quot;, ylim = c(-ymx, ymx), ylab = &quot;Change (After - Before)&quot;, lwd = 2 ) abline(h = 0, lty = 2, col = 1) Line plots are not a bad choice, although I find them harder to follow than the previous two. Boxplots show you the increase, but lose the paired information. z &lt;- rep(c(0, 1), rep(6, 2)) mypar(1, 2) plot( z, c(before, after), xaxt = &quot;n&quot;, ylab = &quot;Response&quot;, xlab = &quot;&quot;, xlim = c(-0.5, 1.5) ) axis(side = 1, at = c(0, 1), c(&quot;Before&quot;, &quot;After&quot;)) segments(rep(0, 6), before, rep(1, 6), after, col = 1) boxplot(before, after, names = c(&quot;Before&quot;, &quot;After&quot;), ylab = &quot;Response&quot; ) 39.4 Gratuitous 3D The follow figure shows three curves. Pseudo 3D is used but it is not clear way. Maybe to separate the three curves? Note how difficult it is to determine the values of the curves at any given point: This plot can be made better by simply using color to distinguish the three lines: # download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv&quot;,tmpfile) x &lt;- read.table(&quot;data//fig8dat.csv&quot;, sep = &quot;,&quot;, header = TRUE) plot( x[, 1], x[, 2], xlab = &quot;log Dose&quot;, ylab = &quot;Proportion survived&quot;, ylim = c(0, 1), type = &quot;l&quot;, lwd = 2, col = 1 ) lines(x[, 1], x[, 3], lwd = 2, col = 2) lines(x[, 1], x[, 4], lwd = 2, col = 3) legend(1, 0.4, c(&quot;Drug A&quot;, &quot;Drug B&quot;, &quot;Drug C&quot;), lwd = 2, col = 1:3 ) 39.5 Ignoring important factors In this example we generate data with a simulation. We are studying a dose response relationship between two groups treatment and control. We have three groups of measurements for both control and treatment. Comparing treatment and control using the common barplot: Instead we should show each curve. We can use color to distinguish treatment and control and dashed and solid lines to distinguish the original data from the mean of the three groups. plot( x, y1, ylim = c(0, 1), type = &quot;n&quot;, xlab = &quot;Dose&quot;, ylab = &quot;Response&quot; ) for (i in 1:3) { lines(x, z[, i], col = 1, lwd = 1, lty = 2 ) } for (i in 1:3) { lines(x, y[, i], col = 2, lwd = 1, lty = 2 ) } lines(x, ym, col = 1, lwd = 2) lines(x, zm, col = 2, lwd = 2) legend(&quot;bottomleft&quot;, lwd = 2, col = c(1, 2), c(&quot;Control&quot;, &quot;Treated&quot;) ) 39.6 Too many significant digits By default, statistical software like R return many significant digits. This does not mean we should report them. Cutting and pasting directly from R is a bad idea as you might end up showing a table like this for, say, heights of basketball players: heights &lt;- cbind( rnorm(8, 73, 3), rnorm(8, 73, 3), rnorm(8, 80, 3), rnorm(8, 78, 3), rnorm(8, 78, 3) ) colnames(heights) &lt;- c(&quot;SG&quot;, &quot;PG&quot;, &quot;C&quot;, &quot;PF&quot;, &quot;SF&quot;) rownames(heights) &lt;- paste(&quot;team&quot;, 1:8) heights #&gt; SG PG C PF SF #&gt; team 1 76.4 76.2 81.7 75.3 77.2 #&gt; team 2 74.1 71.1 80.3 81.6 73.0 #&gt; team 3 71.5 69.0 85.8 80.1 72.8 #&gt; team 4 78.7 72.8 81.3 76.3 82.9 #&gt; team 5 73.4 73.3 79.2 79.7 80.3 #&gt; team 6 72.9 71.8 77.4 81.7 80.4 #&gt; team 7 68.4 73.0 79.1 71.2 77.2 #&gt; team 8 73.8 75.6 83.0 75.6 87.7 Note we are reporting precision up to 0.00001 inches. Do you know of a tape measure with that much precision? This can be easily remedied: round(heights, 1) #&gt; SG PG C PF SF #&gt; team 1 76.4 76.2 81.7 75.3 77.2 #&gt; team 2 74.1 71.1 80.3 81.6 73.0 #&gt; team 3 71.5 69.0 85.8 80.1 72.8 #&gt; team 4 78.7 72.8 81.3 76.3 82.9 #&gt; team 5 73.4 73.3 79.2 79.7 80.3 #&gt; team 6 72.9 71.8 77.4 81.7 80.4 #&gt; team 7 68.4 73.0 79.1 71.2 77.2 #&gt; team 8 73.8 75.6 83.0 75.6 87.7 39.7 Displaying data well In general you should follow these principles: Be accurate and clear. Let the data speak. Show as much information as possible, taking care not to obscure the message. Science not sales: avoid unnecessary frills (esp. gratuitous 3d). In tables, every digit should be meaningful. Don’t drop ending 0s. 39.8 Some further reading ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130 NB Robbins (2004) Creating more effective graphs. Wiley Nature Methods columns "],["odd-design-choices-in-data-visualization.html", "40 ODD: Design choices in data visualization 40.1 How to spot a misleading graph 40.2 Data Visualization and Misrepresentation 40.3 Vox on How coronavirus charts can mislead us 40.4 Vox on Shut up about the y-axis. It shouldn’t always start at zero 40.5 Gloriously Terrible Plots", " 40 ODD: Design choices in data visualization I have curated this collection of external video sources on design choices in data visualization. 40.1 How to spot a misleading graph When they’re used well, graphs can help us intuitively grasp complex data. But as visual software has enabled more usage of graphs throughout all media, it has also made them easier to use in a careless or dishonest way — and as it turns out, there are plenty of ways graphs can mislead and outright manipulate. Lea Gaslowitz shares some things to look out for. 40.2 Data Visualization and Misrepresentation This animation was produced by some of my colleagues at Brown. 40.3 Vox on How coronavirus charts can mislead us 40.4 Vox on Shut up about the y-axis. It shouldn’t always start at zero Also a nice example of appropriately choosing a y-axis window (cf. @sharoz) https://t.co/wCiOeyTo6k&mdash; Brenton Wiernik ️‍ (@bmwiernik) February 18, 2021 40.5 Gloriously Terrible Plots "],["secrets.html", "41 ODD: Secrets of a happy graphing life 41.1 The hidden data gremlins 41.2 Data Frames are Your Friends 41.3 Worked example", " 41 ODD: Secrets of a happy graphing life This optional deep dive (ODD) explores some key principles for effective data visualization in R. While it may seem like graphs are just the final step of analysis, many graphing headaches actually stem from data wrangling issues earlier in the process. Let’s look at some tips to make your visualization workflow smoother. 41.1 The hidden data gremlins Alright, picture this: you’re staring at your screen, wondering why your beautiful ggplot2 creation looks more like a Jackson Pollock painting than a scientific visualization. Before you throw your laptop out the window, ask yourself: Is my stuff hanging out in data frames where it belongs? Are my data frames living their best tidy life? Am I the master of my factors, or are they running wild? In my years of teaching this course, I’ve seen countless students (and, okay, myself) waste hours on plotting problems that were actually data wrangling issues in disguise. Solve these, and watch your graphing woes disappear faster than free pizza at a psych department colloquium. Violating these principles is the source of most graphing woes. When you’re stuck, check if you’re following these rules: - Keep your data in data frames - Keep your data frames tidy - Master your factors 41.2 Data Frames are Your Friends A common rookie mistake is pulling variables out of data frames to work with them separately: library(gapminder) library(tidyverse) life_exp &lt;- gapminder$lifeExp year &lt;- gapminder$year While this might seem convenient, it causes headaches down the road. Instead, keep your variables together in a data frame and pass the whole thing to ggplot2: ggplot(mapping = aes( x = year, y = life_exp )) + geom_jitter() Just leave the variables in place and pass the associated data frame! This advice applies to base and lattice graphics as well. It is not specific to ggplot2. ggplot(data = gapminder, aes( x = year, y = life_exp )) + geom_jitter() What if we wanted to filter the data by country, continent, or year? This is much easier to do safely if all affected variables live together in a data frame, not as individual objects that can get “out of sync.” Don’t write-off ggplot2 as a highly opinionated outlier! In fact, keeping data in data frames and computing and visualizing it in situ are widely regarded as best practices. The option to pass a data frame via data = is a common feature of many high-use R functions, e.g. lm(), aggregate(), plot(), and t.test(), so make this your default modus operandi. 41.2.1 Explicit data frame creation via tibble::tibble() and tibble::tribble() If your data is already lying around and it’s not in a data frame, ask yourself “why not?”. Did you create those variables? Maybe you should have created them in a data frame in the first place! The tibble() function is an improved version of the built-in data.frame(), which makes it possible to define one variable in terms of another and which won’t turn character data into factor. If constructing tiny tibbles “by hand”, tribble() can be an even handier function, in which your code will be laid out like the table you are creating. These functions should remove the most common excuses for data frame procrastination and avoidance. my_dat &lt;- tibble( x = 1:5, y = x^2, text = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;delta&quot;, &quot;epsilon&quot;) ) ## if you&#39;re truly &quot;hand coding&quot;, tribble() is an alternative my_dat &lt;- tribble( ~x, ~y, ~text, 1, 1, &quot;alpha&quot;, 2, 4, &quot;beta&quot;, 3, 9, &quot;gamma&quot;, 4, 16, &quot;delta&quot;, 5, 25, &quot;epsilon&quot; ) str(my_dat) #&gt; tibble [5 × 3] (S3: tbl_df/tbl/data.frame) #&gt; $ x : num [1:5] 1 2 3 4 5 #&gt; $ y : num [1:5] 1 4 9 16 25 #&gt; $ text: chr [1:5] &quot;alpha&quot; &quot;beta&quot; &quot;gamma&quot; &quot;delta&quot; ... ggplot( my_dat, aes(x, y) ) + geom_line() + geom_text(aes(label = text)) Together with dplyr::mutate(), which adds new variables to a data frame, this gives you the tools to work within data frames whenever you’re handling related variables of the same length. 41.2.2 Sidebar: with() Sadly, not all functions offer a data = argument. Take cor(), for example, which computes correlation. This does not work: cor(year, lifeExp, data = gapminder) #&gt; Error in cor(year, lifeExp, data = gapminder): unused argument (data = gapminder) Sure, you can always just repeat the data frame name like so: cor(gapminder$year, gapminder$lifeExp) #&gt; [1] 0.436 but people hate typing. I suspect subconscious dread of repeatedly typing gapminder is what motivates those who copy variables into stand-alone objects in the workspace. The with() function is a better workaround. Provide the data frame as the first argument. The second argument is an expression that will be evaluated in a special environment. It could be a single command or a multi-line snippet of code. What’s special is that you can refer to variables in the data frame by name. with( gapminder, cor(year, lifeExp) ) #&gt; [1] 0.436 If you use the magrittr package, another option is to use the %$% operator to expose the variables inside a data frame for further computation: library(magrittr) gapminder %$% cor(year, lifeExp) #&gt; [1] 0.436 41.3 Worked example Inspired by this question from a student when we first started using ggplot2: How can I focus in on country, Japan for example, and plot all the quantitative variables against year? Your first instinct might be to filter the Gapminder data for Japan and then loop over the variables, creating separate plots which need to be glued together. And, indeed, this can be done. But in my opinion, the data reshaping route is more “R native” given our current ecosystem, than the loop way. 41.3.1 Reshape your data We filter the Gapminder data and keep only Japan. Then we use tidyr::gather() to gather up the variables pop, lifeExp, and gdpPercap into a single value variable, with a companion variable key. japan_dat &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) japan_tidy &lt;- japan_dat %&gt;% gather( key = var, value = value, pop, lifeExp, gdpPercap ) dim(japan_dat) #&gt; [1] 12 6 dim(japan_tidy) #&gt; [1] 36 5 The filtered japan_dat has 12 rows. Since we are gathering or stacking three variables in japan_tidy, it makes sense to see three times as many rows, namely 36 in the reshaped result. 41.3.2 Iterate over the variables via faceting Now that we have the data we need in a tidy data frame, with a proper factor representing the variables we want to “iterate” over, we just have to facet. p &lt;- ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~var, scales = &quot;free_y&quot;) p + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) 41.3.3 Recap Here’s the minimal code to produce our Japan example. japan_tidy &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) %&gt;% gather(key = var, value = value, pop, lifeExp, gdpPercap) ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~var, scales = &quot;free_y&quot;) + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) This snippet demonstrates the payoffs from the rules we laid out at the start: We isolate the Japan data into its own data frame. We reshape the data. We gather three columns into one, because we want to depict them via position along the y-axis in the plot. We use a factor to distinguish the observations that belong in each mini-plot, which then becomes a simple application of faceting. This is an example of expedient data reshaping. I don’t actually believe that gdpPercap, lifeExp, and pop naturally belong together in one variable. But gathering them was by far the easiest way to get this plot. "],["save-figs.html", "42 Writing figures to file 42.1 Step away from the mouse 42.2 Good names are like breadcrumbs 42.3 Graphics devices 42.4 Write figures to file with ggsave() 42.5 Write non-ggplot2 figures to file 42.6 Preemptive answers to some FAQs 42.7 Chunk name determines figure file name 42.8 Clean up", " 42 Writing figures to file It is not always appropriate or sufficient for figures to exist only inside a dynamic report, such as an R Markdown document. You should know how to write key figures to file for downstream use in a variety of settings. During development, you need the immediate feedback from seeing your figures appear in a screen device, such as the RStudio Plots pane. Once you’re satisfied, make sure you have all of the commands to produce the figure saved in an R script. You want everything, nachos to cheesecake: data import, any necessary manipulation, then plotting. Now what? How do you preserve the figure in a file? 42.1 Step away from the mouse Do not succumb to the temptation of a mouse-based process. If might feel handy at the time, but you will regret it. This workflow establishes no link between the source code and the figure product. So when – not if – you need to remake the figure with a different color scheme or aspect ratio or graphics device, you will struggle to dig up the proper source code. Use one of the methods below to avoid this predicament. 42.2 Good names are like breadcrumbs If you save figure-making code in a source file and you give figure files machine-readable, self-documenting names, your future self will be able to find its way back to this code. Hypothetical: a zombie project comes back to life and your collaborator presents you with a figure you made 18 months ago. Can you remake fig08_scatterplot-lifeExp-vs-year.pdf as a TIFF and with smooth regression? Fun times! This filename offers several properties to help you find the code that produced it: Human-readability: It’s helpful to know you’re searching for a scatterplot and maybe which variables are important. It gives important context for your personal archaeological dig. Specificity: Note how specific and descriptive the name of this figure file is; we didn’t settle for the generic fig08.pdf or scatterplot.pdf. This makes the name at least somewhat unique, which will help you search your home directory for files containing part or all of this filename. Machine-readability: Every modern OS provides a way to search your hard drive for a file with a specific name or containing a specific string. This will be easier if the name contains no spaces, punctuation, or other funny stuff. If you use conventional extensions, you can even narrow the search to files ending in .R or .Rmd. All of these human practices will help you zero in on the R code you need, so you can modify, re-run, and reuse. 42.3 Graphics devices Read the R help for Devices to learn about graphics devices in general and which are available on your system (obviously requires that you read your local help). It is essential to understand the difference between vector graphics and raster graphics. Vector graphics are represented in terms of shapes and lines, whereas raster graphics are pixel-based. Vector examples: PDF, postscript, SVG Pros: re-size gracefully, good for print. SVG is where the web is heading, though we are not necessarily quite there yet. Raster examples: PNG, JPEG, BMP, GIF Cons: look awful “blown up” … in fact, look awful quite frequently Pros: play very nicely with Microsoft Office products and the web. Files can be blessedly small! Tough love: you will not be able to pick vector or raster or a single device and use it all the time. You must think about your downstream use cases and plan accordingly. It is entirely possible that you should save key figures in more than one format for maximum flexibility in the future. Worst case, if you obey the rules given here, you can always remake the figure to save in a new format. For what its worth, most of Jenny Bryan’s figures exist as pdf(), png(), or both. Although it is not true yet, SVG will hopefully become the new standard, offering the resizability of vector graphics but with web-friendliness as well. Here are two good posts from the Revolutions Analytics blog with tips for saving figures to file: 10 tips for making your R graphics look their best High-quality R graphics on the Web with SVG 42.4 Write figures to file with ggsave() If you are using ggplot2, write figures to file with ggsave(). If you are staring at a plot you just made on your screen, you can call ggsave(), specifying only a filename: ggsave(&quot;my-awesome-graph.png&quot;) It makes a sensible decision about everything else. In particular, as long as you use a conventional extension, it will guess what type of graphics file you want. If you need control over, e.g., width, height, or dpi, roll up your sleeves and use the arguments. 42.4.1 Passing a plot object to ggsave() After the filename, the most common argument you will provide is plot =, which is the second argument by position. If you’ve been building up a plot with the typical ggplot2 workflow, you will pass the resulting object to ggsave(). Example: p &lt;- ggplot(gapminder, aes(x = year, y = lifeExp)) + geom_jitter() # during development, you will uncomment next line to print p to screen # p ggsave(&quot;fig-io-practice.png&quot;, p) See below for gotchas and FAQs when making figures in a non-interactive setting! 42.4.2 Scaling Figures need to be prepared differently for a presentation versus a poster versus a manuscript. You need to fiddle with the size of text, such as the title and axis labels, relative to the entire plot area. There are at least two ways to do this, with slightly different effects and workflows. Via the scale = argument to ggsave(): This actually changes the physical size of the plot, but as an interesting side effect, it changes the relative size of the title and axis labels. Therefore, tweaking this can be a quick-and-dirty way to get different versions of a figure appropriate for a presentation versus a poster versus a manuscript. You can still insert the figure downstream with a different physical size, though you may need to adjust the dpi accordingly on the front end. When scale &lt; 1, various plot elements will be bigger relative to the plotting area; when scale &gt; 1, these elements will be smaller. YMMV but scale = 0.8 often works well for posters and slides. Figure 42.1 shows two versions of a figure, with exaggerated values of scale, to illustrate its effect. library(ggplot2) library(gapminder) p &lt;- ggplot(gapminder, aes( x = year, y = lifeExp )) + geom_jitter() p1 &lt;- p + ggtitle(&quot;scale = 0.6&quot;) p2 &lt;- p + ggtitle(&quot;scale = 2&quot;) ggsave(&quot;img/fig-io-practice-scale-0.6.png&quot;, p1, scale = 0.6) #&gt; Saving 4.2 x 3 in image ggsave(&quot;img/fig-io-practice-scale-2.png&quot;, p2, scale = 2) #&gt; Saving 14 x 10 in image Figure 42.1: Two versions of a figure with exaggerated values of scale Via the base_size of the active theme: The base_size of the theme refers to the base font size. This is NOT a theme element that can be modified via ggplot(...) + theme(...). Rather, it’s an argument to various functions that set theme elements. Therefore, to get the desired effect you need to create a complete theme, specifying the desired base_size. By setting base size &lt; 12, the default value, you shrink text elements and by setting base_size &gt; 12, you make them larger. Figure 42.2 shows two versions of a figure, with exaggerated values of base_size, to illustrate its effect. p3 &lt;- p + ggtitle(&quot;base_size = 20&quot;) + theme_grey(base_size = 20) p4 &lt;- p + ggtitle(&quot;base_size = 3&quot;) + theme_grey(base_size = 3) ggsave(&quot;img/fig-io-practice-base-size-20.png&quot;, p3) #&gt; Saving 7 x 5 in image ggsave(&quot;img/fig-io-practice-base-size-3.png&quot;, p4) #&gt; Saving 7 x 5 in image Figure 42.2: Two versions of a figure with exaggerated values of base_size Thanks to Casey Shannon for tips about scale = and this cheatsheet from Zev Ross for tips about base_size. 42.5 Write non-ggplot2 figures to file Recall that ggsave() is recommended if you’re using ggplot2. But if you’re using base graphics or lattice, here’s generic advice for writing figures to file. To be clear, this also works for ggplot2 graphs, but I can’t think of any good reasons to NOT use ggsave(). Edit your source code in the following way: precede the figure-making code by opening a graphics device and follow it with a command that closes the device. Here’s an example: pdf(&quot;test-fig-proper.pdf&quot;) # starts writing a PDF to file plot(1:10) # makes the actual plot dev.off() # closes the PDF file #&gt; png #&gt; 2 You will see that there’s a new file in the working directory: list.files(pattern = &quot;^test-fig*&quot;) #&gt; [1] &quot;test-fig-proper.pdf&quot; If you run this code interactively, don’t be surprised when you don’t see the figure appear in your screen device. While you’re sending graphics output to, e.g. the pdf() device, you’ll be “flying blind”, which is why it’s important to work out the graphics commands in advance. This is like using sink(), which diverts the output you’d normally see in R Console. Read the R help for Devices to learn about graphics devices in general and which are available on your system (obviously requires that you read your local help). If you need control over, e.g., width, height, or dpi, roll up your sleeves and use the arguments to the graphics device function you are using. There are many. If you are staring at a plot you just made on your screen, here’s a handy short cut for writing a figure to file: plot(1:10) # makes the actual plot dev.print(pdf, &quot;test-fig-quick-dirty.pdf&quot;) # copies the plot to a the PDF file #&gt; png #&gt; 2 You will see there’s now another new file in the working directory: list.files(pattern = &quot;^test-fig*&quot;) #&gt; [1] &quot;test-fig-proper.pdf&quot; &quot;test-fig-quick-dirty.pdf&quot; The appeal of this method is that you will literally copy the figure in front of your eyeballs to file, which is pleasingly immediate. There’s also less code to repeatedly (de-)comment as you run and re-run the script during development. Why is this method improper? Various aspects of a figure – such as font size – are determined by the target graphics device and its physical size. Therefore, it is best practice to open your desired graphics device explicitly, using any necessary arguments to control height, width, fonts, etc. Make your plot. And close the device. But for lots of everyday plots the dev.print() method works just fine. If you call up the help file for [dev.off(), dev.print(), and friends][rdocs-dev], you can learn about many other functions for controlling graphics devices. 42.6 Preemptive answers to some FAQs 42.6.1 Despair over non-existent or empty figures Certain workflows are suited to interactive development and will break when played back non-interactively or at arm’s length. Wake up and pay attention when you cross these lines: You package graph-producing code into a function or put it inside a loop or other iterative machine. You run an R script non-interactively, e.g. via source(), Rscript, or R CMD batch. Basic issue: When working interactively, if you inspect the plot object p by entering p at the command line, the plot gets printed to screen. You’re actually enjoying the result of print(p), but it’s easy to not realize this. To get the same result from code run non-interactively, you will need to call print() explicitly yourself. Here I wrap plotting commands inside a function. The function on the left will fail to produce a PNG, whereas the function on the right will produce a good PNG. Both assume the Gapminder data is present as gapminder and that ggplot2 has been loaded. ## implicit print --&gt; no PNG f_despair &lt;- function() { png(&quot;test-fig-despair.png&quot;) p &lt;- ggplot(gapminder, aes(x = year, y = lifeExp)) p + geom_jitter() dev.off() } f_despair() ## explicit print --&gt; good PNG f_joy &lt;- function() { png(&quot;test-fig-joy.png&quot;) p &lt;- ggplot(gapminder, aes(x = year, y = lifeExp)) p &lt;- p + geom_jitter() print(p) ## &lt;-- VERY IMPORTANT!!! dev.off() } f_joy() Other versions of this fiasco result in a figure file that is, frustratingly, empty. If you expect a figure, but it’s missing or empty, remember to print the plot explicitly. It is worth noting here that the ggsave() workflow is not vulnerable to this gotcha, which is yet another reason to prefer it when using ggplot2. Some relevant threads on stackoverflow: Using png not working when called within a function ggplot’s qplot does not execute on sourcing Save ggplot within a function 42.6.2 Mysterious empty Rplots.pdf file Note: This has been fixed as of ggplot2 v2.0.0. Hallelujah! I will leave this here for a while, since old versions of a package like ggplot2 linger around for months and years. When creating and writing figures from R running non-interactively, you can inadvertently trigger a request to query the active graphics device. For example, ggsave() might try to ascertain the physical size of the current device. But when running non-interactively there is often no such device available, which can lead to the unexpected creation of Rplots.pdf so this request can be fulfilled. I don’t know of a reliable way to suppress this behavior uniformly and I just peacefully coexist with Rplots.pdf when this happens. That is, I just delete it. Some relevant threads on stackoverflow: How to stop R from creating empty Rplots.pdf file when using ggsave and Rscript 42.7 Chunk name determines figure file name Circling back, we return to the topic of figures produced via an R chunk in an R Markdown file. If you are writing GitHub-flavored markdown or keeping the intermediate markdown, your figures will also be saved to file. Rendering foo.Rmd will leave behind foo.md, maybe foo.html, and a directory foo_files, containing any figures created in the document. By default, they will have meaningless names, like unnamed-chunk-7.png. This makes it difficult to find specific figures, e.g. for unplanned use in another setting. However, if you name an R chunk, this name will be baked into the figure file name. Example: here’s an R chunk called scatterplot-lifeExp-vs-year: ```{r scatterplot-lifeExp-vs-year} p &lt;- ggplot(gapminder, aes(x = year, y = lifeExp)) + geom_jitter() p ``` And it will lead to the creation of a suitably named figure file (you may see other figures produced in the document as well): book_figs &lt;- fs::path(&quot;_bookdown_files&quot;, &quot;stat545_files&quot;, &quot;figure-html&quot;) fs::dir_ls(book_figs, regexp = &quot;scatterplot-lifeExp&quot;) If you have concrete plans to use a figure elsewhere, you should probably write it to file using an explicit method described above. But the chunk-naming trick is a nice way to avoid that work, while maintaining flexibility for the future. 42.8 Clean up Let’s delete the temp files we’ve created. library(fs) file_delete(dir_ls(path(&quot;img&quot;), regexp = &quot;fig-io-practice&quot;)) file_delete(dir_ls(&quot;.&quot;, regexp = &quot;test-fig&quot;)) "],["lab05.html", "43 Lab: Wrangling spatial data 43.1 La Quinta is Spanish for next to Denny’s, Pt. 2” Getting started Warm up The data Exercises", " 43 Lab: Wrangling spatial data 43.1 La Quinta is Spanish for next to Denny’s, Pt. 2” In this lab, we revisit the Denny’s and La Quinta Inn and Suites data we visualized in the previous lab. Getting started Go to the course organization on GitHub. Find your lab repo. Packages In this lab, we will use the tidyverse and dsbox packages. The *dsbox** package is not on CRAN yet; instead it is hosted on github. You will have to download and install it yourself. This piece of code should help get you started. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio-education/dsbox&quot;) library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 library(dsbox) If you cannot get dsbox to install, you can also download the two datasets we will be using manually here and here. githubURL_1 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/laquinta.rda&quot; githubURL_2 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/dennys.rda&quot; load(url(githubURL_1)) load(url(githubURL_2)) Housekeeping Password caching If you would like your git password cached for a week for this project, type the following in the Terminal: git config --global credential.helper &#39;cache --timeout 604800&#39; Project name Currently your project is called Untitled Project. Update the name of your project to be “Lab 05 - Wrangling spatial data”. Warm up Before we introduce the data, let’s warm up with some simple exercises. YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. Commiting and pushing changes: Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update team name” in the Commit message box and hit Commit. Click on Push. This action will prompt a dialogue box where you first need to enter your user name, and then your password. The data The datasets we’ll use are called dennys and laquinta from the dsbox package. Exercises Filter the Denny’s data frame for Alaska (AK) and save the result as dn_ak. How many Denny’s locations are there in Alaska? dn_ak &lt;- dn %&gt;% filter(state == &quot;AK&quot;) nrow(dn_ak) Now, do the same for La Quinta data frame for Alaska (AK) and save the result as lq_ak. How many La Quinta locations are there in Alaska? lq_ak &lt;- YOUR CODE GOES HERE nrow(lq_ak) Next we will be calculating the distance between all Denny’s and all La Quinta locations in Alaska. Let’s take this step by step: Step 1: There are 3 Denny’s and 2 La Quinta locations in Alaska. (If you answered differently above, you might want to recheck your answers.) Step 2: Let’s focus on the first Denny’s location. We’ll need to calculate two distances for it: (1) distance between Denny’s 1 and La Quinta 1 and (2) distance between Denny’s 1 and La Quinta (2). Step 3: Now let’s consider all Denny’s locations. How many pairings are there between all Denny’s and all La Quinta locations in Alaska, i.e., how many distances do we need to calculate between the locations of these establishments in Alaska? Calculate the number of pairings using the data frames you have already made YOUR CODE GOES HERE In order to calculate these distances, we need to first restructure our data to pair the Denny’s and La Quinta locations. To do so, we will join the two data frames. We have six join options in R. Each of these join functions take at least three arguments: x, y, and by. x and y are data frames to join by is the variable(s) to join by Four of these join functions combine variables from the two data frames: Note: These functions are called mutating joins. inner_join(): return all rows from x where there are matching values in y, and all columns from x and y. left_join(): return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. right_join(): return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. full_join(): return all rows and all columns from both x and y, where there are not matching values, returns NA for the one missing. And the other two join functions only keep cases from the left-hand data frame, and are called filtering joins. We’ll learn about these another time, but you can find out more about the join functions in the help files for any one of them, e.g. ?full_join. In practice, we mostly use mutating joins. In this case, we want to keep all rows and columns from both dn_ak and lq_ak data frames. So we will use a full_join. Figure 43.1: Full join of Denny’s and La Quinta locations in AK Let’s join the data on Denny’s and La Quinta locations in Alaska, and take a look at what it looks like: dn_lq_ak &lt;- full_join(dn_ak, lq_ak, by = &quot;state&quot;) dn_lq_ak How many observations are in the joined dn_lq_ak data frame? What are the names of the variables in this data frame. .x in the variable names means the variable comes from the x data frame (the first argument in the full_join call, i.e. dn_ak), and .y means the variable comes from the y data frame. These variables are renamed to include .x and .y because the two data frames have the same variables and it’s not possible to have two variables in a data frame with the exact same name. Now that we have the data in the format we wanted, all that is left is to calculate the distances between the pairs. What function from the tidyverse do we use the add a new variable to a data frame while keeping the existing variables? One way of calculating the distance between any two points on the earth is to use the Haversine distance formula. This formula takes into account the fact that the earth is not flat, but instead spherical. Figure 43.2: Image Credit to TheOtherJesse and Steven G. Johnson via wikimedia commons This function is not available in R, but we can save the following function to a file called haversine.R that we can load and then use: haversine &lt;- function(long1, lat1, long2, lat2, round = 3) { # convert to radians long1 &lt;- long1 * pi / 180 lat1 &lt;- lat1 * pi / 180 long2 &lt;- long2 * pi / 180 lat2 &lt;- lat2 * pi / 180 R &lt;- 6371 # Earth mean radius in km a &lt;- sin((lat2 - lat1) / 2)^2 + cos(lat1) * cos(lat2) * sin((long2 - long1) / 2)^2 d &lt;- R * 2 * asin(sqrt(a)) return(round(d, round)) # distance in km } This function takes five arguments: Longitude and latitude of the first location Longitude and latitude of the second location The number of digits to round your response to Calculate the distances between all pairs of Denny’s and La Quinta locations and save this variable as distance. Make sure to save this variable in THE dn_lq_ak data frame, so that you can use it later. Calculate the minimum distance between a Denny’s and La Quinta for each Denny’s location. To do so we group by Denny’s locations and calculate a new variable that stores the information for the minimum distance. dn_lq_ak_mindist &lt;- dn_lq_ak %&gt;% group_by(address.x) %&gt;% summarize(closest = min(distance)) Describe the distribution of the distances Denny’s and the nearest La Quinta locations in Alaska. Also include an appropriate visualization and relevant summary statistics. Repeat the same analysis for North Carolina: (i) filter Denny’s and La Quinta Data Frames for NC, (ii) join these data frames to get a complete list of all possible pairings, (iii) calculate the distances between all possible pairings of Denny’s and La Quinta in NC, (iv) find the minimum distance between each Denny’s and La Quinta location, (v) visualize and describe the distribution of these shortest distances using appropriate summary statistics. Repeat the same analysis for Texas. Repeat the same analysis for a state of your choosing, different than the ones we covered so far. Among the states you examined, where is Mitch Hedberg’s joke most likely to hold true? Explain your reasoning. "],["mod06.html", "44 Welcome to Confounding and Communication! 44.1 Module Materials 44.2 Video Length", " 44 Welcome to Confounding and Communication! This module introduces ideas related to study design and science communication. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. Note that you only have to do one of the labs. Pick either (or both if you want…) 44.1 Module Materials Slides from Lectures Scientific studies and confounding Communicating data science results effectively Suggested Readings Judea Pearl on Understanding Simpson’s Paradox R4DS on communication, including Graphics for Communication All subchapters of this module Lab Ugly Charts 44.2 Video Length No of videos : 5 Average length of video : 11 minutes, 16 seconds Total length of playlist : 56 minutes, 20 seconds "],["scientific-studies-and-confounding.html", "45 Scientific studies and confounding 45.1 Scientific studies 45.2 Climate Change: A Conditional Probability Case Study 45.3 Introducing Simpson’s Paradox with a case study 45.4 Revisiting Simpson’s Paradox", " 45 Scientific studies and confounding And don&#39;t even start thinking about latent variables... https://t.co/kDRZeYlFe2&mdash; JK Flake  (@JkayFlake) December 27, 2021 You can follow along with the slides here if they do not appear below. 45.1 Scientific studies 45.2 Climate Change: A Conditional Probability Case Study 45.3 Introducing Simpson’s Paradox with a case study 45.4 Revisiting Simpson’s Paradox "],["communicating-data-science-results-effectively.html", "46 Communicating data science results effectively", " 46 Communicating data science results effectively You can follow along with the slides here if they do not appear below. This ⬇️. If you want to be a collaborative statistician, you need to be able to talk to and help people who don&#39;t have the same training as you. It&#39;s not their job to be able to talk like they have a degree in statistics. https://t.co/XcvXazNHEw&mdash; Daphna Harel (@DaphnaHarel) March 2, 2021 "],["lab06.html", "47 Lab: Ugly charts and Simpson’s paradox Getting started Packages Take a sad plot and make it better Stretch Practice with Smokers in Whickham Wrapping up More ugly charts", " 47 Lab: Ugly charts and Simpson’s paradox The two data visualized embedded in this lab violate many data visualization best practices. Improve these visualizations using R and the tips for effective visualizations that we’ve introduced. You should produce one visualization per dataset. Your visualization should be accompanied by a brief paragraph describing the choices you made in your improvement, specifically discussing what you didn’t like in the original plots and why, and how you addressed them in the visualization you created. The learning goals for this lab are: Telling a story with data Data visualization best practices Reshaping data Getting started Go to the course GitHub organization and locate your lab repo. Either Fork it or copy it as a template. Then clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document and Knit it. Make sure it compiles without errors. (Also, remember to check the final version after you upload!) The output will be in the file markdown .md file with the same name. Housekeeping Remember: Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to do some configuration so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. Run the following (but update it for your name and email!) in the Console to configure git: library(usethis) use_git_config( user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot; ) Packages Run the following code in the Console to load this package. library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 Take a sad plot and make it better Instructional staff employment trends The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report compiled by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below. Let’s start by loading the data used to create this plot. staff &lt;- read_csv(&quot;data/instructional-staff.csv&quot;) Each row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year. ## # A tibble: 5 × 12 ## faculty_type `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full-Time Tenu… 29 27.6 25 24.8 21.8 20.3 19.3 17.8 17.2 ## 2 Full-Time Tenu… 16.1 11.4 10.2 9.6 8.9 9.2 8.8 8.2 8 ## 3 Full-Time Non-… 10.3 14.1 13.6 13.6 15.2 15.5 15 14.8 14.9 ## 4 Part-Time Facu… 24 30.4 33.1 33.2 35.5 36 37 39.3 40.5 ## 5 Graduate Stude… 20.5 16.5 18.1 18.8 18.7 19 20 19.9 19.5 ## # ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt; To recreate this visualization, we must first reshape the data by separating it into two variables: one for faculty type and one for year. In other words, we will be converting our data from wide format to long format. However, before we do so, consider this thought exercise: How many rows will the long-format data have? Each combination of year and faculty type will have its own row. If there are 5 faculty types and 11 years of data, how many rows would we have? We can perform the conversion from wide to long format using a new function: pivot_longer(). The animation below demonstrates how this function works, as well as its counterpart pivot_wider(). The function has the following arguments: pivot_longer(data, cols, names_to = &quot;name&quot; ) The first argument is data as usual. The second argument, cols, is where you specify which columns to pivot into longer format – in this case all columns except for the faculty_type The third argument, names_to, is a string specifying the name of the column to create from the data stored in the column names of data – in this case year staff_long &lt;- staff %&gt;% pivot_longer(cols = -faculty_type, names_to = &quot;year&quot;) %&gt;% mutate(value = as.numeric(value)) Let’s take a look at what the new longer data frame looks like. staff_long ## # A tibble: 55 × 3 ## faculty_type year value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Full-Time Tenured Faculty 1975 29 ## 2 Full-Time Tenured Faculty 1989 27.6 ## 3 Full-Time Tenured Faculty 1993 25 ## 4 Full-Time Tenured Faculty 1995 24.8 ## 5 Full-Time Tenured Faculty 1999 21.8 ## 6 Full-Time Tenured Faculty 2001 20.3 ## 7 Full-Time Tenured Faculty 2003 19.3 ## 8 Full-Time Tenured Faculty 2005 17.8 ## 9 Full-Time Tenured Faculty 2007 17.2 ## 10 Full-Time Tenured Faculty 2009 16.8 ## # ℹ 45 more rows And now, let’s plot it as a line graph. A possible approach for creating a line plot and differentiating the lines by faculty type is to use the following method: staff_long %&gt;% ggplot(aes(x = year, y = value, color = faculty_type)) + geom_line() ## `geom_line()`: Each group consists of only one observation. ## ℹ Do you need to adjust the group aesthetic? But note that this code results in a message, as well as an unexpected plot. The message informs us that there is only one observation for each faculty type and year combination. To address this, we can use the group aesthetic in the following code. staff_long %&gt;% ggplot(aes( x = year, y = value, group = faculty_type, color = faculty_type )) + geom_line() Include the line plot you made above in your report and make sure the figure width is large enough to make it legible. Also fix the title, axis labels, and legend label. Suppose the objective of this plot was to show that the proportion of part-time faculty have gone up over time compared to other instructional staff types. What changes would you propose making to this plot to tell this story? ✅ ⬆️ Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Fisheries The Fisheries and Aquaculture Department of the Food and Agriculture Organization of the United Nations (FAO) collects data on the fisheries production of different countries. You can find a list of fishery production for various countries in 2016 on this Wikipedia page. The data includes the tonnage of fish captured and farmed for each country. Note that countries whose total harvest was less than 100,000 tons are excluded from the visualization. A researcher has shared a visualization they created using these data with you. Can you help them make improve it? First, brainstorm how you would improve it. Then create the improved visualization and document your changes/decisions with bullet points. It’s ok if some of your improvements are aspirational, i.e. you don’t know how to implement it, but you think it’s a good idea. Implement what you can and leave notes identifying the aspirational improvements that could not be made. (You don’t need to recreate their plots in order to improve them) fisheries &lt;- read_csv(&quot;data/fisheries.csv&quot;) ✅ ⬆️ Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Stretch Practice with Smokers in Whickham A study conducted in Whickham, England recorded participants’ age and smoking status at baseline, and then 20 years later, their health outcome was recorded. Packages Now, we will work with the mosaicData package. Because this is first time we’re using the mosaicData package, you need to make sure to install it first by running install.packages(\"mosaicData\") in the console. library(tidyverse) library(mosaicData) Note that these packages are also loaded in your R Markdown document. The data The data is in the mosaicData package. You can load it with data(Whickham) Take a peek at the codebook with ?Whickham library(performance) performance::compare_performance() Exercises What type of study do you think these data come from: observational or experiment? Why? How many observations are in this dataset? What does each observation represent? How many variables are in this dataset? What type of variable is each? Display each variable using an appropriate visualization. What would you expect the relationship between smoking status and health outcome to be? Create a visualization depicting the relationship between smoking status and health outcome. Briefly describe the relationship, and evaluate whether this meets your expectations. Additionally, calculate the relevant conditional probabilities to help your narrative. Here is some code to get you started: Whickham %&gt;% count(smoker, outcome) Create a new variable called age_cat using the following scheme: age &lt;= 44 ~ \"18-44\" age &gt; 44 &amp; age &lt;= 64 ~ \"45-64\" age &gt; 64 ~ \"65+\" Re-create the visualization depicting the relationship between smoking status and health outcome, faceted by age_cat. What changed? What might explain this change? Extend the contingency table from earlier by breaking it down by age category and use it to help your narrative. We can use the contingency table to examine how the relationship between smoking status and health outcome differs between different age groups. This extension will help us better understand the patterns we see in the visualization, and explain any changes we observe. Whickham %&gt;% count(smoker, age_cat, outcome) Wrapping up Go back through your write up to make sure you’re following coding style guidelines we discussed in class. Make any edits as needed. Also, make sure all of your R chunks are properly labeled, and your figures are reasonably sized. More ugly charts Want to see more ugly charts? Flowing Data - Ugly Charts Reddit - Data is ugly Missed Opportunities and Graphical Failures (Mostly Bad) Graphics and Tables "],["welcome-to-web-scraping.html", "48 Welcome to web scraping 48.1 Module Materials 48.2 Estimated Video Length", " 48 Welcome to web scraping This module is designed to introduce you to the basic ideas behind web scraping. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 48.1 Module Materials Slides from Lectures Webscraping Suggested Readings All subchapters of this module R4DS, Section on functions Wrap Scraping from An Introduction to Statistical Programming Methods with R Activities Scraping IMDB Lab Better Viz 48.2 Estimated Video Length No of videos : 3 Average length of video : 14 minutes, 40 seconds Total length of playlist : 44 minutes "],["lecture-scraping-the-web.html", "49 Lecture: Scraping the web 49.1 Using the SelectorGadget 49.2 Top 250 movies on IMDB 49.3 Activity 08: IMDB 49.4 Useful RegEx things", " 49 Lecture: Scraping the web You can follow along with the slides here if they do not appear below. 49.1 Using the SelectorGadget 49.2 Top 250 movies on IMDB 49.3 Activity 08: IMDB You can find the materials for this activity here. This time we will be working with .R files, which offer a straightforward method for executing R scripts. Unlike .Rmd (R Markdown) files, which integrate R code with Markdown for a blend of code, output, and narrative, ideal for comprehensive reports and presentations, .R files are focused on script execution, making them simpler for direct script-based activities. 49.4 Useful RegEx things The only cheat sheet I am going to tape to my wall! https://t.co/NMgtnI5cC9I don&#39;t want to have two problems (https://t.co/isLUg0C8Cu) #rstats #regex pic.twitter.com/9Jf3cDEg4F&mdash; Indrajeet Patil (इंद्रजीत पाटील) (@patilindrajeets) November 24, 2021 "],["data-usually-finds-me.html", "50 Data usually finds me 50.1 I don’t go looking for Data … Data usually finds me 50.2 Two Major Approaches to Data Discovery 50.3 The Data Acquisition Spectrum", " 50 Data usually finds me This document is based on my SAM talk on “I don’t go looking for Data … Data usually finds me,” from 2020.Here is a link to the slides of that talk. 50.1 I don’t go looking for Data … Data usually finds me The most interesting aspects of my work (or at least to me) are the aspects related to finding data. However, this part is also the least documented. In my case, it primarily lives in footnotes, personal statements, and appendices. In the world of data science, finding interesting datasets is often more serendipitous than strategic. Let’s explore the various ways data can find us or be found. Fundamentally, data can be discovered through two major approaches: exploratory and confirmatory. The exploratory approach is descriptive and is data-driven. Data scientists often apply this approach. In contrast, the confirmatory approach is question driven, and aimed at testing specific hypotheses. Research scientists often apply this approach, but not always. Both approaches have their merits and challenges, and the methods of data acquisition can vary widely. Let’s delve into the adventure of data retrieval and discover the best places to look for datasets. 50.2 Two Major Approaches to Data Discovery Do you start with the question or with the data? 50.2.1 The Exploratory Approach This data-driven method is often favored by data scientists. It’s descriptive in nature, allowing the data itself to guide the questions and analyses. Most of my early experiences fall into this category, and they’re often the most interesting. In this approach, you start with the data and let it tell you what questions to ask. This can be a bit like a treasure hunt, where you’re not sure what you’ll find until you start digging. The data can be found in various ways, including referrals, reading, rumor, and random chance. We’ll explore these methods in more detail later. But, how does data find you? Data can find you in numerous ways including referrals, reading, rumor, and random chance. A referral example would be when a speaker tells you about an intergenerational data set partially run by the BLS. You may also just stumble across it in your readings, such as when a historian using aspects of a marriage study. A rumor may inspire you, such as observing that a control group is mentioned in the original write-up of the Terman study (1921ish). Serendipity might lead you to fly to SPSP, talk to the person sitting next to you about a study you were in… 50.2.2 Confirmatory Approach to Archival Data In contrast, this question-driven approach is aimed at testing specific hypotheses. It’s commonly used by research scientists, though not exclusively. Here, you start with specific questions that guide your data search. These could be related to theories, measures, subjects, models, replication, or external motivations. Theory-based questions include questions like “Do smart girls delay sex? Measurement based questions can ask things like”Is Coding Speed from the ASVAB a decent proxy for conscientiousness?“. Questions about subjects include”Where can I find Twins Raised Apart?” Modeling questions can include things like How do I illustrate my dual mediated survival model? Replication: Can I replicate my finding in another sample? Externally-inspired questions can include things like Can I address reviewer two’s concern about reliability of difference scores? These questions narrow your search… Otherwise the scope of data is overwhelming. The wonderful Kathy Shields helped me add a section to the WFU library website to get you started guides.zsr.wfu.edu/psychology. This is a great place to start your search for data. Regardless of which approach you take, you’ll need to acquire data. Let’s look at the various ways this can happen. 50.3 The Data Acquisition Spectrum Data acquisition methods generally fall into five categories: Direct Download: The simplest method, where data is available in ready-to-use formats like CSV or Excel files. Wrapped API Access: Using specialized tools or packages to access data repositories. Raw API Interaction: Crafting custom queries for more specific data needs. Web Scraping: Extracting data embedded in web pages. Physical Retrieval: Sometimes, data isn’t digital and requires old-fashioned legwork. Now, let’s explore how these methods play out in real-world scenarios. 50.3.1 How Data Finds You In the exploratory approach, data can appear in your life through various means: Referrals: A conference speaker might mention an intriguing dataset, leading you to directly download it from a repository. Reading: While reviewing literature, you might stumble upon a study with online supplementary data, requiring web scraping to access. Rumors: Colleagues might discuss an old study with valuable data, prompting you to track down physical records. Random chance: A conversation at a conference could lead you to a researcher with access to unique datasets. For the confirmatory approach, your specific questions guide your search through these acquisition methods. For example: - Investigating “Do smart girls delay sexual activity?” might lead you to directly download relevant datasets from the BLS. - Exploring ASVAB’s Coding Speed as a proxy for conscientiousness could involve using a wrapped API to access specific datasets. - Finding data on twins raised apart might require raw API interactions with multiple sources. 50.3.2 The Adventure of Data Retrieval Sometimes, acquiring data involves multiple methods and unexpected challenges. Here’s a real-world example that spans the acquisition spectrum: Apply for access to a “digitized” dataset on Dataverse (Direct Download attempt) for an econometrics class Discover the dataset isn’t actually digitized, requiring physical retrieval and approval from the researchers. Learn that the researchers who created it are hard to find. One researcher (E. Lowell Kelly) died in 1986 and the other (James Conley) is nowhere to be found. Track down a researcher who changed their name in the 1990s by Teaming up with a 2nd-year assistant professor (Josh Jackson) to find Conley Tracking down James Connolly, who legally changed his name in 1992 (or so) from James Conley Convince the researcher to share the data. Determine the data’s location and retrieve boxes from various archives and libraries: Henry A. Murray Research Archive (Part of Harvard’s Dataverse) Jim Connolly’s office Bentley Library (Part of Michigan’s ICPSR) Retrieve boxes of data from various locations, including research archives and libraries. 50.3.3 Where to Look Great places to start your data search (or let it find you) include: - ICPSR (http://icpsr.org): ~15,000 datasets - Harvard Dataverse (http://dataverse.harvard.edu/): ~95,000 datasets - Data.gov (https://catalog.data.gov/dataset): ~250,000 datasets - Other resources: OSF(http://osf.io/), Figshare(http://figshare.com/), Dryad (http://datadryad.org) and field-specific repositories - Nature has a great list of repositories: https://www.nature.com/sdata/policies/repositories - ZSR Guide Remember, the more hurdles between you and the data, the less likely you are to be scooped! Whether you’re letting data find you or actively seeking it out, keep an open mind. The journey to finding the right dataset can be as illuminating as the analysis itself. It&#39;s officially published and all!&quot;Scan Once, Analyse Many: Using large open-access neuroimaging datasets to understand the brain&quot;https://t.co/Ubi4nBTqRxIt&#39;s written to be an introductory guide for those entering the world of neuroimaging using secondary data, 1/2&mdash; Dr Christopher Madan  (he/him) (@cMadan) May 11, 2021 "],["api-wrappers.html", "51 Use API-wrapping packages 51.1 The Data Acquisition Spectrum 51.2 Direct Download 51.3 Data supplied on the web 51.4 Streamlined Data Retrieval with API Wrappers 51.5 Conclusion", " 51 Use API-wrapping packages This section explores how to use R packages that wrap APIs, allowing easy access to web data. We’ll cover various approaches from simple downloads to more complex API interactions. This content builds on Jenny Bryan’s stat545 materials, with significant updates and additions to reflect current best practices. For a more in-depth look at APIs, check out the API chapter in the R for Data Science book. For a huge list of free apis to play with, check out this list Google trends analytics is helpful in the study of global web search patterns.The {gtrends} function from {gtrendsR}  helps extract and visualize this data for specified periods and geolocations https://t.co/yS01ELq5q4#rstats #DataScience pic.twitter.com/mhGTSXB2rN&mdash; R Function A Day (@rfunctionaday) April 13, 2021 51.1 The Data Acquisition Spectrum When it comes to obtaining data from the internet, we can categorize methods into four main type * Direct Download: Grabbing readily available flat files (CSV, XLS, etc.) * Wrapped API Access: Using R packages designed for specific APIs * Raw API Interaction: Crafting custom queries for APIs * Web Scraping: Extracting data embedded in HTML structures We’ll focus primarily on the second method, but now you know about the full spectrum of options at your disposal. For a comprehensive list of R tools for interacting with the internet, check out the rOpenSci repository. These tools include packages for APIs, scraping, and more. 51.2 Direct Download In the simplest case, the data you need is already on the internet in a tabular format. Effectively, you just need to click and download whatever data you need. There are a couple of strategies here: Use read.csv or readr::read_csv to read the data straight into R. Use the command line program curl to do that work, and place it in a Makefile or shell script (see the section on make for more on this). The second case is most useful when the data you want has been provided in a format that needs cleanup. For example, the World Value Survey makes several datasets available as Excel sheets. The safest option here is to download the .xls file, then read it into R with readxl::read_excel() or something similar. An exception to this is data provided as Google Spreadsheets, which can be read straight into R using the googlesheets package. 51.2.1 From rOpenSci web services page From rOpenSci’s CRAN Task View: Web Technologies and Services: downloader::download() for SSL. curl::curl() for SSL. httr::GET data read this way needs to be parsed later with read.table(). rio::import() can “read a number of common data formats directly from an https:// URL”. Isn’t that very similar to the previous? What about packages that install data? 51.3 Data supplied on the web Many times, the data that you want is not already organized into one or a few tables that you can read directly into R. More frequently, you find this data is given in the form of an API. Application Programming Interfaces (APIs) are descriptions of the kind of requests that can be made of a certain piece of software, and descriptions of the kind of answers that are returned. Many sources of data – databases, websites, services – have made all (or part) of their data available via APIs over the internet. Computer programs (“clients”) can make requests of the server, and the server will respond by sending data (or an error message). This client can be many kinds of other programs or websites, including R running from your laptop. 51.4 Streamlined Data Retrieval with API Wrappers Many common web services and APIs have been “wrapped”, i.e. R functions have been written around them which send your query to the server and format the response. This is a great way to get started with APIs, as you don’t need to worry about the details of the API itself. You can just focus on the data you want to get. API-wrapping packages act as intermediaries between your R environment and web services. They handle the nitty-gritty of API calls, authentication, and data parsing, allowing you to focus on analysis rather than data acquisition logistics. These packages are especially useful for beginners, as they abstract away the complexities of web service interaction. For added bonuses, they often ensure that the data you receive is actually from the API you intended to query, and they provide a structured reproducible way to access the data. 51.4.1 Case Study: Ornithological Data with rebird Let’s dive into a practical example using the rebird package, which interfaces with the eBird database. eBird lets birders upload sightings of birds, and allows everyone access to those data. rebird makes it easy to access this data from R (as long as you request an API key) library(tidyverse) library(kableExtra) library(rebird) First, let’s fetch recent bird sightings from a specific location: 51.4.1.1 Search birds by geography The eBird website categorizes some popular locations as “Hotspots”. These are areas where there are both lots of birds and lots of birders. One such location is at Iona Island, near Vancouver. You can see data for this Hotspot at http://ebird.org/ebird/hotspot/L261851. At that link, you will see a page like this: Figure 51.1: Iona Island The data already looks to be organized in a data frame! rebird allows us to read these data directly into R (the ID code for Iona Island is “L261851”). Note that this requires an API key which you have to request from ebird via this link . I have set my key as an environment variable. However you can set it as a global variable in your R session. Like this: ebirdkey &lt;- &quot;SECRET API KEY&quot; ebirdregion(loc = &quot;L261851&quot;, key = ebirdkey) %&gt;% head() %&gt;% kable() speciesCode comName sciName locId locName obsDt howMany lat lng obsValid obsReviewed locationPrivate subId exoticCategory mallar3 Mallard Anas platyrhynchos L261851 Iona Island (General) 2025-02-16 12:04 18 49.2 -123 TRUE FALSE FALSE S213887630 NA gnwtea Green-winged Teal Anas crecca L261851 Iona Island (General) 2025-02-16 12:04 2 49.2 -123 TRUE FALSE FALSE S213887630 NA glwgul Glaucous-winged Gull Larus glaucescens L261851 Iona Island (General) 2025-02-16 12:04 24 49.2 -123 TRUE FALSE FALSE S213887630 NA baleag Bald Eagle Haliaeetus leucocephalus L261851 Iona Island (General) 2025-02-16 12:04 5 49.2 -123 TRUE FALSE FALSE S213887630 NA amecro American Crow Corvus brachyrhynchos L261851 Iona Island (General) 2025-02-16 12:04 7 49.2 -123 TRUE FALSE FALSE S213887630 NA bkcchi Black-capped Chickadee Poecile atricapillus L261851 Iona Island (General) 2025-02-16 12:04 1 49.2 -123 TRUE FALSE FALSE S213887630 NA We can use the function ebirdgeo() to get a list for an area (note that South and West are negative): vanbirds &lt;- ebirdgeo(lat = 49.2500, lng = -123.1000, key = ebirdkey) vanbirds %&gt;% head() %&gt;% kable() speciesCode comName sciName locId locName obsDt howMany lat lng obsValid obsReviewed locationPrivate subId exoticCategory sursco Surf Scoter Melanitta perspicillata L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 6 49.3 -123 TRUE FALSE FALSE S214088938 NA commer Common Merganser Mergus merganser L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 2 49.3 -123 TRUE FALSE FALSE S214088938 NA annhum Anna’s Hummingbird Calypte anna L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 1 49.3 -123 TRUE FALSE FALSE S214088938 NA blkoys Black Oystercatcher Haematopus bachmani L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 1 49.3 -123 TRUE FALSE FALSE S214088938 NA mewgul2 Short-billed Gull Larus brachyrhynchus L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 1 49.3 -123 TRUE FALSE FALSE S214088938 NA glwgul Glaucous-winged Gull Larus glaucescens L292545 West Vancouver–Lighthouse Park 2025-02-17 09:22 12 49.3 -123 TRUE FALSE FALSE S214088938 NA Note: Check the defaults on this function (e.g. radius of circle, time of year). We can also search by “region”, which refers to short codes which serve as common shorthands for different political units. For example, France is represented by the letters FR. frenchbirds &lt;- ebirdregion(&quot;FR&quot;, key = ebirdkey) frenchbirds %&gt;% head() %&gt;% kable() speciesCode comName sciName locId locName obsDt howMany lat lng obsValid obsReviewed locationPrivate subId exoticCategory comcha Common Chaffinch Fringilla coelebs L18233765 Martorey 2025-02-17 18:43 4 45.6 6.75 TRUE FALSE TRUE S214089063 NA combuz1 Common Buzzard Buteo buteo L18233765 Martorey 2025-02-17 18:43 1 45.6 6.75 TRUE FALSE TRUE S214089063 NA blutit Eurasian Blue Tit Cyanistes caeruleus L18233765 Martorey 2025-02-17 18:43 2 45.6 6.75 TRUE FALSE TRUE S214089063 NA coatit2 Coal Tit Periparus ater L18233765 Martorey 2025-02-17 18:43 1 45.6 6.75 TRUE FALSE TRUE S214089063 NA carcro1 Carrion Crow Corvus corone L18233765 Martorey 2025-02-17 18:43 6 45.6 6.75 TRUE FALSE TRUE S214089063 NA gretit1 Great Tit Parus major L18233765 Martorey 2025-02-17 18:43 6 45.6 6.75 TRUE FALSE TRUE S214089063 NA Find out when a bird has been seen in a certain place! Choosing a name from vanbirds above (the Bald Eagle): eagle &lt;- ebirdgeo(species = &quot;Haliaeetus leucocephalus&quot;, lat = 42, lng = -76, key = ebirdkey) #&gt; Error in ebird_GET(url, args, key = key, ...): 400 BAD_REQUEST -- error.data.unknown_species -- Field sci of rawDataCmd: Unknown species: Haliaeetus leucocephalus eagle %&gt;% head() %&gt;% kable() #&gt; Error: object &#39;eagle&#39; not found rebird knows where you are: ebirdgeo(species = &quot;Buteo lagopus&quot;, key = ebirdkey) #&gt; Warning: As a complete lat/long pair was not provided, your location was #&gt; determined using your computer&#39;s public-facing IP address. This will likely not #&gt; reflect your physical location if you are using a remote server or proxy. #&gt; Error in ebird_GET(url, args, key = key, ...): 400 BAD_REQUEST -- error.data.unknown_species -- Field sci of rawDataCmd: Unknown species: Buteo lagopus 51.4.2 Searching geographic info: geonames rOpenSci has a package called geonames for accessing the GeoNames API. First, install the geonames package from CRAN and load it. # install.packages(&quot;geonames&quot;) library(geonames) The geonames package website tells us that there are a few things we need to do before we can use geonames to access the GeoNames API: Go to the GeoNames site and create a new user account. Check your email and follow the instructions to activate your account. You have to manually enable the free web services for your account (Note! You must be logged into your GeoNames account). Tell R your GeoNames username. To do the last step, we could run this line in R… options(geonamesUsername=&quot;my_user_name&quot;) …but this is insecure. We don’t want to risk committing this line and pushing it to our public GitHub page! Instead, we can add this line to our .Rprofile so it will be hidden. One way to edit your .Rprofile is with the helper function edit_r_profile() from the usethis package. Install/load the usethis package and run edit_r_profile() in the R Console: # install.packages(&quot;usethis&quot;) library(usethis) edit_r_profile() This will open up your .Rprofile file. Add options(geonamesUsername=\"my_user_name\") on a new line (replace “my_user_name” with your GeoNames username). Important: Make sure your .Rprofile ends with a blank line! Save the file, close it, and restart R. Now we’re ready to start using geonames to search the GeoNames API. (Also see the Cache credentials for HTTPS chapter of Happy Git and GitHub for the useR.) 51.4.2.1 Using GeoNames What can we do? We can get access to lots of geographical information via the various GeoNames WebServices. countryInfo &lt;- GNcountryInfo() glimpse(countryInfo) #&gt; Rows: 250 #&gt; Columns: 18 #&gt; $ continent &lt;chr&gt; &quot;EU&quot;, &quot;AS&quot;, &quot;AS&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;EU&quot;, &quot;AS&quot;, &quot;AF&quot;, &quot;AN&quot;,… #&gt; $ capital &lt;chr&gt; &quot;Andorra la Vella&quot;, &quot;Abu Dhabi&quot;, &quot;Kabul&quot;, &quot;Saint John… #&gt; $ languages &lt;chr&gt; &quot;ca&quot;, &quot;ar-AE,fa,en,hi,ur&quot;, &quot;fa-AF,ps,uz-AF,tk&quot;, &quot;en-A… #&gt; $ geonameId &lt;chr&gt; &quot;3041565&quot;, &quot;290557&quot;, &quot;1149361&quot;, &quot;3576396&quot;, &quot;3573511&quot;,… #&gt; $ south &lt;chr&gt; &quot;42.4287475&quot;, &quot;22.6315119400001&quot;, &quot;29.3770645357176&quot;,… #&gt; $ isoAlpha3 &lt;chr&gt; &quot;AND&quot;, &quot;ARE&quot;, &quot;AFG&quot;, &quot;ATG&quot;, &quot;AIA&quot;, &quot;ALB&quot;, &quot;ARM&quot;, &quot;AGO… #&gt; $ north &lt;chr&gt; &quot;42.6558875&quot;, &quot;26.0693916590001&quot;, &quot;38.4907920755748&quot;,… #&gt; $ fipsCode &lt;chr&gt; &quot;AN&quot;, &quot;AE&quot;, &quot;AF&quot;, &quot;AC&quot;, &quot;AV&quot;, &quot;AL&quot;, &quot;AM&quot;, &quot;AO&quot;, &quot;AY&quot;,… #&gt; $ population &lt;chr&gt; &quot;77006&quot;, &quot;9630959&quot;, &quot;37172386&quot;, &quot;96286&quot;, &quot;13254&quot;, &quot;28… #&gt; $ east &lt;chr&gt; &quot;1.7866939&quot;, &quot;56.381222289&quot;, &quot;74.8894511481168&quot;, &quot;-61… #&gt; $ isoNumeric &lt;chr&gt; &quot;020&quot;, &quot;784&quot;, &quot;004&quot;, &quot;028&quot;, &quot;660&quot;, &quot;008&quot;, &quot;051&quot;, &quot;024… #&gt; $ areaInSqKm &lt;chr&gt; &quot;468.0&quot;, &quot;82880.0&quot;, &quot;647500.0&quot;, &quot;443.0&quot;, &quot;102.0&quot;, &quot;28… #&gt; $ countryCode &lt;chr&gt; &quot;AD&quot;, &quot;AE&quot;, &quot;AF&quot;, &quot;AG&quot;, &quot;AI&quot;, &quot;AL&quot;, &quot;AM&quot;, &quot;AO&quot;, &quot;AQ&quot;,… #&gt; $ west &lt;chr&gt; &quot;1.4135734&quot;, &quot;51.5904085340001&quot;, &quot;60.4720833972263&quot;, … #&gt; $ countryName &lt;chr&gt; &quot;Principality of Andorra&quot;, &quot;United Arab Emirates&quot;, &quot;I… #&gt; $ postalCodeFormat &lt;chr&gt; &quot;AD###&quot;, &quot;##### #####&quot;, &quot;&quot;, &quot;&quot;, &quot;AI-####&quot;, &quot;####&quot;, &quot;#… #&gt; $ continentName &lt;chr&gt; &quot;Europe&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;North America&quot;, &quot;North Ame… #&gt; $ currencyCode &lt;chr&gt; &quot;EUR&quot;, &quot;AED&quot;, &quot;AFN&quot;, &quot;XCD&quot;, &quot;XCD&quot;, &quot;ALL&quot;, &quot;AMD&quot;, &quot;AOA… This countryInfo dataset is very helpful for accessing the rest of the data because it gives us the standardized codes for country and language. 51.4.2.2 Remixing geonames What are the cities of France? francedata &lt;- countryInfo %&gt;% filter(countryName == &quot;France&quot;) frenchcities &lt;- with(francedata, GNcities( north = north, east = east, south = south, west = west, maxRows = 500 )) glimpse(frenchcities) #&gt; Rows: 133 #&gt; Columns: 12 #&gt; $ lng &lt;chr&gt; &quot;2.3488&quot;, &quot;4.34878349304199&quot;, &quot;7.44744300842285&quot;, &quot;6.13&quot;, … #&gt; $ geonameId &lt;chr&gt; &quot;2988507&quot;, &quot;2800866&quot;, &quot;2661552&quot;, &quot;2960316&quot;, &quot;2993458&quot;, &quot;30… #&gt; $ countrycode &lt;chr&gt; &quot;FR&quot;, &quot;BE&quot;, &quot;CH&quot;, &quot;LU&quot;, &quot;MC&quot;, &quot;JE&quot;, &quot;AD&quot;, &quot;GG&quot;, &quot;ES&quot;, &quot;IT&quot;… #&gt; $ name &lt;chr&gt; &quot;Paris&quot;, &quot;Brussels&quot;, &quot;Bern&quot;, &quot;Luxembourg&quot;, &quot;Monaco&quot;, &quot;Sain… #&gt; $ fclName &lt;chr&gt; &quot;city, village,...&quot;, &quot;city, village,...&quot;, &quot;city, village,.… #&gt; $ toponymName &lt;chr&gt; &quot;Paris&quot;, &quot;Brussels&quot;, &quot;Bern&quot;, &quot;Luxembourg&quot;, &quot;Monaco&quot;, &quot;Sain… #&gt; $ fcodeName &lt;chr&gt; &quot;capital of a political entity&quot;, &quot;capital of a political e… #&gt; $ wikipedia &lt;chr&gt; &quot;en.wikipedia.org/wiki/Paris&quot;, &quot;en.wikipedia.org/wiki/City… #&gt; $ lat &lt;chr&gt; &quot;48.85341&quot;, &quot;50.8504450552593&quot;, &quot;46.9480943365053&quot;, &quot;49.61… #&gt; $ fcl &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;… #&gt; $ population &lt;chr&gt; &quot;2138551&quot;, &quot;1019022&quot;, &quot;121631&quot;, &quot;76684&quot;, &quot;32965&quot;, &quot;28000&quot;,… #&gt; $ fcode &lt;chr&gt; &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;PPLC&quot;, &quot;P… 51.4.3 Wikipedia searching We can use geonames to search for georeferenced Wikipedia articles. Here are those within 20 km of Rio de Janerio, comparing results for English-language Wikipedia (lang = \"en\") and Portuguese-language Wikipedia (lang = \"pt\"): rio_english &lt;- GNfindNearbyWikipedia( lat = -22.9083, lng = -43.1964, radius = 20, lang = &quot;en&quot;, maxRows = 500 ) rio_portuguese &lt;- GNfindNearbyWikipedia( lat = -22.9083, lng = -43.1964, radius = 20, lang = &quot;pt&quot;, maxRows = 500 ) nrow(rio_english) #&gt; [1] 457 nrow(rio_portuguese) #&gt; [1] 500 51.4.4 Is it a boy or a girl? gender-associated names throughout US history The gender package allows you access to data on the gender of names in the US. Because names change gender over the years, the probability of a name belonging to a man or a woman also depends on the year. First, install/load the gender package from CRAN. You may be prompted to also install the companion package, genderdata. Go ahead and say yes. If you don’t see this message no need to worry, it is a one-time install. # install.packages(&quot;gender&quot;) #remotes::install_github(&quot;lmullen/genderdata&quot;) library(gender) #&gt; Warning: package &#39;gender&#39; was built under R version 4.4.2 Let’s do some searches for the name Kelsey. gender(&quot;Kelsey&quot;) #&gt; # A tibble: 1 × 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Kelsey 0.0314 0.969 female 1932 2012 gender(&quot;Kelsey&quot;, years = 1940) #&gt; # A tibble: 1 × 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Kelsey 1 0 male 1940 1940 As you can see, the probability of a name belonging to a specific gender can change over time. df &lt;- gender(&quot;Kelsey&quot;) years &lt;- c(df$year_min:df$year_max) for(i in 1:length(years)){ df &lt;-rbind(df,gender(&quot;Kelsey&quot;, years = years[i])) } df %&gt;% filter(year_min==year_max) %&gt;% ggplot( aes(year_min, proportion_male)) + geom_smooth(span = 0.1) + labs(title = &quot;Proportion of men named Kelsey over time&quot;, x = &quot;Year&quot;, y = &quot;Proporiton Male&quot;) + ggthemes::theme_excel() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; In contrast, the name Mason has been more consistently male. gender(&quot;Mason&quot;) #&gt; # A tibble: 1 × 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mason 0.987 0.0134 male 1932 2012 gender(&quot;Mason&quot;, years = 1940) #&gt; # A tibble: 1 × 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mason 1 0 male 1940 1940 mason &lt;- gender(&quot;Mason&quot;) years &lt;- c(mason$year_min:mason$year_max) df &lt;- rbind(df,gender(&quot;Mason&quot;)) for(i in 1:length(years)){ df &lt;-rbind(df,gender(&quot;Mason&quot;, years = years[i])) } df %&gt;% filter(year_min==year_max&amp;name==&quot;Mason&quot;) %&gt;% ggplot( aes(year_min, proportion_male)) + geom_point() + geom_smooth(span = 0.1) + labs(title = &quot;Proportion of men named Mason over time&quot;, x = &quot;Year&quot;, y = &quot;Proporiton Male&quot;) + ggthemes::theme_excel() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; And when we compare the two names, we see that Kelsey has changed a lot more over time than Mason. df %&gt;% filter(year_min==year_max) %&gt;% ggplot( aes(year_min, proportion_male)) + geom_point() + geom_smooth(span = 0.1) + labs(title = &quot;Proportion of men over time&quot;, x = &quot;Year&quot;, y = &quot;Proporiton Male&quot;) + ggthemes::theme_excel() + facet_wrap(~name) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 51.5 Conclusion API-wrapping packages in R provide powerful tools for accessing diverse data sources. As you progress in your data science journey, mastering these tools will greatly expand the range of data available for your analyses. Remember to always check the terms of service for any API you use, and be mindful of rate limits and data usage policies. "],["diy-web-data.html", "52 DIY web data 52.1 Interacting with an API 52.2 Intro to JSON and XML 52.3 Introducing the easy way: httr 52.4 Scraping 52.5 Scraping via CSS selectors 52.6 Random observations on scraping 52.7 Extras", " 52 DIY web data In earlier sections, we explored R packages that simplify working with web data by “wrapping” APIs—handling request creation and output formatting. But what happens behind the scenes? How do these functions actually interact with web-based data sources? This section builds on that foundation by taking a closer look at the mechanics of API requests and responses. Instead of relying on pre-built wrappers, we’ll construct API requests manually, interpret structured data formats like JSON and XML, and explore best practices for handling web-based data. By the end, you’ll have a deeper understanding of how R interacts with APIs, allowing you to retrieve, parse, and integrate online data into your analysis. These notes, adapted from Jenny Bryan’s stat545 and originally written by Andrew MacDonald, provide a hands-on guide to these concepts with practical examples. No OMDb key available. Code chunks will not be evaluated. 52.1 Interacting with an API Earlier, we experimented with several packages that “wrapped” APIs. They handle request creation and output formatting. In this section, we’re going to look at (part of) what these functions were doing. 52.1.1 Loading Required Packages We will be using the functions from the tidyverse throughout this chapter, so go ahead and load tidyverse package now. library(tidyverse) 52.1.2 Understanding API Requests with the Open Movie Database First, we’re going to examine the structure of API requests using the Open Movie Database (OMDb). OMDb is similar to IMDb but has a simpler API. We can go to the website, input some search parameters, and obtain both the XML query and the response from it. Exercise: determine the shape of an API request. Scroll down to the “Examples” section on the OMDb site and play around with the parameters. Take a look at the resulting API call and the query you get back. If we enter the following parameters: title = Interstellar, year = 2014, plot = full, response = JSON Here is what we see: The request URL is: http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=full Notice the pattern in the request. Let’s try changing the response field from JSON to XML. Now the request URL is: http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=full&amp;r=xml Try pasting these URLs into your browser. You should see this if you tried the first URL: {&quot;Response&quot;:&quot;False&quot;,&quot;Error&quot;:&quot;No API key provided.&quot;} …and this if you tried the second URL (where r=xml): &lt;root response=&quot;False&quot;&gt; &lt;error&gt;No API key provided.&lt;/error&gt; &lt;/root&gt; 52.1.3 Create an OMDb API Key This response tells us that we need an API key to access the OMDb API. We will store our key for the OMDb API in our .Renviron file using the helper function edit_r_environ() from the usethis package. Follow these steps: Visit this URL and request your free API key: https://www.omdbapi.com/apikey.aspx Check your email and follow the instructions to activate your key. Install/load the usethis package and run edit_r_environ() in the R Console: # install.packages(&quot;usethis&quot;) library(usethis) edit_r_environ() Add OMDB_API_KEY=&lt;your-secret-key&gt; on a new line, press enter to add a blank line at the end (important!), save the file, and close it. Note that we use &lt;your-secret-key&gt; as a placeholder here and throughout these instructions. Your actual API key will look something like: p319s0aa (no quotes or other characters like &lt; or &gt; should go on the right of the = sign). Restart R. You can now access your OMDb API key from the R console and save it as an object: Sys.getenv(&quot;OMDB_API_KEY&quot;) We can use this to easily add our API key to the request URL. Let’s make this API key an object we can refer to as movie_key: # save it as an object movie_key &lt;- Sys.getenv(&quot;OMDB_API_KEY&quot;) 52.1.3.1 Alternative strategy for keeping keys: .Rprofile Remember to protect your key! It is important for your privacy. You know, like a key. Now we follow the rOpenSci tutorial on API keys: Add .Rprofile to your .gitignore !! Make a .Rprofile file (windows tips; mac tips). Write the following in it: options(OMBD_API_KEY = &quot;YOUR_KEY&quot;) Restart R (i.e. reopen your RStudio project). This code adds another element to the list of options, which you can see by calling options(). Part of the work done by rplos::searchplos() and friends is to go and obtain the value of this option with the function getOption(\"OMBD_API_KEY\"). This indicates two things: Spelling is important when you set the option in your .Rprofile You can do a similar process for an arbitrary package or key. For example: ## in .Rprofile options(&quot;this_is_my_key&quot; = XXXX) ## later, in the R script: key &lt;- getOption(&quot;this_is_my_key&quot;) This approach is a simple way to keep your keys private, especially when sharing authentication across multiple projects. 52.1.3.2 A few timely reminders about your .Rprofile print(&quot;This is Andrew&#39;s Rprofile and you can&#39;t have it!&quot;) options(OMBD_API_KEY = &quot;XXXXXXXXX&quot;) It must end with a blank line! It lives in the project’s working directory, i.e. the location of your .Rproj. It must be gitignored. Remember that using .Rprofile makes your code un-reproducible. In this case, that is exactly what we want! 52.1.4 Recreate the request URL in R How can we recreate the same request URLs in R? We could use the glue package to paste together the base URL, parameter labels, and parameter values: request &lt;- glue::glue(&quot;http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=short&amp;r=xml&amp;apikey={movie_key}&quot;) request This code works, but it only works for a movie titled Interstellar from 2014 where we want the short plot in an XML format. Let’s try to pull out more variables and paste them in with glue: glue::glue(&quot;http://www.omdbapi.com/?t={title}&amp;y={year}&amp;plot={plot}&amp;r={format}&amp;apikey={api_key}&quot;, title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key ) We could go even further and make this code into a function called omdb() that we can reuse more easily. omdb &lt;- function(title, year, plot, format, api_key) { glue::glue(&quot;http://www.omdbapi.com/?t={title}&amp;y={year}&amp;plot={plot}&amp;r={format}&amp;apikey={api_key}&quot;) } 52.1.5 Get data using the curl package Now we have a handy function that returns the API query. We can paste in the link, but we can also obtain data from within R using the curl package. Install/load the curl package first. # install.packages(&quot;curl&quot;) library(curl) Using curl to get the data in XML format: request_xml &lt;- omdb( title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key ) con &lt;- curl(request_xml) answer_xml &lt;- readLines(con, warn = FALSE) close(con) answer_xml Using curl to get the data in JSON format: request_json &lt;- omdb( title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;json&quot;, api_key = movie_key ) con &lt;- curl(request_json) answer_json &lt;- readLines(con, warn = FALSE) close(con) answer_json We have two forms of data that are obviously structured. What are they? 52.2 Intro to JSON and XML There are two common languages of web services: JavaScript Object Notation (JSON) eXtensible Markup Language (XML) Here’s an example of JSON (from this wonderful site): { &quot;crust&quot;: &quot;original&quot;, &quot;toppings&quot;: [&quot;cheese&quot;, &quot;pepperoni&quot;, &quot;garlic&quot;], &quot;status&quot;: &quot;cooking&quot;, &quot;customer&quot;: { &quot;name&quot;: &quot;Brian&quot;, &quot;phone&quot;: &quot;573-111-1111&quot; } } And here is XML (also from this site): &lt;order&gt; &lt;crust&gt;original&lt;/crust&gt; &lt;toppings&gt; &lt;topping&gt;cheese&lt;/topping&gt; &lt;topping&gt;pepperoni&lt;/topping&gt; &lt;topping&gt;garlic&lt;/topping&gt; &lt;/toppings&gt; &lt;status&gt;cooking&lt;/status&gt; &lt;/order&gt; You can see that both of these data structures are quite easy to read. They are “self-describing”. In other words, they tell you how they are meant to be read. There are easy means of taking these data types and creating R objects. 52.2.1 Parsing the JSON response with jsonlite Our JSON response above can be parsed using jsonlite::fromJSON(). First install/load the jsonlite package. # install.packages(&quot;jsonlite&quot;) library(jsonlite) Parsing our JSON response with fromJSON(): answer_json %&gt;% fromJSON() The output is a named list. A familiar and friendly R structure. Because data frames are lists and because this list has no nested lists-within-lists, we can coerce it very simply: answer_json %&gt;% fromJSON() %&gt;% as_tibble() %&gt;% glimpse() 52.2.2 Parsing the XML response using xml2 We can use the xml2 package to wrangle our XML response. # install.packages(&quot;xml2&quot;) library(xml2) Parsing our XML response with read_xml(): (xml_parsed &lt;- read_xml(answer_xml)) Not exactly the result we were hoping for! However, this does tell us about the XML document’s structure: It has a &lt;root&gt; node, which has a single child node, &lt;movie&gt;. The information we want is all stored as attributes (e.g. title, year, etc.). The xml2 package has various functions to assist in navigating through XML. We can use the xml_children() function to extract all of the children nodes (i.e. the single child, &lt;movie&gt;): (contents &lt;- xml_contents(xml_parsed)) The xml_attrs() function “retrieves all attribute values as a named character vector”. Let’s use this to extract the information that we want from the &lt;movie&gt; node: (attrs &lt;- xml_attrs(contents)[[1]]) We can transform this named character vector into a data frame with the help of dplyr::bind_rows(): attrs %&gt;% bind_rows() %&gt;% glimpse() 52.3 Introducing the easy way: httr httr is yet another star in the tidyverse. It is a package designed to facilitate all things HTTP from within R. This includes the major HTTP verbs, which are: GET() - Fetch an existing resource. The URL contains all the necessary information the server needs to locate and return the resource. POST() - Create a new resource. POST requests usually carry a payload that specifies the data for the new resource. PUT() - Update an existing resource. The payload may contain the updated data for the resource. DELETE() - Delete an existing resource. HTTP is the foundation for APIs; understanding how it works is the key to interacting with all the diverse APIs out there. An excellent beginning resource for APIs (including HTTP basics) is An Introduction to APIs by Brian Cooksey. httr also facilitates a variety of authentication protocols. httr contains one function for every HTTP verb. The functions have the same names as the verbs (e.g. GET(), POST()). They have more informative outputs than simply using curl and come with nice convenience functions for working with the output: # install.packages(&quot;httr&quot;) library(httr) Using httr to get the data in JSON format: request_json &lt;- omdb( title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;json&quot;, api_key = movie_key ) response_json &lt;- GET(request_json) content(response_json, as = &quot;parsed&quot;, type = &quot;application/json&quot;) Using httr to get the data in XML format: request_xml &lt;- omdb( title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key ) response_xml &lt;- GET(request_xml) content(response_xml, as = &quot;parsed&quot;) httr also gives us access to lots of useful information about the quality of our response. For example, the header: headers(response_xml) And also a handy means to extract specifically the HTTP status code: status_code(response_xml) In fact, we didn’t need to create omdb() at all. httr provides a straightforward means of making an HTTP request with the query argument: the_martian &lt;- GET(&quot;http://www.omdbapi.com/?&quot;, query = list( t = &quot;The Martian&quot;, y = 2015, plot = &quot;short&quot;, r = &quot;json&quot;, apikey = movie_key ) ) content(the_martian) With httr, we are able to pass in the named arguments to the API call as a named list. We are also able to use spaces in movie names; httr encodes these in the URL before making the GET request. It is very good to learn your HTTP status codes. The documentation for httr includes a vignette of “Best practices for writing an API package”, which is useful for when you want to bring your favorite web resource into the world of R. 52.4 Scraping What if data is present on a website, but isn’t provided in an API at all? It is possible to grab that information too. How easy that is to do depends a lot on the quality of the website that we are using. HTML is a structured way of displaying information. It is very similar in structure to XML (in fact many modern html sites are actually XHTML5, which is also valid XML). Two pieces of equipment: The rvest package (cran; GitHub). Install via install.packages(\"rvest)\". SelectorGadget: point and click CSS selectors. Install in your browser. Before we go any further, let’s play a game together! 52.4.1 Obtain a table Let’s make a simple HTML table and then parse it. Make a new, empty project Make a totally empty .Rmd file and save it as \"GapminderHead.Rmd\" Copy this into the body: --- output: html_document --- ```{r echo=FALSE, results=&#39;asis&#39;} library(gapminder) knitr::kable(head(gapminder)) ``` Knit the document and click “View in Browser”. It should look like this: We have created a simple HTML table with the head of gapminder in it! We can get our data back by parsing this table into a data frame again. Extracting data from HTML is called “scraping”, and we can do it in R with the rvest package: # install.packages(&quot;rvest&quot;) library(rvest) read_html(&quot;GapminderHead.html&quot;) %&gt;% html_table() 52.5 Scraping via CSS selectors Let’s practice scraping websites using our newfound abilities. Here is a table of research publications by country. We can try to get this data directly into R using read_html() and html_table(): research &lt;- read_html(&quot;https://www.scimagojr.com/countryrank.php&quot;) %&gt;% html_table(fill = TRUE) If you look at the structure of research (i.e. via str(research)) you’ll see that we’ve obtained a list of data.frames. The top of the page contains another table element. This was also scraped! Can we be more specific about what we obtain from this page? We can, by highlighting that table with CSS selectors: research &lt;- read_html(&quot;http://www.scimagojr.com/countryrank.php&quot;) %&gt;% html_node(&quot;.tabla_datos&quot;) %&gt;% html_table() glimpse(research) 52.6 Random observations on scraping Make sure you’ve obtained ONLY what you want! Scroll over the whole page to ensure that SelectorGadget hasn’t found too many things. If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking (e.g. being more precise). MOST IMPORTANTLY confirm that there is NO rOpenSci package and NO API before you spend hours scraping (the API was right here). 52.7 Extras 52.7.1 Airports First, go to this website about Airports. Follow the link to get your API key (you will need to click a confirmation email). List of all the airports on the planet: https://airport.api.aero/airport/?user_key={yourkey} List of all the airports matching Toronto: https://airport.api.aero/airport/match/toronto?user_key={yourkey} The distance between YVR and LAX: https://airport.api.aero/airport/distance/YVR/LAX?user_key={yourkey} Do you need just the US airports? This API does that (also see this) and is free. "],["lab07.html", "53 Lab: Better Viz Conveying the right message through visualization Learning Goals Getting started Exercises", " 53 Lab: Better Viz Conveying the right message through visualization In this lab, our goal is to reconstruct and improve a data visualization concerning COVID-19 and mask-wearing practices. We aim to explore how data visualizations can sometimes mislead and learn techniques to correct these misrepresentations. Learning Goals Critiquing visualizations that misrepresent data Constructing datasets suitable for visual analysis Applying principles of effective data visualizationo to improve clarity and accuracy Getting started Go to the course GitHub organization and locate the template. Clone and then open the R Markdown document. Ensure it compiles without errors to confirm your setup is correct. Warm up Let’s warm up with some simple exercises. Update the YAML of your R Markdown file with your information, knit, commit, and push your changes. Make sure to commit with a meaningful commit message. Then, go to your repo on GitHub and confirm that your changes are visible in your Rmd and md files. If anything is missing, commit and push again. Packages We’ll use the tidyverse package for much of the data wrangling and visualization. This package is already installed for you. You can load it by running the following in your Console: library(tidyverse) Data In this lab, you’ll be constructing the dataset! Exercises The following visualization was shared on Twitter as “extraordinary misleading”. Hey, @maddow and @MaddowBlog @SecNormanas as much as we&#39;d all hope everyone would wear masks, this chart is extraordinary misleading. If you don&#39;t believe me, ask @AlbertoCairo who wrote the book on it. Check the scale on the two axes. pic.twitter.com/JLxxgxzbua&mdash; Jon Boeckenstedt de la Azure Cheque (@JonBoeckenstedt) August 7, 2020 Before you dive in, think about what is misleading about this visualization and how you might go about fixing it. Create a data frame that can be used to re-construct this visualization. You may need to guess some of the numbers, that’s ok. You should first think about how many rows and columns you’ll need and what you want to call your variables. Then, you can use the tribble() function for this. For example, if you wanted to construct the following data frame df #&gt; # A tibble: 3 × 2 #&gt; date count #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1/1/2020 15 #&gt; 2 2/1/2020 20 #&gt; 3 3/1/2020 22 you can write df &lt;- tribble( ~date, ~count, &quot;1/1/2020&quot;, 15, &quot;2/1/2020&quot;, 20, &quot;3/1/2020&quot;, 22, ) Make a visualization that more accurately (and honestly) reflects the data and conveys a clear message. What message is more clear in your visualization than it was in the original visualization? What, if any, useful information do these data and your visualization tell us about mask wearing and COVID? It’ll be difficult to set aside what you already know about mask wearing, but you should try to focus only on what this visualization tells. Feel free to also comment on whether that lines up with what you know about mask wearing. Using the same dataset you constructed, your goal now is to create a new visualization that intentionally conveys the opposite message of your previous, accurate visualization. This exercise is designed to highlight the impact of visualization choices on the interpretation of data. It’s a practical exploration of how changing the presentation can alter the perceived message, underscoring the ethical implications of data visualization. Reflect on the message conveyed by your accurate visualization regarding mask-wearing and COVID-19. Discuss the key factors that contribute to this message, such as the variables used, the scale of the axes, and the type of visualization. Plan Your Opposite Visualization: Briefly determine what opposite message you want to covey. Consider the data you have available (or could easily add). Use visualization techniques to craft a chart or graph that conveys this contrary perspective. Pay careful attention to how different visualization choices, like altering the y-axis scale or changing the chart type, can influence the message received by the audience.  ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work. "],["welcome-to-functions-and-automation.html", "54 Welcome to Functions and Automation 54.1 Module Materials", " 54 Welcome to Functions and Automation This module is designed to introduce functions. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 54.1 Module Materials Slides from Lectures Functions and Automation Activities Functions and Art! Suggested Readings All subchapters of this module, including ‘Notes on Functions’ r4ds Sections on functions, and Iterations Lab Conveying the right message through visualization [Optional Lab Money in Politics][lab08b] "],["functions-1.html", "55 Functions 55.1 Code Along pt 1 55.2 Functions for real 55.3 Code Along pt 2 55.4 Writing Functions", " 55 Functions Wow I’m so humbly grateful much love to y’all&mdash; Missy Elliott (@MissyElliott) April 26, 2021 You can follow along with the slides here if they do not appear below. 55.1 Code Along pt 1 You can find the materials for this activity here. This time we will be working with .R files, which offer a straightforward method for executing R scripts. Unlike .Rmd (R Markdown) files, which integrate R code with Markdown for a blend of code, output, and narrative, ideal for comprehensive reports and presentations, .R files are focused on script execution, making them simpler for direct script-based activities. 55.2 Functions for real 55.3 Code Along pt 2 You can find the materials for this activity here. 55.4 Writing Functions "],["automation.html", "56 Automation 56.1 Code Along pt 3 56.2 Math to Coding", " 56 Automation You can follow along with the slides here if they do not appear below. 56.1 Code Along pt 3 You can find the materials for this activity here. 56.2 Math to Coding btw these large scary math symbols are just for-loops pic.twitter.com/Kq6dcihPp4&mdash; Freya Holmér (@FreyaHolmer) September 11, 2021 "],["functions-part1.html", "57 Write your own R functions 57.1 What and why? 57.2 Load the nycflights13 data 57.3 Example Analysis: Average Delay by Airline 57.4 Get something that works 57.5 Turn the Working Interactive Code into a Function 57.6 Test the Function", " 57 Write your own R functions Writing your own functions in R is a fundamental skill that enhances your ability to perform repetitive tasks efficiently, customize analyses, and improve the readability of your code. A function in R is a set of instructions designed to perform a specific task, which can be as simple or complex as needed. By now, you’ve used plenty of functions in R. Hopefully, you’ve absorbed some of their logic, and have seen first-hand how they simplify complex tasks. It’s time to take that experience and start crafting your own. Doing so isn’t just about following a set of instructions; it’s about embracing the modular, building-block nature of R. This approach doesn’t just make your code smarter; it makes it significantly more readable and customizable. Let’s dive in and transform how you interact with R, turning you from a useR into a creatoR. 57.1 What and why? This section aims to demystify the process experienced R useRs follow to write functions. I want to shed light on the rationale behind each step. Merely looking at the finished product, e.g., source code for R packages, can be extremely deceiving. Reality is generally much uglier … but more interesting! Why are we covering this now, smack in the middle of data aggregation? Powerful machines like dplyr, purrr, and the built-in “apply” family of functions, are ready and waiting to apply your purpose-built functions to various bits of your data. If you can express your analytical wishes in a function, these tools will give you great power. 57.2 Load the nycflights13 data We’ll begin by loading the nycflights13 dataset, which contains information about all flights that departed from New York City in 2013. This dataset provides a rich source of real-world data for practicing data manipulation and analysis library(nycflights13) #&gt; Warning: package &#39;nycflights13&#39; was built under R version 4.4.2 library(dplyr) data(&quot;flights&quot;) #str(flights) 57.3 Example Analysis: Average Delay by Airline Consider we want to compute the average delay experienced by each airline. This is a great example of a typical input for a function. You can imagine wanting to get this statistic to evaluate airline performance. You might want to do this for different years, months, or days of the week. You might want to do this for different airports, or for different combinations of airports. You might want to do this for different types of delays. You might want to do this for different subsets of the data, e.g., only for flights that were delayed. You might want to do this for different airlines. You might want to do this for different combinations of the above. 57.4 Get something that works First, develop some working code for interactive use, using a representative input. I’ll use flights operated by a specific airline as an example. R functions that will be useful: mean() and filter() from the dplyr package. ## Investigate the structure of the flights dataset str(flights) #&gt; tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame) #&gt; $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013.. #&gt; $ month : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ... #&gt; $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ... #&gt; $ dep_delay : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... #&gt; $ arr_time : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ... #&gt; $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ... #&gt; $ arr_delay : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ... #&gt; $ carrier : chr [1:336776] &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... #&gt; $ flight : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 .. #&gt; $ tailnum : chr [1:336776] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... #&gt; $ origin : chr [1:336776] &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... #&gt; $ dest : chr [1:336776] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... #&gt; $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ... #&gt; $ distance : num [1:336776] 1400 1416 1089 1576 762 ... #&gt; $ hour : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ... #&gt; $ minute : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ... #&gt; $ time_hour : POSIXct[1:336776], format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-&quot;.. ## get to know the functions mentioned above mean(flights$dep_delay) #&gt; [1] NA filter(.data = flights, carrier == &quot;AA&quot;) #&gt; # A tibble: 32,729 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 542 540 2 923 850 #&gt; 2 2013 1 1 558 600 -2 753 745 #&gt; 3 2013 1 1 559 600 -1 941 910 #&gt; 4 2013 1 1 606 610 -4 858 910 #&gt; 5 2013 1 1 623 610 13 920 915 #&gt; 6 2013 1 1 628 630 -2 1137 1140 #&gt; 7 2013 1 1 629 630 -1 824 810 #&gt; 8 2013 1 1 635 635 0 1028 940 #&gt; 9 2013 1 1 656 700 -4 854 850 #&gt; 10 2013 1 1 656 659 -3 949 959 #&gt; # ℹ 32,719 more rows #&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Carrier Average Departure Delay 9E 16.44 AA 8.57 AS 5.83 B6 12.97 DL 9.22 EV 19.84 F9 20.20 FL 18.61 HA 4.90 MQ 10.45 OO 12.59 UA 12.02 US 3.75 VX 12.76 WN 17.66 YV 18.90 Now lets go through some natural solutions to get the average delay for the airline “AA” 57.4.1 Using dplyr for Data Filtering and Summary This solution employs the dplyr package to filter flights by the airline code and then calculate the average departure delay. flights %&gt;% filter(carrier == &quot;AA&quot;) %&gt;% summarise(average_delay = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 8.59 57.4.2 Using Base R with Subsetting Here, we use base R to achieve the same task without the dplyr package, directly subsetting the dataframe. mean(flights$dep_delay[flights$carrier==&quot;AA&quot;], na.rm = TRUE) #&gt; [1] 8.59 57.4.3 Using with() Function The with() function provides a convenient way to perform operations within a dataframe subset, making the code more readable. with(flights[flights$carrier == &quot;AA&quot;, ], mean(dep_delay, na.rm = TRUE)) #&gt; [1] 8.59 57.4.4 Using aggregate() Function The aggregate() function in R can be used to compute summary statistics for subgroups of data, which in this case are flights operated by “AA”. aggregate(dep_delay ~ carrier, data = flights[flights$carrier == &quot;AA&quot;, ], FUN = mean, na.rm = TRUE)$dep_delay #&gt; [1] 8.59 57.4.5 Using tapply() Function The tapply() function applies a function to subsets of a vector, which we can use to calculate the average delay for “AA” flights. tapply(flights$dep_delay, flights$carrier, mean, na.rm = TRUE)[&quot;AA&quot;] #&gt; AA #&gt; 8.59 Now, internalize this “answer” because our informal testing relies on you noticing departures from this number when we generalize the function. 57.5 Turn the Working Interactive Code into a Function When crafting your own functions in R, it’s beneficial to start with a straightforward, minimal version that accomplishes the basic task at hand. This approach is akin to building a ‘skateboard’—a simple, yet functional product. Let’s apply this philosophy to our task of calculating the average delay for a specific airline in the nycflights13 dataset. 57.5.1 Initial Simple Function: The ‘Skateboard’ average_delay_by_airline &lt;- function(airline_code) { flights %&gt;% filter(carrier == airline_code) %&gt;% summarise(average_delay = mean(dep_delay, na.rm = TRUE)) } Check that you’re getting the same answer as you did with your interactive code. # Test the function with American Airlines (AA) average_delay_by_airline(&quot;AA&quot;) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 8.59 This function represents our ‘skateboard’. It’s basic, and we have added no new functionality. Yet, it gets the job done by providing the average delay for a given airline code. It doesn’t include error handling or support for additional details like distinguishing between departure and arrival delays, but it serves as a solid starting point. This is a minimal viable product (MVP) that we can build upon to create a more complex function (the ‘car’). Figure 57.1: This image widely attributed to the Spotify development team conveys an important point. From Your ultimate guide to Minimum Viable Product (+great examples) This idea is related to the valuable Telescope Rule: It is faster to make a four-inch mirror then a six-inch mirror than to make a six-inch mirror. 57.6 Test the Function 57.6.1 Test on new inputs Pick some new artificial inputs where you know (at least approximately) what your function should return. average_delay_by_airline(&quot;UA&quot;) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 12.1 I know that UA had about 12 minutes of a delay. 57.6.2 Test on real data but different real data Back to the real world now. So typically, the next step is to check to see if your function can handle different data. This is a good way to check if your function is robust and generalizable. However, ours doesn’t. It’s hard-wired to the flights dataset. We’ll fix that in the next section. average_delay_by_airline &lt;- function(data = flights, airline_code) { data %&gt;% filter(carrier == airline_code) %&gt;% summarise(average_delay = mean(dep_delay, na.rm = TRUE)) } I’ve now added another variable to the function, data, which defaults to flights. This is a good habit to get into. It makes your function more flexible and more generalizable. It also makes it easier to test your function on different datasets. Now, we can test our function on a modified flights dataset, that I have named the flights2 dataset. The only thing I have done to this dataset is multiplied all of the delays by 2. flights2 &lt;- flights flights2$dep_delay &lt;- flights2$dep_delay * 2 average_delay_by_airline(flights2, &quot;AA&quot;) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 17.2 "],["enhancing-the-function-towards-the-perfectly-formed-rear-view-mirror.html", "58 Enhancing the Function: Towards the ‘Perfectly Formed Rear-View Mirror’", " 58 Enhancing the Function: Towards the ‘Perfectly Formed Rear-View Mirror’ Once you have a basic function that works, you can incrementally add features to make it more robust and versatile. This could include error handling (to gracefully deal with unexpected inputs), parameters for selecting between departure and arrival delays, or options for filtering by specific airports or dates. Let’s add the capability to choose between departure (dep_delay) and arrival (arr_delay) delays. # Enhanced function to calculate average delay, with options for delay type and error handling average_delay_by_airline &lt;- function(data = flights, airline_code, delay_type = &quot;dep_delay&quot;) { data %&gt;% filter(carrier == airline_code) %&gt;% summarise(average_delay = mean(get(delay_type), na.rm = TRUE)) } # Test the enhanced function average_delay_by_airline(airline_code = &quot;AA&quot;,delay_type = &quot;arr_delay&quot;) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 0.364 In this version, the function now accepts an additional argument, delay_type, which allows the user to specify the type of delay they are interested in analyzing. The function checks if the provided delay_type is valid and uses dynamic column selection via get(delay_type) to compute the mean delay. "],["test-on-unexpected-inputs.html", "59 Test on Unexpected Inputs 59.1 Error Handling 59.2 Check the validity of arguments 59.3 Wrap-up and what’s next? 59.4 Where were we? Where are we going? 59.5 Load the Gapminder data 59.6 Restore our max minus min function 59.7 Generalize our function to other quantiles 59.8 Get something that works, again 59.9 Turn the working interactive code into a function, again 59.10 Argument names: freedom and conventions 59.11 What a function returns 59.12 Default values: freedom to NOT specify the arguments 59.13 Check the validity of arguments, again 59.14 Wrap-up and what’s next? 59.15 Where were we? Where are we going? 59.16 Load the Gapminder data 59.17 Restore our max minus min function 59.18 Be proactive about NAs 59.19 The useful but mysterious ... argument 59.20 Use testthat for formal unit tests", " 59 Test on Unexpected Inputs Now, let’s try to break our function. Don’t get truly diabolical (yet). Just make the kind of mistakes you can imagine making at 2am when, 3 years from now, you rediscover this useful function you wrote. Give your function inputs it’s not expecting. average_delay_by_airline(flights) #&gt; Error in `filter()`: #&gt; ℹ In argument: `carrier == airline_code`. #&gt; Caused by error: #&gt; ! argument &quot;airline_code&quot; is missing, with no default average_delay_by_airline (airline_code=&quot;AB&quot;) #&gt; # A tibble: 1 × 1 #&gt; average_delay #&gt; &lt;dbl&gt; #&gt; 1 NaN average_delay_by_airline(delay_type = &quot;arr-delay&quot;) #&gt; Error in `filter()`: #&gt; ℹ In argument: `carrier == airline_code`. #&gt; Caused by error: #&gt; ! argument &quot;airline_code&quot; is missing, with no default How happy are you with those error messages? You must imagine that some entire script has failed and that you were hoping to just source() it without re-reading it. If a colleague or future you encountered these errors, do you run screaming from the room? How hard is it to pinpoint the usage problem? 59.1 Error Handling We’ll add some simple checks to ensure that the airline code provided to the function is valid. If it’s not, we’ll return an informative error message. We’ll also add a check to ensure that the delay type provided is valid. average_delay_by_airline &lt;- function(data = flights, airline_code, delay_type = &quot;dep_delay&quot;) { # Check if any of the provided airline codes are not in the dataset if (!any(airline_code %in% c(data$carrier, NA))) { stop(&quot;One or more airline codes not found in the dataset.&quot;) } if (!delay_type %in% c(&quot;dep_delay&quot;, &quot;arr_delay&quot;)) { stop(&quot;Invalid delay type specified. Choose either &#39;dep_delay&#39; or &#39;arr_delay&#39;.&quot;) } data %&gt;% filter(carrier == airline_code) %&gt;% summarise(average_delay = mean(get(delay_type), na.rm = TRUE)) } 59.2 Check the validity of arguments For functions that will be used again – which is not all of them! – it is good to check the validity of arguments. This implements a rule from the Unix philosophy: Rule of Repair: When you must fail, fail noisily and as soon as possible. 59.2.1 stop if not stopifnot() is the entry level solution. I use it here to make sure the input data is a data.frame. mmm &lt;- function(data = flights, airline_code, delay_type = &quot;dep_delay&quot;) { stopifnot(is.data.frame(data)) # Check if any of the provided airline codes are not in the dataset if (!any(airline_code %in% c(data$carrier, NA))) { stop(&quot;One or more airline codes not found in the dataset.&quot;) } if (!delay_type %in% c(&quot;dep_delay&quot;, &quot;arr_delay&quot;)) { stop(&quot;Invalid delay type specified. Choose either &#39;dep_delay&#39; or &#39;arr_delay&#39;.&quot;) } data %&gt;% filter(carrier == airline_code) %&gt;% summarise(average_delay = mean(get(delay_type), na.rm = TRUE)) } mmm(&quot;eggplants are purple&quot;) #&gt; Error in mmm(&quot;eggplants are purple&quot;): is.data.frame(data) is not TRUE And we see that it catches all of the self-inflicted damage we would like to avoid. 59.2.2 if then stop stopifnot() doesn’t provide a helpful error message. The next approach is widely used. Put your validity check inside an if() statement and call stop() yourself, with a custom error message, in the body. mmm2 &lt;- function(x) { if (!is.numeric(x)) { stop( &quot;I am so sorry, but this function only works for numeric input!\\n&quot;, &quot;You have provided an object of class: &quot;, class(x)[1] ) } max(x) - min(x) } mmm2(gapminder) #&gt; Error: object &#39;gapminder&#39; not found In addition to a gratuitous apology, the error raised also contains two more pieces of helpful info: Which function threw the error. Hints on how to fix things: expected class of input vs actual class. If it is easy to do so, I highly recommend this template: “you gave me THIS, but I need THAT”. The tidyverse style guide has a very useful chapter on how to construct error messages. 59.2.3 Sidebar: non-programming uses for assertions Another good use of this pattern is to leave checks behind in data analytical scripts. Consider our repetitive use of Gapminder in this course. Every time we load it, we inspect it, hoping to see the usual stuff. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run ad nauseam on auto-pilot or non-interactively. 59.3 Wrap-up and what’s next? Here’s the function we’ve written so far: mmm2 #&gt; function (x) #&gt; { #&gt; if (!is.numeric(x)) { #&gt; stop(&quot;I am so sorry, but this function only works for numeric input!\\n&quot;, #&gt; &quot;You have provided an object of class: &quot;, class(x)[1]) #&gt; } #&gt; max(x) - min(x) #&gt; } What we’ve accomplished: We’ve written our first function. We are checking the validity of its input, argument x. We’ve done a good amount of informal testing. Where to next? In part 2 we generalize this function to take differences in other quantiles and learn how to set default values for arguments. 59.4 Where were we? Where are we going? In part 1 we wrote our simple R function to compute the difference between the max and min of a numeric vector. We checked the validity of the function’s only argument and, informally, we verified that it worked pretty well. In this part, we generalize this function, learn more technical details about R functions, and set default values for some arguments. 59.5 Load the Gapminder data As usual, load gapminder. library(gapminder) 59.6 Restore our max minus min function Let’s keep our previous function around as a baseline. mmm &lt;- function(x) { stopifnot(is.numeric(x)) max(x) - min(x) } 59.7 Generalize our function to other quantiles The max and the min are special cases of a quantile. Here are other special cases you may have heard of: median = 0.5 quantile 1st quartile = 0.25 quantile 3rd quartile = 0.75 quantile If you’re familiar with box plots, the rectangle typically runs from the 1st quartile to the 3rd quartile, with a line at the median. If \\(q\\) is the \\(p\\)-th quantile of a set of \\(n\\) observations, what does that mean? Approximately \\(pn\\) of the observations are less than \\(q\\) and \\((1 - p)n\\) are greater than \\(q\\). Yeah, you need to worry about rounding to an integer and less/greater than or equal to, but these details aren’t critical here. Let’s generalize our function to take the difference between any two quantiles. We can still consider the max and min, if we like, but we’re not limited to that. 59.8 Get something that works, again The eventual inputs to our new function will be the data x and two probabilities. First, play around with the quantile() function. Convince yourself you know how to use it, for example, by cross-checking your results with other built-in functions. quantile(gapminder$lifeExp) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.7 70.8 82.6 quantile(gapminder$lifeExp, probs = 0.5 ) #&gt; 50% #&gt; 60.7 median(gapminder$lifeExp) #&gt; [1] 60.7 quantile(gapminder$lifeExp, robs = c(0.25, 0.75) ) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.7 70.8 82.6 boxplot(gapminder$lifeExp, plot = FALSE)$stats #&gt; [,1] #&gt; [1,] 23.6 #&gt; [2,] 48.2 #&gt; [3,] 60.7 #&gt; [4,] 70.8 #&gt; [5,] 82.6 Now write a code snippet that takes the difference between two quantiles. the_probs &lt;- c(0.25, 0.75) the_quantiles &lt;- quantile(gapminder$lifeExp, probs = the_probs) max(the_quantiles) - min(the_quantiles) #&gt; [1] 22.6 59.9 Turn the working interactive code into a function, again I’ll use qdiff as the base of our function’s name. I copy the overall structure from our previous “max minus min” work but replace the guts of the function with the more general code we just developed. qdiff1 &lt;- function(x, probs) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } qdiff1(gapminder$lifeExp, probs = c(0.25, 0.75)) #&gt; [1] 22.6 IQR(gapminder$lifeExp) # hey, we&#39;ve reinvented IQR #&gt; [1] 22.6 qdiff1(gapminder$lifeExp, probs = c(0, 1)) #&gt; [1] 59 mmm(gapminder$lifeExp) #&gt; [1] 59 Again we do some informal tests against familiar results and external implementations. 59.10 Argument names: freedom and conventions I want you to understand the importance of argument names. I can name my arguments almost anything I like. Proof: qdiff2 &lt;- function(zeus, hera) { stopifnot(is.numeric(zeus)) the_quantiles &lt;- quantile(x = zeus, probs = hera) max(the_quantiles) - min(the_quantiles) } qdiff2(zeus = gapminder$lifeExp, hera = 0:1) #&gt; [1] 59 Although I can name my arguments after Greek gods, it’s usually a bad idea. Take all opportunities to make things more self-explanatory via meaningful names. Future you will thank you. If you are going to pass the arguments of your function as arguments of a built-in function, consider copying the argument names. Unless you have a good reason to do your own thing (some argument names are bad!), be consistent with the existing function. Again, the reason is to reduce your cognitive load. This is what I’ve been doing all along and now you know why: qdiff1 #&gt; function (x, probs) #&gt; { #&gt; stopifnot(is.numeric(x)) #&gt; the_quantiles &lt;- quantile(x = x, probs = probs) #&gt; max(the_quantiles) - min(the_quantiles) #&gt; } #&gt; &lt;bytecode: 0x0000012cf7578770&gt; We took this detour so you could see there is no structural relationship between my arguments (x and probs) and those of quantile() (also x and probs). The similarity or equivalence of the names accomplishes nothing as far as R is concerned; it is solely for the benefit of humans reading, writing, and using the code. Which is very important! 59.11 What a function returns By this point, I expect someone will have asked about the last line in my function’s body. Look above for a reminder of the function’s definition. By default, a function returns the result of the last line of the body. I am just letting that happen with the line max(the_quantiles) - min(the_quantiles). However, there is an explicit function for this: return(). I could just as easily make this the last line of my function’s body: return(max(the_quantiles) - min(the_quantiles)) You absolutely must use return() if you want to return early based on some condition, i.e. before execution gets to the last line of the body. Otherwise, you can decide your own conventions about when you use return() and when you don’t. 59.12 Default values: freedom to NOT specify the arguments What happens if we call our function but neglect to specify the probabilities? qdiff1(gapminder$lifeExp) #&gt; Error in qdiff1(gapminder$lifeExp): argument &quot;probs&quot; is missing, with no default Oops! At the moment, this causes a fatal error. It can be nice to provide some reasonable default values for certain arguments. In our case, it would not be a good idea to specify a default value for the primary input x, but it would be a good idea to specify a default for probs. We started by focusing on the max and the min, so I think those make reasonable defaults. Here’s how to specify that in a function definition. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } Again we check how the function works, in old examples and new, specifying the probs argument and not. qdiff3(gapminder$lifeExp) #&gt; [1] 59 mmm(gapminder$lifeExp) #&gt; [1] 59 qdiff3(gapminder$lifeExp, c(0.1, 0.9)) #&gt; [1] 33.6 59.13 Check the validity of arguments, again Exercise: upgrade our argument validity checks in light of the new argument probs. ## problems identified during class ## we&#39;re not checking that probs is numeric ## we&#39;re not checking that probs is length 2 ## we&#39;re not checking that probs are in [0,1] 59.14 Wrap-up and what’s next? Here’s the function we’ve written so far: qdiff3 #&gt; function (x, probs = c(0, 1)) #&gt; { #&gt; stopifnot(is.numeric(x)) #&gt; the_quantiles &lt;- quantile(x, probs) #&gt; max(the_quantiles) - min(the_quantiles) #&gt; } #&gt; &lt;bytecode: 0x0000012cf5d1ad98&gt; What we’ve accomplished: We’ve generalized our first function to take a difference between arbitrary quantiles. We’ve specified default values for the probabilities that set the quantiles. Where to next? Next, we tackle NAs, the special ... argument, and formal unit testing. 59.15 Where were we? Where are we going? Previously, we generalized our first R function so it could take the difference between any two quantiles of a numeric vector. We also set default values for the underlying probabilities, so that, by default, we compute the max minus the min. In this part, we tackle NAs, the special argument ... and formal testing. 59.16 Load the Gapminder data As usual, load gapminder. library(gapminder) 59.17 Restore our max minus min function Let’s keep our previous function around as a baseline. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } 59.18 Be proactive about NAs I am being gentle by letting you practice with the Gapminder data. In real life, missing data will make your life a living hell. If you are lucky, it will be properly indicated by the special value NA, but don’t hold your breath. Many built-in R functions have an na.rm = argument through which you can specify how you want to handle NAs. Typically the default value is na.rm = FALSE and typical default behavior is to either let NAs propagate or to raise an error. Let’s see how quantile() handles NAs: z &lt;- gapminder$lifeExp z[3] &lt;- NA quantile(gapminder$lifeExp) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.7 70.8 82.6 quantile(z) #&gt; Error in quantile.default(z): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE quantile(z, na.rm = TRUE) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.8 70.8 82.6 So quantile() simply will not operate in the presence of NAs unless na.rm = TRUE. How shall we modify our function? If we wanted to hardwire na.rm = TRUE, we could. Focus on our call to quantile() inside our function definition. qdiff4 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = TRUE) max(the_quantiles) - min(the_quantiles) } qdiff4(gapminder$lifeExp) #&gt; [1] 59 qdiff4(z) #&gt; [1] 59 This works but it is dangerous to invert the default behavior of a well-known built-in function and to provide the user with no way to override this. We could add an na.rm = argument to our own function. We might even enforce our preferred default – but at least we’re giving the user a way to control the behavior around NAs. qdiff5 &lt;- function(x, probs = c(0, 1), na.rm = TRUE) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = na.rm) max(the_quantiles) - min(the_quantiles) } qdiff5(gapminder$lifeExp) #&gt; [1] 59 qdiff5(z) #&gt; [1] 59 qdiff5(z, na.rm = FALSE) #&gt; Error in quantile.default(x, probs, na.rm = na.rm): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE 59.19 The useful but mysterious ... argument You probably could have lived a long and happy life without knowing there are at least 9 different algorithms for computing quantiles. Go read about the type argument of quantile(). TLDR: If a quantile is not unambiguously equal to an observed data point, you must somehow average two data points. You can weight this average different ways, depending on the rest of the data, and type = controls this. Let’s say we want to give the user of our function the ability to specify how the quantiles are computed, but we want to accomplish with as little fuss as possible. In fact, we don’t even want to clutter our function’s interface with this! This calls for the very special ... argument. In English, this set of three dots is frequently called an “ellipsis”. qdiff6 &lt;- function(x, probs = c(0, 1), na.rm = TRUE, ...) { the_quantiles &lt;- quantile(x = x, probs = probs, na.rm = na.rm, ...) max(the_quantiles) - min(the_quantiles) } The practical significance of the type = argument is virtually nonexistent, so we can’t demo with the Gapminder data. Thanks to @wrathematics, here’s a small example where we can (barely) detect a difference due to type. set.seed(1234) z &lt;- rnorm(10) quantile(z, type = 1) #&gt; 0% 25% 50% 75% 100% #&gt; -2.346 -0.890 -0.564 0.429 1.084 quantile(z, type = 4) #&gt; 0% 25% 50% 75% 100% #&gt; -2.346 -1.049 -0.564 0.353 1.084 all.equal(quantile(z, type = 1), quantile(z, type = 4)) #&gt; [1] &quot;Mean relative difference: 0.178&quot; Now we can call our function, requesting that quantiles be computed in different ways. qdiff6(z, probs = c(0.25, 0.75), type = 1) #&gt; [1] 1.32 qdiff6(z, probs = c(0.25, 0.75), type = 4) #&gt; [1] 1.4 While the difference may be subtle, it’s there. Marvel at the fact that we have passed type = 1 through to quantile() even though it was not a formal argument of our own function. The special argument ... is very useful when you want the ability to pass arbitrary arguments down to another function, but without constantly expanding the formal arguments to your function. This leaves you with a less cluttered function definition and gives you future flexibility to specify these arguments only when you need to. You will also encounter the ... argument in many built-in functions – read up on c() or list() – and now you have a better sense of what it means. It is not a breezy “and so on and so forth.” There are also downsides to ..., so use it with intention. In a package, you will have to work harder to create truly informative documentation for your user. Also, the quiet, absorbent properties of ... mean it can sometimes silently swallow other named arguments, when the user has a typo in the name. Depending on whether or how this fails, it can be a little tricky to find out what went wrong. The ellipsis package provides tools that help package developers use ... more safely. The in-progress tidyverse principles guide provides further guidance on the design of functions that take ... in Data, dots, details. 59.20 Use testthat for formal unit tests Until now, we’ve relied on informal tests of our evolving function. If you are going to use a function a lot, especially if it is part of a package, it is wise to use formal unit tests. The testthat package (cran; GitHub) provides excellent facilities for this, with a distinct emphasis on automated unit testing of entire packages. However, we can take it out for a test drive even with our one measly function. We will construct a test with test_that() and, within it, we put one or more expectations that check actual against expected results. You simply harden your informal, interactive tests into formal unit tests. Here are some examples of tests and indicative expectations. library(testthat) test_that(&quot;invalid args are detected&quot;, { expect_error(qdiff6(&quot;eggplants are purple&quot;)) expect_error(qdiff6(iris)) }) test_that(&quot;NA handling works&quot;, { expect_error(qdiff6(c(1:5, NA), na.rm = FALSE)) expect_equal(qdiff6(c(1:5, NA)), 4) }) No news is good news! Let’s see what test failure would look like. Let’s revert to a version of our function that does no NA handling, then test for proper NA handling. We can watch it fail. qdiff_no_NA &lt;- function(x, probs = c(0, 1)) { the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } test_that(&quot;NA handling works&quot;, { expect_that(qdiff_no_NA(c(1:5, NA)), equals(4)) }) Similar to the advice to use assertions in data analytical scripts, I recommend you use unit tests to monitor the behavior of functions you (or others) will use often. If your tests cover the function’s important behavior, then you can edit the internals freely. You’ll rest easy in the knowledge that, if you broke anything important, the tests will fail and alert you to the problem. A function that is important enough for unit tests probably also belongs in a package, where there are obvious mechanisms for running the tests as part of overall package checks. "],["functions-practicum.html", "60 Function-writing practicum 60.1 Overview 60.2 Load the Gapminder data 60.3 Get data to practice with 60.4 Get some code that works 60.5 Turn working code into a function 60.6 Test on other data and in a clean workspace 60.7 Are we there yet? 60.8 Resources", " 60 Function-writing practicum 60.1 Overview We recently learned how to write our own R functions (part 1, part 2, part 3). Now we use that knowledge to write another useful function, within the context of the Gapminder data: Input: a data.frame that contains (at least) a life expectancy variable lifeExp and a variable for year year Output: a vector of estimated intercept and slope, from a linear regression of lifeExp on year The ultimate goal is to apply this function to the Gapminder data for a specific country. We will eventually scale up to all countries using external machinery, e.g., the dplyr::group_by() + dplyr::do(). 60.2 Load the Gapminder data As usual, load gapminder. Load ggplot2 because we’ll make some plots and load dplyr too. library(gapminder) library(ggplot2) library(dplyr) 60.3 Get data to practice with I’ve extract the data for one country in order to develop some working code interactively. j_country &lt;- &quot;France&quot; # pick, but do not hard wire, an example (j_dat &lt;- gapminder %&gt;% filter(country == j_country)) #&gt; # A tibble: 12 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 France Europe 1952 67.4 42459667 7030. #&gt; 2 France Europe 1957 68.9 44310863 8663. #&gt; 3 France Europe 1962 70.5 47124000 10560. #&gt; 4 France Europe 1967 71.6 49569000 13000. #&gt; 5 France Europe 1972 72.4 51732000 16107. #&gt; 6 France Europe 1977 73.8 53165019 18293. #&gt; 7 France Europe 1982 74.9 54433565 20294. #&gt; 8 France Europe 1987 76.3 55630100 22066. #&gt; 9 France Europe 1992 77.5 57374179 24704. #&gt; 10 France Europe 1997 78.6 58623428 25890. #&gt; 11 France Europe 2002 79.6 59925035 28926. #&gt; 12 France Europe 2007 80.7 61083916 30470. Always always always plot the data. Yes, even now. p &lt;- ggplot(j_dat, aes(x = year, y = lifeExp)) p + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; 60.4 Get some code that works Fit the regression: j_fit &lt;- lm(lifeExp ~ year, j_dat) coef(j_fit) #&gt; (Intercept) year #&gt; -397.765 0.239 Whoa, check out that wild intercept! Apparently the life expectancy in France around year 0 A.D. was minus 400 years! Never forget to sanity check a model. In this case, a reparametrization is in order. I think it makes more sense for the intercept to correspond to life expectancy in 1952, the earliest date in our dataset. Estimate the intercept eye-ball-o-metrically from the plot and confirm that we’ve got something sane and interpretable now. j_fit &lt;- lm(lifeExp ~ I(year - 1952), j_dat) coef(j_fit) #&gt; (Intercept) I(year - 1952) #&gt; 67.790 0.239 60.4.1 Sidebar: regression stuff There are two things above that might prompt questions. First, how did I know to get the estimated coefficients from a fitted model via coef()? Years of experience. But how might a novice learn such things? Read the documentation for lm(), in this case. The “See also” section advises us about many functions that can operate on fitted linear model objects, including, but by no means limited to, coef(). Read the documentation on coef() too. Second, what am I doing here: lm(lifeExp ~ I(year - 1952))? I want the intercept to correspond to 1952 and an easy way to accomplish that is to create a new predictor on the fly: year minus 1952. The way I achieve that in the model formula, I(year - 1952), uses the I() function which “inhibits interpretation/conversion of objects”. By protecting the expression year - 1952, I ensure it is interpreted in the obvious arithmetical way. 60.5 Turn working code into a function Create the basic definition of a function and drop your working code inside. Add arguments and edit the inner code to match. Apply it to the practice data. Do you get the same result as before? le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) coef(the_fit) } le_lin_fit(j_dat) #&gt; (Intercept) I(year - offset) #&gt; 67.790 0.239 I had to decide how to handle the offset. Given that I will scale this code up to many countries, which, in theory, might have data for different dates, I chose to set a default of 1952. Strategies that compute the offset from data, either the main Gapminder dataset or the excerpt passed to this function, are also reasonable to consider. I loathe the names on this return value. This is not my first rodeo and I know that, downstream, these will contaminate variable names and factor levels and show up in public places like plots and tables. Fix names early! le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) setNames(coef(the_fit), c(&quot;intercept&quot;, &quot;slope&quot;)) } le_lin_fit(j_dat) #&gt; intercept slope #&gt; 67.790 0.239 Much better! 60.6 Test on other data and in a clean workspace It’s always good to rotate through examples during development. The most common error this will help you catch is when you accidentally hard-wire your example into your function. If you’re paying attention to your informal tests, you will find it creepy that your function returns exactly the same results regardless which input data you give it. This actually happened to me while I was writing this document, I kid you not! I had left j_fit inside the call to coef(), instead of switching it to the_fit. How did I catch that error? I saw the fitted line below, which clearly did not have an intercept in the late 60s and a positive slope, as my first example did. Figures are a mighty weapon in the fight against nonsense. I don’t trust analyses that have few/no figures. j_country &lt;- &quot;Zimbabwe&quot; (j_dat &lt;- gapminder %&gt;% filter(country == j_country)) #&gt; # A tibble: 12 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zimbabwe Africa 1952 48.5 3080907 407. #&gt; 2 Zimbabwe Africa 1957 50.5 3646340 519. #&gt; 3 Zimbabwe Africa 1962 52.4 4277736 527. #&gt; 4 Zimbabwe Africa 1967 54.0 4995432 570. #&gt; 5 Zimbabwe Africa 1972 55.6 5861135 799. #&gt; 6 Zimbabwe Africa 1977 57.7 6642107 686. #&gt; 7 Zimbabwe Africa 1982 60.4 7636524 789. #&gt; 8 Zimbabwe Africa 1987 62.4 9216418 706. #&gt; 9 Zimbabwe Africa 1992 60.4 10704340 693. #&gt; 10 Zimbabwe Africa 1997 46.8 11404948 792. #&gt; 11 Zimbabwe Africa 2002 40.0 11926563 672. #&gt; 12 Zimbabwe Africa 2007 43.5 12311143 470. p &lt;- ggplot(j_dat, aes(x = year, y = lifeExp)) p + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; le_lin_fit(j_dat) #&gt; intercept slope #&gt; 55.221 -0.093 The linear fit is comically bad, but yes I believe the visual line and the regression results match up. It’s also a good idea to clean out the workspace, rerun the minimum amount of code, and retest your function. This will help you catch another common mistake: accidentally relying on objects that were lying around in the workspace during development but that are not actually defined in your function nor passed as formal arguments. rm(list = ls()) le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) setNames(coef(the_fit), c(&quot;intercept&quot;, &quot;slope&quot;)) } le_lin_fit(gapminder %&gt;% filter(country == &quot;Zimbabwe&quot;)) #&gt; intercept slope #&gt; 55.221 -0.093 60.7 Are we there yet? Yes. Given how I plan to use this function, I don’t feel the need to put it under formal unit tests or put in argument validity checks. 60.8 Resources Packages for runtime assertions (the last 3 seem to be under more active development than assertthat): assertthat on cran and GitHub - the Hadleyverse option ensurer on cran and GitHub - general purpose, pipe-friendly assertr on cran and GitHub - explicitly data pipeline oriented assertive on cran and Bitbucket - rich set of built-in functions Hadley Wickham’s book Advanced R (2015): Section on defensive programming Section on function arguments Section on return values Unit testing with testthat On cran, development on GitHub, main webpage Wickham and Bryan’s R Packages book (in progress) Testing chapter Wickham’s testthat: Get Started with Testing article in The R Journal (2011a). Maybe this is completely superseded by the newer chapter above? Be aware that parts could be out of date, but I recall it was a helpful read. "],["lab08.html", "61 Lab: University of Edinburgh Art Collection Getting started R scripts vs. R Markdown documents SelectorGadget Functions Iteration Analysis", " 61 Lab: University of Edinburgh Art Collection The University of Edinburgh Art Collection “supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.”3. Note: See the sidebar here and note that there are 3307 pieces in the art collection we’re collecting data on. In this lab, we’ll scrape data on all art pieces in the Edinburgh College of Art collection. The learning goals of this lab are: Web scraping from a single page Writing functions Iteration Writing data Before getting started, let’s check that a bot has permissions to access pages on this domain. paths_allowed(&quot;https://collections.ed.ac.uk/art)&quot;) ## collections.ed.ac.uk ## [1] TRUE Getting started Go to the course GitHub organization and locate your lab repo, which should be named lab-08-uoe-art. Grab the URL of the repo, and make a template for your own copy. R scripts vs. R Markdown documents Today we will be using both R scripts and R Markdown documents: .R: R scripts are plain text files containing only code and brief comments, We’ll use R scripts in the web scraping stage and ultimately save the scraped data as a csv. .Rmd: R Markdown documents are plain text files containing. We’ll use an R Markdown document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage. Here is the organization of your repo, and the corresponding section in the lab that each file will be used for: |-data | |- README.md |-lab-08-uoe-art.Rmd # analysis |-lab-08-uoe-art.Rproj # project management |-README.md |-scripts # webscraping | |- 01-scrape-page-one.R # scraping a single page | |- 02-scrape-page-function.R # functions | |- 03-scrape-page-many.R # iteration SelectorGadget For this lab, I recommend using Google Chrome as your web browser. In case you haven’t installed the SelectorGadget extension… go to the SelectorGadget extension page on the Chrome Web Store and click on “Add to Chrome” (big blue button). A pop up window will ask Add “SelectorGadget”?, click “Add extension”. Another pop up window will ask whether you want to get your extensions on all your computer. If you want this, you can turn on sync, but you don’t need to for the purpose of this lab. You should now be able to access SelectorGadget by clicking on the icon next to the search bar in the Chrome browser. Scraping a single page Tip: To run the code you can highlight or put your cursor next to the lines of code you want to run and hit Command+Enter. Work in scripts/01-scrape-page-one.R. We will start off by scraping data on the first 10 pieces in the collection from here. First, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R. # set url first_url &lt;- &quot;https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0&quot; # read html page page &lt;- read_html(first_url) For the ten pieces on this page, we will extract title, artist, and link information, and put these three variables in a data frame. 61.0.1 Titles Let’s start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes: page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) ## {xml_nodeset (10)} ## [1] &lt;a href=&quot;./record/99974?highlight=*:*&quot;&gt;Untitled ... ## [2] &lt;a href=&quot;./record/20534?highlight=*:*&quot;&gt;Agriculture ... ## [3] &lt;a href=&quot;./record/21465?highlight=*:*&quot;&gt;Rooftops ... ## [4] &lt;a href=&quot;./record/22040?highlight=*:*&quot;&gt;William Shakespeare Otelo ... ## [5] &lt;a href=&quot;./record/112501?highlight=*:*&quot;&gt;Untitled ... ## [6] &lt;a href=&quot;./record/21392?highlight=*:*&quot;&gt;Portrait of a Man in Profile ... ## [7] &lt;a href=&quot;./record/122080?highlight=*:*&quot;&gt;Lid ... ## [8] &lt;a href=&quot;./record/21451?highlight=*:*&quot;&gt;Architectural Scene ... ## [9] &lt;a href=&quot;./record/20846?highlight=*:*&quot;&gt;Head of a Woman Wearing Beads ... ## [10] &lt;a href=&quot;./record/99468?highlight=*:*&quot;&gt;The Misadventure ... Then we extract the text with html_text(): page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() ## [1] &quot;Untitled (2007)&quot; ## [2] &quot;Agriculture &quot; ## [3] &quot;Rooftops &quot; ## [4] &quot;William Shakespeare Otelo (1983)&quot; ## [5] &quot;Untitled &quot; ## [6] &quot;Portrait of a Man in Profile (1951)&quot; ## [7] &quot;Lid &quot; ## [8] &quot;Architectural Scene (1989)&quot; ## [9] &quot;Head of a Woman Wearing Beads (1952)&quot; ## [10] &quot;The Misadventure (1989)&quot; And get rid of all the spurious whitespace in the text with str_squish(): Take a look at the help docs for `str_squish()` (with `?str_squish`) to page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() ## [1] &quot;Untitled (2007)&quot; ## [2] &quot;Agriculture&quot; ## [3] &quot;Rooftops&quot; ## [4] &quot;William Shakespeare Otelo (1983)&quot; ## [5] &quot;Untitled&quot; ## [6] &quot;Portrait of a Man in Profile (1951)&quot; ## [7] &quot;Lid&quot; ## [8] &quot;Architectural Scene (1989)&quot; ## [9] &quot;Head of a Woman Wearing Beads (1952)&quot; ## [10] &quot;The Misadventure (1989)&quot; And finally save the resulting data as a vector of length 10: titles &lt;- page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() Links The same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes. A mini HTML lesson! The following is how we define hyperlinked text in HTML: &lt;a href=&quot;https://www.google.com&quot;&gt;Seach on Google&lt;/a&gt; And this is how the text would look like on a webpage: Seach on Google. Here the text is Seach on Google and the href attribute contains the url of the website you’d go to if you click on the hyperlinked text: https://www.google.com. The moral of the story is: the link is stored in the href attribute. page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% # same nodes html_node(&quot;h3 a&quot;) %&gt;% # as before html_attr(&quot;href&quot;) # but get href attribute instead of text ## [1] &quot;./record/99974?highlight=*:*&quot; &quot;./record/20534?highlight=*:*&quot; ## [3] &quot;./record/21465?highlight=*:*&quot; &quot;./record/22040?highlight=*:*&quot; ## [5] &quot;./record/112501?highlight=*:*&quot; &quot;./record/21392?highlight=*:*&quot; ## [7] &quot;./record/122080?highlight=*:*&quot; &quot;./record/21451?highlight=*:*&quot; ## [9] &quot;./record/20846?highlight=*:*&quot; &quot;./record/99468?highlight=*:*&quot; These don’t really look like urls as we know them though. They’re relative links. Note: See the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments. Click on one of art piece titles in your browser and take note of the URL of the webpage it takes you to. How does that URL compare to what we scraped above? How is it different? Using str_replace(), fix the URLs. Artists Fill in the blanks to scrape artist names. Put it altogether Fill in the blanks to organize everything in a tibble. Scrape the next page Click on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten. Functions Work in scripts/02-scrape-page-function.R. You’ve been using R functions, now it’s time to write your own! Let’s start simple. Here is a function that takes in an argument x, and adds 2 to it. add_two &lt;- function(x) { x + 2 } Let’s test it: add_two(3) ## [1] 5 add_two(10) ## [1] 12 The skeleton for defining functions in R is as follows: function_name &lt;- function(input) { # do something with the input(s) # return something } Then, a function for scraping a page should look something like: Tip: Function names should be short but evocative verbs. function_name &lt;- function(url) { # read page at url # extract title, link, artist info for n pieces on page # return a n x 3 tibble } Fill in the blanks using code you already developed in the previous exercises. Name the function scrape_page. Test out your new function by running the following in the console. Does the output look right? scrape_page(first_url) scrape_page(second_url) Iteration Work in scripts/03-scrape-page-many.R. We went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection. Reminder: The collection has 3307 pieces in total, as of the last time this page was compiled. That means we develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3307 rows and 3 columns. List of URLs Click through the first few of the pages in the art collection and observe their URLs to confirm the following pattern: [sometext]offset=0 # Pieces 1-10 [sometext]offset=10 # Pieces 11-20 [sometext]offset=20 # Pieces 21-30 [sometext]offset=30 # Pieces 31-40 ... [sometext]offset=2900 # Pieces 2900-2909 We can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 2900. Two new functions are helpful for accomplishing this: paste0() for pasting two pieces of text and seq() for generating a sequence of numbers. Fill in the blanks to construct the list of URLs. Mapping Finally, we’re ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R (more details on that soon!), and they each take the following form: map([x], [function to apply to each element of x]) In our case, x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. Now, we want a data frame, so we use map_dfr function: map_dfr(urls, scrape_page) Fill in the blanks to scrape all pages, and to create a new data frame called uoe_art. Write out data Finally write out the data frame you constructed into the data folder so that you can use it in the analysis section. Analysis Work in lab-08-uoe-art.Rmd for the rest of the lab. Now that we have a tidy dataset that we can analyze, let’s do that! We’ll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up! First thing we’ll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to “separate the title column at the first occurence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date” Luckily, there’s a function that does just this: separate()! Once we have completed separating the single title column into title and date, we need to do further cleanup in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable. Hint: Remember escaping special characters from that video? Which video… oh you know the one. You’ll need to use that trick again. Fill in the blanks to implement the data wrangling we described above. Note that this approach will result in some warnings when you run the code, and that’s OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it’s convenient to do so. Print out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing? Make a histogram of years. Use a reasonable bin width. Do you see anything out of the ordinary? Find which piece has the out-of-the-ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn’t capture the correct year information? Correct the error in the data frame and visualize the data again. Hint: You’ll want to use mutate() and if_else() or case_when() to implement the correction. Who is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them? Final question! How many art pieces have the word “child” in their title? See if you can figure it out, and ask for help if not. Hint: yOU can use a combination of filter() and str_detect(). You will want to read the help for str_detect() at a minimum, and consider how you might capture titles where the word appears as “child” and “Child”. Source: https://collections.ed.ac.uk/art/about↩︎ "],["welcome-to-data-and-ethics.html", "62 Welcome to Data and Ethics 62.1 Module Materials", " 62 Welcome to Data and Ethics This module is a bit different than the typical module. We’ll be introducing ideas related to privacy and ethics in the context of data science. There are fewer videos this week. Instead, this week is dedicated to your mid-semester check-in with Mason. Please take the time to watch the video and review the accompanying notes. The video playlist for this module can be accessed here. The slides used in the videos are available in the slides repo. 62.1 Module Materials Slides from Lectures Ethics Algorithmic bias Readings How to make a racist AI without really trying How to write a racist AI in R without really trying Check out the annotated bibliography. Activities Annotated Bibliography Lab Lab Me hearing my students make nuanced and important points during a class discussion of Data Science Ethics: pic.twitter.com/7TUupLb9MH&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) March 31, 2021 "],["data-science-and-ethics.html", "63 Data Science and Ethics 63.1 Module Commentary 63.2 Misrepresenting Data 63.3 Maps", " 63 Data Science and Ethics 63.1 Module Commentary 63.2 Misrepresenting Data 63.3 Maps "],["bias.html", "64 Bias 64.1 Curated Videography 64.2 Annotated Bibliography Instructions", " 64 Bias You can follow along with the slides here if they do not appear below. 64.1 Curated Videography Random commenter: Why does CS need an ethics focus?Me: Because technology hurts people and maybe if doesn’t have to. Them: Technology is just a tool. Me: That. That is why CS people need more ethics focused training. I swear this is 100% real.&mdash; Dr. Casey Fiesler is no longer on here (@cfiesler) January 24, 2022 64.1.1 Data Science Ethics in 6 Minutes 64.1.2 AI for Good in the R and Python ecosystems 64.1.3 Are We Automating Racism? 64.1.4 Big Tech’s B.S. about AI ethics After spending a few years cutting through Big Tech&#39;s B.S. about AI ethics, I&#39;ve created a glossary to help you decode what all of their favorite terms actually mean.I had way too much fun working on this! Threading some of my favorites below. https://t.co/h0TGqoFpHA&mdash; Karen Hao (@_KarenHao) April 13, 2021 Wow, who could have predicted that Google&#39;s mistreatment of @timnitGebru and @mmitchell_ai would have consequences?Google still has taken no responsibility for what it did. https://t.co/FGKc5NSiKR&mdash; Jonathan Aldrich (@JAldrichPL) October 22, 2021 64.1.5 More Bias Colorization APIs are becoming widespread; AI-colorized historical photos are circulated without caveat. But is AI colorization providing an accurate image of the past? To find out, I digitally desaturated these color photos by Sergey Prokudin-Gorsky, taken between 1909 and 1915. pic.twitter.com/YzJO2b0aza&mdash; Gwen C. Katz (@gwenckatz) April 12, 2021 64.2 Annotated Bibliography Instructions An annotated bibliography is a list of citations but with commentary! Ok, more like just an enhanced list where you summarize the source and explain why it is important to include. Your mission is to add either a citation or an annotation (or both) to this list of Data Science and Ethics Readings. Carole Cadwalladr and Emma Graham-Harrison. How Cambridge Analytica turned Facebook ‘likes’ into a lucrative political tool. The Guardian. 17 March 2018. Chen Wenhong and Anabel Quan-Haase. Big Data Ethics and Politics: Toward New Understandings. Social Science Computer Review. 14 November 2018. Nitasha Tiku. [Google hired Timnit Gebru to be an outspoken critic of unethical AI. Then she was fired for it.]. (https://www.washingtonpost.com/technology/2020/12/23/google-timnit-gebru-ai-ethics/). The Washington Post. 23 December 2020. Dan Swinhoe. The biggest data breach fines, penalties, and settlements so far. CSO. 5 March 2021. Sara Morrison.Why you should care about data privacy even if you have “nothing to hide”. Vox. Jan 28 2021. Richard Van Noorden. The ethical questions that haunt facial-recognition research. Nature. 18 November 2020. Karen Hao. Big Tech’s guide to talking about AI ethics. MIT Technology Review. April 13 2021 Catherine D’Ignazio and Lauren F. Klein. Data feminism. Mit Press, 2020. Inclusive Communication Principles in Health Equity Guiding Principles for Inclusive Communication. CDC, 2022 "],["society-and-ai.html", "65 Society and AI 65.1 Curated Videography", " 65 Society and AI Artificial intelligence (AI) is one of the most transformative technologies of our time, with the potential to revolutionize the way we live, work, and interact with each other. I’ll eventually have a lecture where I’ll explore the ethical and social implications that arise as a result. I will examine how AI is enhancing productivity, improving healthcare, and providing personalized education, while also exploring its potential to displace jobs, perpetuate bias and discrimination, and threaten privacy and security. Furthermore, I will delve into the ethical concerns that arise with the development and deployment of AI, such as transparency, accountability, fairness, and human values. Finally, I will consider the future of AI and its potential outcomes, as well as the role of society in shaping its development and deployment. This lecture aims to provide an in-depth understanding of the complex relationship between AI and society and to encourage responsible and ethical practices in its development and deployment. In the meantime, this curated videography contains a broad overview of the current landscape… 65.1 Curated Videography 65.1.1 Last Week Tonight with John Oliver 65.1.1.1 AI Images 65.1.1.2 Artificial Intelligence "],["lab09.html", "66 Lab: Modeling professor attractiveness and course evaluations Getting started Warm up The data Exercises", " 66 Lab: Modeling professor attractiveness and course evaluations Getting started Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings. (Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. http://www.sciencedirect.com/science/article/pii/S0272775704001165.) For this assignment, you will analyze the data from this study in order to learn what goes into a positive professor evaluation. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. Packages In this lab, we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) Housekeeping Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. Project name Update the name of your project to match the lab’s title. Warm up Before we introduce the data, let’s warm up with some simple exercises. YAML Open the R Markdown (Rmd) file in your project and knit the document. Committing and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update name” in the Commit message box and hit Commit. Click on Push. This will prompt a dialog box where you first need to enter your user name, and then your password. The data The dataset we’ll be using is called evals from the openintro package. Take a peek at the codebook with ?evals. Exercises Part 1: Exploratory Data Analysis Visualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response. Visualize and describe the relationship between score and the variable bty_avg, a professor’s average beauty rating. Hint: See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html. Replot the scatterplot from Exercise 3, but this time use geom_jitter()? What does “jitter” mean? What was misleading about the initial scatterplot? Part 2: Linear regression with a numerical predictor Recall: Linear model is in the form \\(\\hat{y} = b_0 + b_1 x\\). Let’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model. Replot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line. Interpret the slope of the linear model in context of the data. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context. Determine the \\(R^2\\) of the model and interpret it in context of the data. Part 3: Linear regression with a categorical predictor Fit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data. What is the equation of the line corresponding to male professors? What is it for female professors? Fit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Create a new variable called rank_relevel where \"tenure track\" is the baseline level. Fit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. Create another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\". Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. "],["welcome-to-modeling-the-tidy-way.html", "67 Welcome to modeling the tidy way! 67.1 Module Materials", " 67 Welcome to modeling the tidy way! This module introduces you to the ideas behind fitting and interpreting models within the framework of tidymodels. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. In the spirit of transparency, I have strong [non-positive][tidymodelthoughts] thoughts on tidymodels, but I also think that it is useful for you to learn them. I have similar feelings about the necessary evil that is ANOVA or SPSS. 67.1 Module Materials Videos Located in the subchapters of this module Slidedecks Language of Models Fitting and interpreting models Modeling non-linear relationships Models with multiple predictors Suggested Readings All subchapters of this module Articles Rodgers, J. L. (2010). The epistemology of mathematical and statistical modeling: A quiet methodological revolution. American Psychologist, 65, 1-12. R4DS Model Section, including Model Basics Model Building Lab Lab 10 "],["language-of-models.html", "68 Language of Models 68.1 What is a model? 68.2 Modeling the relationship between variables", " 68 Language of Models You can follow along with the slides here if they do not appear below. 68.1 What is a model? 68.2 Modeling the relationship between variables "],["fitting-and-interpreting-models.html", "69 Fitting and interpreting models 69.1 Models with numerical explanatory variables 69.2 A More Technical Worked Example 69.3 Models with categorical explanatory variables", " 69 Fitting and interpreting models You can follow along with the slides here if they do not appear below. 69.1 Models with numerical explanatory variables 69.2 A More Technical Worked Example Let’s load our standard libraries: library(lattice) library(ggplot2) library(tidyverse) If you’ve taken a regression course, you might recognize this model as a special case of a linear model. If you haven’t, well, it doesn’t really matter much except… we can use the lm() function to fit the model. The advantage is that lm() easily splits the data into fitted values and residuals: Observed value = Fitted value + residual Let’s get the fitted values and residuals for each voice part: lm_singer &lt;- lm(height ~ 0 + voice.part, data = singer ) We can extract the fitted values using fitted.values(lm_singer) and the residuals with residuals(lm_singer) or lm_singer$residuals. For convenience, we create a data frame with two columns: the voice parts and the residuals. res_singer &lt;- data.frame( voice_part = singer$voice.part, residual = residuals(lm_singer) ) We can also do this with group_by and mutate: fits &lt;- singer %&gt;% group_by(voice.part) %&gt;% mutate( fit = mean(height), residual = height - mean(height) ) 69.2.1 Does the linear model fit? To assess whether the linear model is a good fit to the data, we need to know whether the errors look like they come from normal distributions with the same variance. The residuals are our estimates of the errors, and so we need to check both homoscedasticity and normality. 69.2.2 Homoscedasticity Homoscedasticity, or homogeneity of variances, is a modeling assumption that we make that assummes we have equal or similar variances in different groups being compared. In other words, the variance of the residuals should be the same for all values of the predictor. We can check this by looking at the residuals for each voice part. There are a few ways we can look at the residuals. Side-by-side boxplots give a broad overview: ggplot(res_singer, aes(x = voice_part, y = residual)) + geom_boxplot() We can also look at the ecdfs of the residuals for each voice part. ggplot(res_singer, aes(x = residual, color = voice_part)) + stat_ecdf() From these plots, it seems like the residuals in each group have approximately the same variance. 69.2.3 Normality Normality of the residuals is another important assumption of the linear model. We can check this assumption by looking at normal QQ plots of the residuals for each voice part. We can do this by faceting, which will allow us to examine each voice part separately. We can use the stat_qq() function to create the QQ plots.: ggplot(res_singer, aes(sample = residual)) + stat_qq() + facet_wrap(~voice_part, ncol = 4) Not only do the lines look reasonably straight, the scales look similar for all eight voice parts. This similarity suggests a model where all of the errors are normal with the same standard deviation. This is convenient because it is the form of a standard linear model: Singer height = Average height for their voice part + Normal(\\(0, \\sigma^2\\)) error. 69.2.4 Normality of pooled residuals If this assumption for our linear model holds, then all the residuals come from the same normal distribution. We’ve already checked for normality of the residuals within each voice part, but to get a little more power to see divergence from normality, we can pool the residuals and make a normal QQ plot of all the residuals together. ggplot(res_singer, aes(sample = residual)) + stat_qq() It’s easier to check normality if we plot the line that the points should fall on: if we think the points come from a \\(N(\\mu, \\sigma^2)\\) distribution, they should lie on a line with intercept \\(\\mu\\) and slope \\(\\sigma\\) (the standard deviation). In the linear model, we assume that the mean of the error terms is zero. We don’t know what their variance should be, but we can estimate it using the variance of the residuals. Therefore, we add a line with the mean of the residuals (which should be zero) as the intercept, and the SD of the residuals as the slope. This is: ggplot(res_singer, aes(sample = residual)) + stat_qq() + geom_abline(aes( intercept = 0, slope = sd(res_singer$residual) )) #&gt; Warning: Use of `res_singer$residual` is discouraged. #&gt; ℹ Use `residual` instead. 69.2.5 The actually correct way Pedantic note: We should use an \\(n-8\\) denominator instead of \\(n-1\\) in the SD calculation for degrees of freedom reasons. The \\(n-8\\) part is necessary because there are 7 different variables associated with the model we fitted with lm_singer. We can get the SD directly from the linear model: sd(res_singer$residual) #&gt; [1] 2.47 round(summary(lm_singer)$sigma, 3) #&gt; [1] 2.5 However, the difference between this adjustment and the SD above is negligible. Add the line: ggplot(res_singer, aes(sample = residual)) + stat_qq() + geom_abline(intercept = mean(res_singer$residual), slope = summary(lm_singer)$sigma) The straight line isn’t absolutely perfect, but it’s doing a pretty good job. The residuals look like they come from a normal distribution with the same variance for each voice part. 69.2.6 Our final model Since the errors seem to be pretty normal, our final model is: Singer height = Average height for their voice part + Normal(\\(0, 2.5^2\\)) error. Note: Although normality (or lack thereof) can be important for probabilistic prediction or (sometimes) for inferential data analysis, it’s relatively unimportant for EDA. If your residuals are about normal that’s nice, but as long as they’re not horribly skewed they’re probably not a problem. 69.2.7 What have we learned? About singers: We’ve seen that average height increases as the voice part range decreases. Within each voice part, the residuals look like they come from a normal distribution with the same variance for each voice part. This suggests that there’s nothing further we need to do to explain singer heights: we have an average for each voice part, and there is no suggestion of systematic differences beyond that due to voice part. About data analysis: We can use some of our univariate visualization tools, particularly boxplots and ecdfs, to look for evidence of heteroscedasticity. We can use normal QQ plots on both pooled and un-pooled residuals to look for evidence of non-normality. If we wanted to do formal tests or parameter estimation for singer heights, we would feel pretty secure using results based on normal theory. 69.2.8 Commentary on Model Performance Somebody’s got some zero inflated count data he’s modeling as normal …&mdash; Brenton Wiernik ️‍ (@bmwiernik) April 20, 2022 library(performance) #&gt; Warning: package &#39;performance&#39; was built under R version 4.4.2 check_model(lm_singer) # model_performance(lm_singer) # QQ-plot plot(check_normality(lm_singer), type = &quot;qq&quot;) #&gt; For confidence bands, please install `qqplotr`. # PP-plot plot(check_normality(lm_singer), type = &quot;pp&quot;) #&gt; For confidence bands, please install `qqplotr`. 69.3 Models with categorical explanatory variables "],["modeling-non-linear-relationships.html", "70 Modeling non-linear relationships", " 70 Modeling non-linear relationships You can follow along with the slides here if they do not appear below. "],["modeling-with-multiple-predictors.html", "71 Modeling with multiple predictors 71.1 The linear model with multiple predictors 71.2 Two numerical predictors 71.3 My Thoughts on Tidy Modeling {tidymodelthoughts}", " 71 Modeling with multiple predictors You can follow along with the slides here if they do not appear below. 71.1 The linear model with multiple predictors 71.2 Two numerical predictors 71.3 My Thoughts on Tidy Modeling {tidymodelthoughts} That&#39;s a good distinction -- I think tidyverse is good for data cleaning and onboarding undergrads (my lab is very undergrad heavy); but that tidymodels can die in a fire because they don&#39;t report f-tests!!!!&mdash; Prof. Mason Garrison ✨ (@SMasonGarrison) April 28, 2021 Underfitting vs Overfitting in Machine Learning.#datascience #machinelearning #coding #programming #python #data #deeplearning #technology #dataanalytics #datascientist #ai #computerscience #tech #developer #analytics #iot #coder #programmerlife #100DaysOfCode #DeepLearning pic.twitter.com/QRZqeHMsCg&mdash; cat (@kitty_attac) December 1, 2021 "],["notes-on-logistic-regression.html", "72 Notes on Logistic Regression 72.1 Predicting categorical data 72.2 Sensitivity and specificity", " 72 Notes on Logistic Regression 72.1 Predicting categorical data 72.1.0.1 Spam filters Data from 3921 emails and 21 variables on them Outcome: whether the email is spam or not Predictors: number of characters, whether the email had “Re:” in the subject, time at which email was sent, number of times the word “inherit” shows up in the email, etc. library(openintro) data(email) glimpse(email) #&gt; Rows: 3,921 #&gt; Columns: 21 #&gt; $ spam &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ to_multiple &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ from &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, … #&gt; $ sent_email &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, … #&gt; $ time &lt;dttm&gt; 2012-01-01 01:16:41, 2012-01-01 02:03:59, 2012-01-01 11:… #&gt; $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ dollar &lt;dbl&gt; 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, … #&gt; $ winner &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n… #&gt; $ inherit &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ password &lt;dbl&gt; 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, … #&gt; $ num_char &lt;dbl&gt; 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421… #&gt; $ line_breaks &lt;int&gt; 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191… #&gt; $ format &lt;fct&gt; 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, … #&gt; $ re_subj &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, … #&gt; $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, … #&gt; $ urgent_subj &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ exclaim_mess &lt;dbl&gt; 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0… #&gt; $ number &lt;fct&gt; big, small, small, small, none, none, big, small, small, … Question: Would you expect longer or shorter emails to be spam? #&gt; Picking joint bandwidth of 1.18 #&gt; # A tibble: 2 × 2 #&gt; spam mean_num_char #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 0 11.3 #&gt; 2 1 5.44 Question: Would you expect emails that have subjects starting with “Re:”, “RE:”, “re:”, or “rE:” to be spam or not? 72.1.0.2 Modeling spam Both number of characters and whether the message has “re:” in the subject might be related to whether the email is spam. How do we come up with a model that will let us explore this relationship? For simplicity, we’ll focus on the number of characters (num_char) as predictor, but the model we describe can be expanded to take multiple predictors as well. We can’t reasonably fit a linear model to this data– we need something different! 72.1.0.3 Framing the problem We can treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials Bernoulli trial: a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted Each Bernoulli trial can have a separate probability of success \\[ y_i ∼ Bern(p) \\] We can then use the predictor variables to model that probability of success, \\(p_i\\) We can’t just use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range 72.1.0.4 Generalized linear models This is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs) Logistic regression is just one example 72.1.0.5 Three characteristics of GLMs All GLMs have the following three characteristics: A probability distribution describing a generative model for the outcome variable A linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\] A link function that relates the linear model to the parameter of the outcome distribution 72.1.0.6 Logistic regression Logistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors To finish specifying the Logistic model we just need to define a reasonable link function that connects \\(\\eta_i\\) to \\(p_i\\): logit function Logit function: For \\(0\\le p \\le 1\\) \\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\] 72.1.0.7 Logit function, visualized d &lt;- tibble(p = seq(0.001, 0.999, length.out = 1000 )) %&gt;% mutate(logit_p = log(p / (1 - p))) ggplot(d, aes(x = p, y = logit_p)) + geom_line() + xlim(0, 1) + ylab(&quot;logit(p)&quot;) + labs(title = &quot;logit(p) vs. p&quot;) + theme_bw() 72.1.0.8 Properties of the logit The logit function takes a value between 0 and 1 and maps it to a value between \\(-\\infty\\) and \\(\\infty\\) Inverse logit (logistic) function: \\[g^{-1}(x) = \\frac{\\exp(x)}{1+\\exp(x)} = \\frac{1}{1+\\exp(-x)}\\] The inverse logit function takes a value between \\(-\\infty\\) and \\(\\infty\\) and maps it to a value between 0 and 1 This formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success – more on this later 72.1.0.9 The logistic regression model Based on the three GLM criteria, we have \\(y_i \\sim \\text{Bern}(p_i)\\) \\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\) \\(\\text{logit}(p_i) = \\eta_i\\) From which we get \\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\] 72.1.0.10 Modeling spam In R, we fit a GLM in the same way as a linear model except we specify the model with logistic_reg() use \"glm\" instead of \"lm\" as the engine define family = \"binomial\" for the link function to be used in the model spam_fit &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% fit(spam ~ num_char, data = email, family = &quot;binomial&quot; ) # tidy(spam_fit) 72.1.0.11 Spam model tidy(spam_fit) #&gt; # A tibble: 2 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -1.80 0.0716 -25.1 2.04e-139 #&gt; 2 num_char -0.0621 0.00801 -7.75 9.50e- 15 Model: \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times \\text{num_char}\\] 72.1.0.12 P(spam) for an email with 2000 characters \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times 2\\] \\[\\frac{p}{1-p} = \\exp(-1.9242) = 0.15 \\rightarrow p = 0.15 \\times (1 - p)\\] \\[p = 0.15 - 0.15p \\rightarrow 1.15p = 0.15\\] \\[p = 0.15 / 1.15 = 0.13\\] Question: What is the probability that an email with 15000 characters is spam? What about an email with 40000 characters? 2K chars: P(spam) = 0.13 15K chars, P(spam) = 0.06 40K chars, P(spam) = 0.01 Question: Would you prefer an email with 2000 characters to be labeled as spam or not? How about 40,000 characters? 72.2 Sensitivity and specificity 72.2.0.1 False positive and negative Email is spam Email is not spam Email labeled spam True positive False positive (Type 1 error) Email labeled not spam False negative (Type 2 error) True negative False negative rate = P(Labeled not spam | Email spam) = FN / (TP + FN) False positive rate = P(Labeled spam | Email not spam) = FP / (FP + TN) 72.2.0.2 Sensitivity and specificity Email is spam Email is not spam Email labeled spam True positive False positive (Type 1 error) Email labeled not spam False negative (Type 2 error) True negative Sensitivity = P(Labeled spam | Email spam) = TP / (TP + FN) Sensitivity = 1 − False negative rate Specificity = P(Labeled not spam | Email not spam) = TN / (FP + TN) Specificity = 1 − False positive rate Question: If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision? "],["lab10.html", "73 Lab: Modeling with multiple predictors Professor attractiveness and course evaluations, Pt. 2 Getting started Warm up The data Exercises", " 73 Lab: Modeling with multiple predictors Professor attractiveness and course evaluations, Pt. 2 In this lab, we revisit the professor evaluations data we modeled in the previous lab. In the last lab, we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores. If you don’t remember the data, review the previous lab’s introduction before continuing to the exercises. Getting started Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) Warm up Before we introduce the data, let’s warm up with some simple exercises. YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update team name” in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. The data The dataset we’ll be using is called evals from the openintro package. Take a peek at the codebook with ?evals. Exercises Part 1: Simple linear regression Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). Part 2: Multiple linear regression Fit a linear model (one you have fit before): m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). Interpret the slope and intercept of m_bty_gen in context of the data. What percent of the variability in score is explained by the model m_bty_gen. What is the equation of the line corresponding to just male professors? For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score? How does the relationship between beauty and evaluation score vary between male and female professors? How do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beauty score of the professor. Compare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg? Create a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data. Part 3: The search for the best model Going forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg. Which variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professor’s score. Check your suspicions from the previous exercise. Include the model output for that variable in your response. Suppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why? Fit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question. Using backward-selection with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on. Interpret the slopes of one numerical and one categorical predictor based on your final model. Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score. Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not? "],["welcome-to-overfitting-and-cross-validation.html", "74 Welcome to Overfitting and Cross-Validation 74.1 Module Materials", " 74 Welcome to Overfitting and Cross-Validation This module is designed to introduce you to cross-validation and overfitting. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 74.1 Module Materials Videos and Slides from Lectures Overfitting Cross-validation Suggested Readings All subchapters of this module Articles de Rooij, M., &amp; Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248-263. MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40. R4DS Many Models "],["lecture-overfitting.html", "75 Lecture: Overfitting 75.1 Prediction 75.2 Workflow", " 75 Lecture: Overfitting You can follow along with the slides here if they do not appear below. The embedded code for feature enginering can be found here 75.1 Prediction 75.2 Workflow "],["lecture-cross-validation.html", "76 Lecture: Cross-Validation 76.1 V-Fold", " 76 Lecture: Cross-Validation You can follow along with the slides here if they do not appear below. 76.1 V-Fold "],["featurenotes.html", "77 Notes on Feature Engineering 77.1 Feature engineering 77.2 Modeling workflow, revisited 77.3 Building recipes 77.4 Building workflows 77.5 Making decisions", " 77 Notes on Feature Engineering 77.1 Feature engineering We prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity Variables that go into the model and how they are represented are just as critical to success of the model Feature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance) 77.1.1 Same training and testing sets as before # Fix random numbers by setting the seed # Enables analysis to be reproducible when random numbers are used set.seed(1066) # Put 80% of the data into the training set email_split &lt;- initial_split(email, prop = 0.80) # Create data frames for the two sets: train_data &lt;- training(email_split) test_data &lt;- testing(email_split) 77.1.2 A simple approach: mutate() train_data %&gt;% mutate( date = date(time), dow = wday(time), month = month(time) ) %&gt;% select(time, date, dow, month) %&gt;% sample_n(size = 5) # shuffle to show a variety #&gt; # A tibble: 5 × 4 #&gt; time date dow month #&gt; &lt;dttm&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2012-02-07 14:17:38 2012-02-07 3 2 #&gt; 2 2012-02-28 19:24:15 2012-02-28 3 2 #&gt; 3 2012-03-12 08:07:31 2012-03-12 2 3 #&gt; 4 2012-01-26 13:58:18 2012-01-26 5 1 #&gt; 5 2012-02-21 08:50:29 2012-02-21 3 2 77.2 Modeling workflow, revisited Create a recipe for feature engineering steps to be applied to the training data Fit the model to the training data after these steps have been applied Using the model estimates from the training data, predict outcomes for the test data Evaluate the performance of the model on the test data 77.3 Building recipes 77.3.1 Initiate a recipe email_rec &lt;- recipe( spam ~ ., # formula data = train_data # data to use for cataloguing names and types of variables ) summary(email_rec) #&gt; # A tibble: 21 × 4 #&gt; variable type role source #&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 to_multiple &lt;chr [3]&gt; predictor original #&gt; 2 from &lt;chr [3]&gt; predictor original #&gt; 3 cc &lt;chr [2]&gt; predictor original #&gt; 4 sent_email &lt;chr [3]&gt; predictor original #&gt; 5 time &lt;chr [1]&gt; predictor original #&gt; 6 image &lt;chr [2]&gt; predictor original #&gt; 7 attach &lt;chr [2]&gt; predictor original #&gt; 8 dollar &lt;chr [2]&gt; predictor original #&gt; 9 winner &lt;chr [3]&gt; predictor original #&gt; 10 inherit &lt;chr [2]&gt; predictor original #&gt; 11 viagra &lt;chr [2]&gt; predictor original #&gt; 12 password &lt;chr [2]&gt; predictor original #&gt; 13 num_char &lt;chr [2]&gt; predictor original #&gt; 14 line_breaks &lt;chr [2]&gt; predictor original #&gt; 15 format &lt;chr [3]&gt; predictor original #&gt; 16 re_subj &lt;chr [3]&gt; predictor original #&gt; 17 exclaim_subj &lt;chr [2]&gt; predictor original #&gt; 18 urgent_subj &lt;chr [3]&gt; predictor original #&gt; 19 exclaim_mess &lt;chr [2]&gt; predictor original #&gt; 20 number &lt;chr [3]&gt; predictor original #&gt; 21 spam &lt;chr [3]&gt; outcome original 77.3.2 Remove certain variables email_rec &lt;- email_rec %&gt;% step_rm(from, sent_email) #&gt; #&gt; ── Recipe ────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 20 #&gt; #&gt; ── Operations #&gt; • Variables removed: from and sent_email 77.3.3 Feature engineer date email_rec &lt;- email_rec %&gt;% step_date(time, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% step_rm(time) #&gt; #&gt; ── Recipe ────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 20 #&gt; #&gt; ── Operations #&gt; • Variables removed: from and sent_email #&gt; • Date features from: time #&gt; • Variables removed: time 77.3.4 Discretize numeric variables Proceed with major caution! And please be sure to read MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40. and play around with the demo data from Kris’s website: http://www.quantpsy.org/mzpr.htm email_rec &lt;- email_rec %&gt;% step_cut(cc, attach, dollar, breaks = c(0, 1)) %&gt;% step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) #&gt; #&gt; ── Recipe ────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 20 #&gt; #&gt; ── Operations #&gt; • Variables removed: from and sent_email #&gt; • Date features from: time #&gt; • Variables removed: time #&gt; • Cut numeric for: cc, attach, dollar #&gt; • Cut numeric for: inherit and password 77.3.5 Create dummy variables email_rec &lt;- email_rec %&gt;% step_dummy(all_nominal(), -all_outcomes()) #&gt; #&gt; ── Recipe ────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 20 #&gt; #&gt; ── Operations #&gt; • Variables removed: from and sent_email #&gt; • Date features from: time #&gt; • Variables removed: time #&gt; • Cut numeric for: cc, attach, dollar #&gt; • Cut numeric for: inherit and password #&gt; • Dummy variables from: all_nominal() and -all_outcomes() 77.3.6 Remove zero variance variables Variables that contain only a single value email_rec &lt;- email_rec %&gt;% step_zv(all_predictors()) #&gt; #&gt; ── Recipe ────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 20 #&gt; #&gt; ── Operations #&gt; • Variables removed: from and sent_email #&gt; • Date features from: time #&gt; • Variables removed: time #&gt; • Cut numeric for: cc, attach, dollar #&gt; • Cut numeric for: inherit and password #&gt; • Dummy variables from: all_nominal() and -all_outcomes() #&gt; • Zero variance filter on: all_predictors() 77.3.7 All in one place email_rec &lt;- recipe(spam ~ ., data = email) %&gt;% step_rm(from, sent_email) %&gt;% step_date(time, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% step_rm(time) %&gt;% step_cut(cc, attach, dollar, breaks = c(0, 1)) %&gt;% step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) 77.4 Building workflows 77.4.1 Define model email_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) email_mod #&gt; Logistic Regression Model Specification (classification) #&gt; #&gt; Computational engine: glm 77.4.2 Define workflow Workflows bring together models and recipes so that they can be easily applied to both the training and test data. email_wflow &lt;- workflow() %&gt;% add_model(email_mod) %&gt;% add_recipe(email_rec) #&gt; ══ Workflow ════════════════════════════════════════════════════════════════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: logistic_reg() #&gt; #&gt; ── Preprocessor ──────────────────────────────────────────────────────────────────────────────────── #&gt; 7 Recipe Steps #&gt; #&gt; • step_rm() #&gt; • step_date() #&gt; • step_rm() #&gt; • step_cut() #&gt; • step_cut() #&gt; • step_dummy() #&gt; • step_zv() #&gt; #&gt; ── Model ─────────────────────────────────────────────────────────────────────────────────────────── #&gt; Logistic Regression Model Specification (classification) #&gt; #&gt; Computational engine: glm 77.4.3 Fit model to training data email_fit &lt;- email_wflow %&gt;% fit(data = train_data) #&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred tidy(email_fit) %&gt;% print(n = 31) #&gt; # A tibble: 31 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -1.11 0.264 -4.19 2.75e- 5 #&gt; 2 image -1.72 0.956 -1.80 7.23e- 2 #&gt; 3 viagra 2.30 182. 0.0126 9.90e- 1 #&gt; 4 num_char 0.0469 0.0271 1.73 8.41e- 2 #&gt; 5 line_breaks -0.00563 0.00151 -3.74 1.87e- 4 #&gt; 6 exclaim_subj -0.0721 0.271 -0.266 7.90e- 1 #&gt; 7 exclaim_mess 0.0101 0.00214 4.72 2.36e- 6 #&gt; 8 to_multiple_X1 -2.83 0.377 -7.49 6.71e-14 #&gt; 9 cc_X.1.68. -0.178 0.468 -0.382 7.03e- 1 #&gt; 10 attach_X.1.21. 2.39 0.381 6.27 3.61e-10 #&gt; 11 dollar_X.1.64. 0.0544 0.222 0.246 8.06e- 1 #&gt; 12 winner_yes 2.20 0.415 5.31 1.11e- 7 #&gt; 13 inherit_X.1.5. -9.14 756. -0.0121 9.90e- 1 #&gt; 14 inherit_X.5.10. 1.95 1.27 1.54 1.23e- 1 #&gt; 15 password_X.1.5. -1.71 0.759 -2.25 2.43e- 2 #&gt; 16 password_X.5.10. -12.4 420. -0.0296 9.76e- 1 #&gt; 17 password_X.10.20. -13.8 812. -0.0170 9.86e- 1 #&gt; 18 password_X.20.28. -13.8 814. -0.0170 9.86e- 1 #&gt; 19 format_X1 -0.802 0.161 -4.99 6.17e- 7 #&gt; 20 re_subj_X1 -2.78 0.400 -6.97 3.25e-12 #&gt; 21 urgent_subj_X1 2.57 1.13 2.27 2.31e- 2 #&gt; 22 number_small -0.659 0.170 -3.87 1.10e- 4 #&gt; 23 number_big 0.199 0.246 0.807 4.20e- 1 #&gt; 24 time_dow_Mon -0.0207 0.300 -0.0690 9.45e- 1 #&gt; 25 time_dow_Tue 0.306 0.273 1.12 2.63e- 1 #&gt; 26 time_dow_Wed -0.204 0.279 -0.731 4.65e- 1 #&gt; 27 time_dow_Thu -0.237 0.297 -0.799 4.24e- 1 #&gt; 28 time_dow_Fri 0.000702 0.284 0.00247 9.98e- 1 #&gt; 29 time_dow_Sat 0.261 0.307 0.851 3.95e- 1 #&gt; 30 time_month_Feb 0.912 0.183 4.97 6.68e- 7 #&gt; 31 time_month_Mar 0.638 0.187 3.41 6.52e- 4 77.4.4 Make predictions for test data email_pred &lt;- predict(email_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(test_data) email_pred #&gt; # A tibble: 785 × 23 #&gt; .pred_0 .pred_1 spam to_multiple from cc sent_email time image attach dollar #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.948 0.0522 0 0 1 0 0 2012-01-01 04:09:49 0 0 0 #&gt; 2 0.909 0.0914 0 0 1 0 0 2012-01-01 22:00:18 0 0 0 #&gt; 3 0.971 0.0292 0 0 1 0 0 2012-01-02 00:42:16 0 0 5 #&gt; 4 0.887 0.113 0 0 1 0 0 2012-01-01 20:58:14 0 0 0 #&gt; 5 0.992 0.00755 0 0 1 0 0 2012-01-02 02:07:22 0 0 21 #&gt; 6 1.00 0.000432 0 1 1 2 0 2012-01-02 13:09:45 0 0 0 #&gt; 7 0.999 0.000691 0 0 1 1 0 2012-01-02 10:12:51 0 0 0 #&gt; 8 1.00 0.000280 0 1 1 2 0 2012-01-02 16:24:21 0 0 0 #&gt; 9 0.971 0.0292 0 0 1 0 0 2012-01-03 04:34:50 0 0 11 #&gt; 10 0.978 0.0224 0 0 1 0 0 2012-01-03 08:33:28 0 0 18 #&gt; # ℹ 775 more rows #&gt; # ℹ 12 more variables: winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;, #&gt; # line_breaks &lt;int&gt;, format &lt;fct&gt;, re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, #&gt; # exclaim_mess &lt;dbl&gt;, number &lt;fct&gt; 77.4.5 Evaluate the performance email_pred %&gt;% roc_curve( truth = spam, .pred_1, event_level = &quot;second&quot; ) %&gt;% autoplot() email_pred %&gt;% roc_auc( truth = spam, .pred_1, event_level = &quot;second&quot; ) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 roc_auc binary 0.864 77.5 Making decisions 77.5.1 Cutoff probability: 0.5 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5. cutoff_prob &lt;- 0.5 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 700 68 Email labelled spam 7 10 77.5.2 Cutoff probability: 0.25 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25. cutoff_prob &lt;- 0.25 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 673 42 Email labelled spam 34 36 77.5.3 Cutoff probability: 0.75 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75. cutoff_prob &lt;- 0.75 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 703 73 Email labelled spam 4 5 "],["odd-notes-on-cross-validation.html", "78 ODD: Notes on Cross validation 78.1 Example: Regression 78.2 Example: Mixture models 78.3 Better Solution: Cross validation 78.4 Example 78.5 Choice of \\(K\\) 78.6 Summing up", " 78 ODD: Notes on Cross validation These lecture notes are an optional deep-dive into cross-validation. The lecture presents examples from regression and mixture models. I highlight the effectiveness of resampling methods like cross-validation when the observed data accurately reflects the true data-generating distribution. We have: Data \\(X_1, \\ldots, X_n\\). A tuning parameter \\(\\theta\\). Each value of \\(\\theta\\) corresponds to a different set of models. A function \\(L\\) that takes a fitted model and a data point and returns a measure of model quality. We would like to choose one model from the set of candidate models indexed by \\(\\theta\\). 78.1 Example: Regression Data: Pairs of predictors and response variables, \\((y_i, X_i)\\), \\(i = 1,\\ldots, n\\), \\(y_i \\in \\mathbb R\\), \\(X_i \\in \\mathbb R^p\\) Models: \\(y_i = X \\beta + \\epsilon\\), \\(\\beta_j = 0, j \\in S_\\theta\\), where \\(S_\\theta \\subseteq \\{1,\\ldots, p\\}\\). Model quality: Squared-error loss. If \\(\\hat \\beta_\\theta\\) are our estimates of the regression coefficients in model \\(\\theta\\), model quality is measured by \\[ L(\\hat \\beta_\\theta, (y_i, X_i)) = (y_i - X_i^T \\hat \\beta_\\theta)^2 \\] We want to choose a subset of the predictors that do the best job of explaining the response. Naive solution: Find the model that has the lowest value for the squared-error loss. Why doesn’t this work? 78.2 Example: Mixture models Data: \\(x_1,\\ldots, x_n\\), \\(x_i \\in \\mathbb R\\) Models: Gaussian mixture models with \\(\\theta\\) mixture components. Model quality: Negative log likelihood of the data. If \\(\\hat p_\\theta\\) is the density of the fitted model with \\(\\theta\\) components, model quality is measured by \\(L(\\hat p_\\theta, x_i) = -\\log \\hat p_\\theta(x_i)\\). We want to choose the number of mixture components that best explains the data. Naive solution: Choose the number of mixture components that minimizes the negative log likelihood of the data. 78.3 Better Solution: Cross validation Idea: Instead of measuring model quality on the same data we used to fit the model, we estimate model quality on new data. If we knew the true distribution of the data, we could simulate new data and use a Monte Carlo estimate based on the simulations. We can’t actually get new data, and so we hold some back when we fit the model and then pretend that the held back data is new data. Procedure: Divide the data into \\(K\\) folds Let \\(X^{(k)}\\) denote the data in fold \\(k\\), and let \\(X^{(-k)}\\) denote the data in all the folds except for \\(k\\). For each fold and each value of the tuning parameter \\(\\theta\\), fit the model on \\(X^{(-k)}\\) to get \\(\\hat f_\\theta^{(k)}\\) Compute \\[ \\text{CV}(\\theta) = \\frac{1}{n} \\sum_{k=1}^K \\sum_{x \\in X^{(k)}} L(\\hat f_\\theta^{(k)}, x) \\] Choose \\(\\hat \\theta = \\text{argmin}_{\\theta} \\text{CV}(\\theta)\\) 78.4 Example n &lt;- 100 p &lt;- 20 X &lt;- matrix(rnorm(n * p), nrow = n) y &lt;- rnorm(n) get_rss_submodels &lt;- function(n_predictors, y, X) { if (n_predictors == 0) { lm_submodel &lt;- lm(y ~ 0) } else { lm_submodel &lt;- lm(y ~ 0 + X[, 1:n_predictors, drop = FALSE]) } return(sum(residuals(lm_submodel)^2)) } p_vec &lt;- 0:p rss &lt;- sapply(p_vec, get_rss_submodels, y, X) plot(rss ~ p_vec) get_cv_error &lt;- function(n_predictors, y, X, folds) { cv_vec &lt;- numeric(length(unique(folds))) for (f in unique(folds)) { cv_vec[f] &lt;- rss_on_held_out( n_predictors, y_train = y[folds != f], X_train = X[folds != f, ], y_test = y[folds == f], X_test = X[folds == f, ] ) } return(mean(cv_vec)) } rss_on_held_out &lt;- function(n_predictors, y_train, X_train, y_test, X_test) { if (n_predictors == 0) { lm_submodel &lt;- lm(y_train ~ 0) preds_on_test &lt;- rep(0, length(y_test)) } else { lm_submodel &lt;- lm(y_train ~ 0 + X_train[, 1:n_predictors, drop = FALSE]) preds_on_test &lt;- X_test[, 1:n_predictors, drop = FALSE] %*% coef(lm_submodel) } return(sum((y_test - preds_on_test)^2)) } K &lt;- 5 ## normally you would do this at random folds &lt;- rep(1:K, each = n / K) p_vec &lt;- 0:p cv_errors &lt;- sapply(p_vec, get_cv_error, y, X, folds) plot(cv_errors ~ p_vec) 78.5 Choice of \\(K\\) Considerations: Larger \\(K\\) means more computation (although sometimes there is a shortcut for leave-one-out cross validation) Larger \\(K\\) means less bias in the estimate of model accuracy Larger \\(K\\) also means more variance in the estimate, so we don’t necessarily want \\(K = n\\) Usually choose \\(K = 5\\) or \\(K = 10\\) If your problem is structured (e.g. time series, spatial), you should choose the folds to respect the structure. 78.6 Summing up We can use simulations to estimate arbitrary functions of our random variables. If we know the underlying distribution, we can simply simulate from it (Monte Carlo integration). If we don’t know the underlying distribution, we can “simulate” from the data by resampling from the data (cross validation; ish). Resampling methods will do well to the extent that the observed data reflect the true data-generating distribution. "],["labmoney.html", "79 Optional Lab Packages Data collection via web scraping Data cleaning Data visualization and interpretation", " 79 Optional Lab This lab is optional. You are welcome to try it and can count it either as a lab or as a portfolio piece. Disclaimer It is a more extensive lab that I’m not quite happy with. Specifically, I think it is a bit too time consuming and I’ve not been able to calibrate the difficulty. In this assignment you will work on data scraping. If you need some help, I recommend working thru this interactive tutorial titled Money in politics. They use the same website. Tired of typing your password? Chances are your browser has already saved your password, but if not, you can ask Git to save (cache) your password for a period of time, where you indicate the period of time in seconds. For example, if you want it to cache your password for 1 hour, that would be 3,600 seconds. To do so, run the following in the console, usethis::use_git_config(credential.helper = \"cache --timeout=3600\"). If you want to cache it for a longer amount of time, you can adjust the number in the code. Packages In this assignment we will use the following packages: tidyverse: a collection of packages for doing data analysis in a “tidy” way robotstxt: provides functions to download and parse robots.txt files, making it easy to check if bots (spiders, crawler, scrapers, …) are allowed to access specific resources on a domain rvest: helps you scrape information from web pages scales: provides the internal scaling infrastructure used by ggplot2 and gives you tools to override the default breaks, labels, transformations and palettes Data collection via web scraping The data come from OpenSecrets.org, a “website tracking the influence of money on U.S. politics, and how that money affects policy and citizens’ lives”. This website is hosted by The Center for Responsive Politics, which is a nonpartisan, independent nonprofit that “tracks money in U.S. politics and its effect on elections and public policy.”4 Before getting started, let’s check that a bot has permissions to access pages on this domain. library(robotstxt) paths_allowed(&quot;https://www.opensecrets.org&quot;) #&gt; [1] TRUE Our goal is to scrape data for contributions in all election years Open Secrets has data for. Because that means repeating a task many times, let’s first write a function that works on the first page. Confirm it works on a few others. Then iterate it over pages for all years. Complete the following set of steps in the scrape-pac.R file in the scripts folder of your repository. This file already contains some starter code to help you out. Write a function called scrape_pac() that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year. This function should have one input: the URL of the webpage and should return a data frame. rename variables scraped, using snake_case naming. clean up the Country of Origin/Parent Company variable with str_squish(). add a new column to the data frame for year. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesn’t take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the str_sub() function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify “last 4 characters”. Define the URLs for 2020, 2018, and 1998 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do? Construct a vector called urls that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year. Map the scrape_pac() function over urls in a way that will result in a data frame called pac_all. Write the data frame to a csv file called pac-all.csv in the data folder. ✅⬆️ If you haven’t yet done so, now is definitely a good time to commit and push your changes to GitHub with an appropriate commit message (e.g. “Data scraping complete”). Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. In your R Markdown file, load pac-all.csv and report its number of observations and variables using inline code. Data cleaning In this section, we clean the pac_all data frame to prepare it for analysis and visualization. We have two goals in data cleaning: Separate the country_parent into two such that country and parent company appear in different columns for country-level analysis. Convert contribution amounts in total, dems, and repubs from character strings to numeric values. The following exercises walk you through how to make these fixes to the data. Use the separate() function to separate country_parent into country and parent columns. Note that country and parent company names are separated by \\ (which will need to be specified in your function) and also note that there are some entries where the \\ sign appears twice and in these cases we want to only split the value at the first occurrence of \\. This can be accomplished by setting the extra argument in to \"merge\" so that the cell is split into only 2 segments, e.g. we want \"Denmark/Novo Nordisk A/S\" to be split into \"Denmark\" and \"Novo Nordisk A/S\". (See help for separate() for more on this.) End your code chunk by printing out the top 10 rows of your data frame (if you just type the data frame name it should automatically do this for you). Remove the character strings including $ and , signs in the total, dems,and repubs columns and convert these columns to numeric. End your code chunk by printing out the top 10 rows of your data frame (if you just type the data frame name it should automatically do this for you). A couple hints to help you out: The $ character is a special character so it will need to be escaped. Some contribution amounts are in the millions (e.g. Anheuser-Busch contributed a total of $1,510,897 in 2008). In this case we need to remove all occurrences of ,, which we can do by using str_remove_all() instead of str_remove().  ✅ ⬆️ Now is a good time to knit your document, and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Data visualization and interpretation Create a line plot of total contributions from all foreign-connected PACs in the Canada and Mexico over the years. Once you have made the plot, write a brief interpretation of what the graph reveals. Few hints to help you out: Filter for only Canada and Mexico. Calculate sum of total contributions from PACs for each year for each country by using a sequence of group_by() then summarize(). Make a plot of total contributions (y-axis) by year (x-axis) where two lines identified by different colors represent each of Canada and Mexico. Recreate the following visualization. Once you have made the plot, write a brief interpretation of what the graph reveals. Note that these are only UK contributions. You will need to make use of functions from the scales package for axis labels as well as from ggplot2. Remember, if you can’t figure out certain bits, you can always ask on the class github!  ✅ ⬆️ Knit your document, and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Source: Open Secrets - About.↩︎ "],["welcome-to-quantifying-uncertainty.html", "80 Welcome to Quantifying Uncertainty 80.1 Module Materials", " 80 Welcome to Quantifying Uncertainty This module is designed to introduce ideas related to representing uncertainty. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 80.1 Module Materials Slides from Lectures Quantifying Uncertainty Bootstrapping Suggested Readings All subchapters of this module About the Needle The NYT’s election forecast needle is stressing people out with fake jitter Kopf, D. (2020, November 3). Why it’s okay to look at the New York Times “election needle”. Quartz Lab Lab "],["quantifying-uncertainty.html", "81 Quantifying Uncertainty", " 81 Quantifying Uncertainty You can follow along with the slides here if they do not appear below. Introduce them to insight::format_p() !&mdash; Brenton Wiernik ️‍ (@bmwiernik) October 23, 2021 "],["bootstrapping.html", "82 Bootstrapping", " 82 Bootstrapping You can follow along with the slides here if they do not appear below. "],["notes-on-hypothesis-testing.html", "83 Notes on Hypothesis Testing 83.1 Hypothesis testing for a single proportion 83.2 One vs. two sided hypothesis tests 83.3 Testing for independence", " 83 Notes on Hypothesis Testing 83.1 Hypothesis testing for a single proportion ## Packages library(tidyverse) library(tidymodels) 83.1.1 Case Study: Organ Donors Organ donors may seek the assistance of a medical consultant to help them navigate the surgical process. The consultant’s goal is to minimize any potential complications during the procedure and recovery. Patients may choose a consultant based on their past clients’ complication rates. A consultant marketed her services by highlighting her exceptional track record. She stated that while the average complication rate for liver donor surgeries in the US is 10%, only 3 out of 62 surgeries she facilitated resulted in complications. She believes this rate demonstrates the significant impact of her work in reducing complications, making her a top choice for potential patients. 83.1.1.1 Data organ_donor %&gt;% count(outcome) #&gt; # A tibble: 2 × 2 #&gt; outcome n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 complication 3 #&gt; 2 no complication 59 83.1.2 Parameter vs. statistic A parameter for a hypothesis test is the “true” value of interest. We typically estimate the parameter using a sample statistic as a point estimate. \\(p~\\): true rate of complication \\(\\hat{p}~\\): rate of complication in the sample = \\(\\frac{3}{62}\\) = 0.048 83.1.3 Correlation vs. causation Is it possible to assess the consultant’s claim using the data? No. The claim is that there is a causal connection, but the data are observational. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate. Although it is not possible to assess the causal claim, it is still possible to test for an association using these data. The question to consider is, is the low complication rate of 3 out of 62 surgeries ( \\(\\hat{p}\\) = 0.048) simply due to chance? 83.1.4 Two claims Null hypothesis: “There is nothing going on” Complication rate for this consultant is no different than the US average of 10% Alternative hypothesis: “There is something going on” Complication rate for this consultant is lower than the US average of 10% 83.1.5 Hypothesis testing as a court trial Null hypothesis, \\(H_0\\): Defendant is innocent Alternative hypothesis, \\(H_A\\): Defendant is guilty Present the evidence: Collect data Judge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?” Yes: Fail to reject \\(H_0\\) No: Reject \\(H_0\\) 83.1.6 Hypothesis testing framework Start with a null hypothesis, \\(H_0\\), that represents the status quo Set an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we’re testing for Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true) if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis if they do, then reject the null hypothesis in favor of the alternative 83.1.7 Setting the hypotheses Which of the following is the correct set of hypotheses? \\(H_0: p = 0.10\\); \\(H_A: p \\ne 0.10\\) \\(H_0: p = 0.10\\); \\(H_A: p &gt; 0.10\\) \\(H_0: p = 0.10\\); \\(H_A: p &lt; 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} \\ne 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} &gt; 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} &lt; 0.10\\) 83.1.8 Simulating the null distribution Since \\(H_0: p = 0.10\\), we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10. Describe how you would simulate the null distribution for this study using a bag of chips. How many chips? What colors? What do the colors indicate? How many draws? With replacement or without replacement? 83.1.9 What do we expect? When sampling from the null distribution, what is the expected proportion of success (complications)? 83.1.10 Simulation Here are some simulations…. #&gt; sim1 #&gt; complication no complication #&gt; 3 59 #&gt; [1] 0.0484 #&gt; sim2 #&gt; complication no complication #&gt; 9 53 #&gt; [1] 0.145 #&gt; sim3 #&gt; complication no complication #&gt; 8 54 #&gt; [1] 0.129 This process is getting boring… We need a way to automate this process! 83.1.11 Using tidymodels to generate the null distribution null_dist &lt;- organ_donor %&gt;% specify(response = outcome, success = &quot;complication&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = c(&quot;complication&quot; = 0.10, &quot;no complication&quot; = 0.90)) %&gt;% generate(reps = 100, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) #&gt; The `&quot;simulate&quot;` generation type has been renamed to `&quot;draw&quot;`. Use `type = #&gt; &quot;draw&quot;` instead to quiet this message. #&gt; Response: outcome (factor) #&gt; Null Hypothesis: point #&gt; # A tibble: 100 × 2 #&gt; replicate stat #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.161 #&gt; 2 2 0.081 #&gt; 3 3 0.161 #&gt; 4 4 0.145 #&gt; 5 5 0.097 #&gt; 6 6 0.145 #&gt; 7 7 0.081 #&gt; 8 8 0.097 #&gt; 9 9 0.161 #&gt; 10 10 0.048 #&gt; # ℹ 90 more rows 83.1.12 Visualizing the null distribution What would you expect the center of the null distribution to be? ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.01) + labs(title = &quot;Null distribution&quot;) 83.1.13 Calculating the p-value, visually What is the p-value, i.e. in what % of the simulations was the simulated sample proportion at least as extreme as the observed sample proportion? 83.1.14 Calculating the p-value, directly null_dist %&gt;% filter(stat &lt;= (3/62)) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 × 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.12 83.1.15 Significance level We often use 5% as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. This cutoff value is called the significance level, \\(\\alpha\\). If p-value &lt; \\(\\alpha\\), reject \\(H_0\\) in favor of \\(H_A\\): The data provide convincing evidence for the alternative hypothesis. If p-value &gt; \\(\\alpha\\), fail to reject \\(H_0\\) in favor of \\(H_A\\): The data do not provide convincing evidence for the alternative hypothesis. 83.1.16 Conclusion What is the conclusion of the hypothesis test? Since the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than 10% (overall US complication rate). 83.1.17 Let’s get real 100 simulations is not sufficient We usually simulate around 15,000 times to get an accurate distribution, but we’ll do 1,000 here for efficiency. 83.1.18 Run the test null_dist &lt;- organ_donor %&gt;% specify(response = outcome, success = &quot;complication&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = c(&quot;complication&quot; = 0.10, &quot;no complication&quot; = 0.90)) %&gt;% generate(reps = 1000, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) #&gt; The `&quot;simulate&quot;` generation type has been renamed to `&quot;draw&quot;`. Use `type = &quot;draw&quot;` instead to quiet this message. 83.1.19 Visualize and calculate ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = 3/62, color = &quot;red&quot;) null_dist %&gt;% filter(stat &lt;= 3/62) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 × 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.124 83.2 One vs. two sided hypothesis tests 83.2.1 Types of alternative hypotheses One sided (one tailed) alternatives: The parameter is hypothesized to be less than or greater than the null value, &lt; or &gt; Two sided (two tailed) alternatives: The parameter is hypothesized to be not equal to the null value, \\(\\ne\\) Calculated as two times the tail area beyond the observed sample statistic More objective, and hence more widely preferred Average systolic blood pressure of people with Stage 1 Hypertension is 150 mm Hg. Suppose we want to use a hypothesis test to evaluate whether a new blood pressure medication has an effect on the average blood pressure of heart patients. What are the hypotheses? 83.3 Testing for independence 83.3.1 Is yawning contagious? Do you think yawning is contagious? An experiment conducted by the MythBusters tested if a person can be subconsciously influenced into yawning if another person near them yawns. (Video) 83.3.2 Study description This study involved 50 participants who were randomly divided into two groups. 34 participants were in the treatment group where they saw someone near them yawn, while 16 participants were in the control group where they did not witness a yawn. The data are in the openintro package: yawn yawn %&gt;% count(group, result) #&gt; # A tibble: 4 × 3 #&gt; group result n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 ctrl not yawn 12 #&gt; 2 ctrl yawn 4 #&gt; 3 trmt not yawn 24 #&gt; 4 trmt yawn 10 83.3.3 Proportion of yawners yawn %&gt;% count(group, result) %&gt;% group_by(group) %&gt;% mutate(p_hat = n / sum(n)) #&gt; # A tibble: 4 × 4 #&gt; # Groups: group [2] #&gt; group result n p_hat #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 ctrl not yawn 12 0.75 #&gt; 2 ctrl yawn 4 0.25 #&gt; 3 trmt not yawn 24 0.706 #&gt; 4 trmt yawn 10 0.294 Proportion of yawners in the treatment group: \\(\\frac{10}{34} = 0.2941\\) Proportion of yawners in the control group: \\(\\frac{4}{16} = 0.25\\) Difference: \\(0.2941 - 0.25 = 0.0441\\) Our results match the ones calculated on the MythBusters episode. 83.3.4 Independence? Based on the proportions we calculated, do you think yawning is really contagious, i.e. are seeing someone yawn and yawning dependent? #&gt; # A tibble: 4 × 4 #&gt; # Groups: group [2] #&gt; group result n p_hat #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 ctrl not yawn 12 0.75 #&gt; 2 ctrl yawn 4 0.25 #&gt; 3 trmt not yawn 24 0.706 #&gt; 4 trmt yawn 10 0.294 83.3.5 Dependence, or another possible explanation? The observed differences might suggest that yawning is contagious, i.e. seeing someone yawn and yawning are dependent. But the differences are small enough that we might wonder if they might simple be due to chance. Perhaps if we were to repeat the experiment, we would see slightly different results. So we will do just that - well, somewhat - and see what happens. Instead of actually conducting the experiment many times, we will simulate our results. 83.3.6 Two competing claims “There is nothing going on.” Yawning and seeing someone yawn are independent, yawning is not contagious, observed difference in proportions is simply due to chance. \\(\\rightarrow\\) Null hypothesis “There is something going on.” Yawning and seeing someone yawn are dependent, yawning is contagious, observed difference in proportions is not due to chance. \\(\\rightarrow\\) Alternative hypothesis 83.3.7 Simulation setup A regular deck of cards is comprised of 52 cards: 4 aces, 4 of numbers 2-10, 4 jacks, 4 queens, and 4 kings. Take out two aces from the deck of cards and set them aside. The remaining 50 playing cards to represent each participant in the study: 14 face cards (including the 2 aces) represent the people who yawn. 36 non-face cards represent the people who don’t yawn. 83.3.8 Running the simulation Shuffle the 50 cards at least 7 times5 to ensure that the cards counted out are from a random process. Count out the top 16 cards and set them aside. These cards represent the people in the control group. Out of the remaining 34 cards (treatment group) count the (the number of people who yawned in the treatment group). Calculate the difference in proportions of yawners (treatment - control), and plot it on the board. Mark the difference you find on the dot plot on the board. 83.3.9 Simulation by hand Do the simulation results suggest that yawning is contagious, i.e. does seeing someone yawn and yawning appear to be dependent? yawn-sim-results 83.3.10 Simulation by computation null_dist &lt;- yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;trmt&quot;, &quot;ctrl&quot;)) Start with the data frame Specify the variables Since the response variable is categorical, specify the level which should be considered as “success” yawn %&gt;% {{ specify(response = result, explanatory = group, success = &quot;yawn&quot;) }} State the null hypothesis (yawning and whether or not you see someone yawn are independent) yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% {{ hypothesize(null = &quot;independence&quot;) }} Generate simulated differences via permutation yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% {{ generate(100, type = &quot;permute&quot;) }} Calculate the sample statistic of interest (difference in proportions) Since the explanatory variable is categorical, specify the order in which the subtraction should occur for the calculation of the sample statistic, \\((\\hat{p}_{treatment} - \\hat{p}_{control})\\). yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% {{ calculate(stat = &quot;diff in props&quot;, order = c(&quot;trmt&quot;, &quot;ctrl&quot;)) }} 83.3.11 Recap Save the result Start with the data frame Specify the variables Since the response variable is categorical, specify the level which should be considered as “success” State the null hypothesis (yawning and whether or not you see someone yawn are independent) Generate simulated differences via permutation Calculate the sample statistic of interest (difference in proportions) Since the explanatory variable is categorical, specify the order in which the subtraction should occur for the calculation of the sample statistic, \\((\\hat{p}_{treatment} - \\hat{p}_{control})\\). {{null_dist &lt;- yawn %&gt;% }} specify(response = outcome, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;treatment&quot;, &quot;control&quot;)) 83.3.12 Visualizing the null distribution What would you expect the center of the null distribution to be? ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.05) + labs(title = &quot;Null distribution&quot;) 83.3.13 Calculating the p-value, visually What is the p-value, i.e. in what % of the simulations was the simulated difference in sample proportion at least as extreme as the observed difference in sample proportions? 83.3.14 Calculating the p-value, directly null_dist %&gt;% filter(stat &gt;= 0.0441) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 × 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.53 83.3.15 Conclusion What is the conclusion of the hypothesis test? Do you “buy” this conclusion? http://www.dartmouth.edu/~chance/course/topics/winning_number.html↩︎ "],["lab11.html", "84 Lab: So what if you smoke when pregnant? Non-parametric-based inference Getting started 84.1 The data 84.2 Exercises 84.3 Wrap up", " 84 Lab: So what if you smoke when pregnant? Non-parametric-based inference In 2004, North Carolina released a comprehensive birth record dataset that holds valuable insights for researchers examining the connection between expectant mothers’ habits and practices and their child’s birth. In this lab, we’ll be exploring a randomly selected subset of the data. You’ll learn how to use non-parametric-based inference to analyze the impact of maternal smoking during pregnancy on the weight of the baby. You will also explore the relationships between other variables, such as the mother’s age and the baby’s birth weight. Through the exercises, you will practice data manipulation, visualization, hypothesis testing, and calculating confidence intervals. You can find the lab here Getting started Packages In this lab, we will work with the tidyverse, infer, and openintro packages. We can install and load them with the following code: library(tidyverse) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 library(infer) library(openintro) Housekeeping Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. Project name Update the name of your project to match the lab’s title. Warm up Before diving into the dataset, let’s warm up with some simple exercises. YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. Doing so will allow you to personalize your Rmd file. Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update team name” in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. Set a seed! In this lab, we’ll be generating random samples. To make sure our results stay consistent, make sure to set a seed before you dive into the sampling process. Otherwise, the data will change each time you knit your lab. To set your seed, simply find the designated R chunk in your R Markdown file and insert the seed value there. 84.1 The data Load the ncbirths data from the openintro package: data(ncbirths) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is provided in the following table. variable description fage father’s age in years. mage mother’s age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. Before analyzing any new dataset, it’s important to get to know your data. Start by summarizing the variables and determining if their data type. Are they categorical? Are they numerical? For numerical variables, check for outliers. If you aren’t sure or want to take a closer look at the data, create a graph. 84.2 Exercises What are the cases in this data set? How many cases are there in our sample? 84.2.1 Baby weights Wen, Shi Wu, Michael S. Kramer, and Robert H. Usher. “Comparison of birth weight distributions between Chinese and Caucasian infants.” American Journal of Epidemiology 141.12 (1995): 1177-1187. A 1995 study suggests that average weight of White babies born in the US is 3,369 grams (7.43 pounds).In this dataset, we have pretty limited information on race, we only know whether the mother is White. We will make the simplifying assumption that babies of White mothers are also White, i.e. whitemom = \"white\". (Yes, I know that this assumption is a gross oversimplification). We want to evaluate whether the average weight of White babies has changed since 1995. Our null hypothesis should state “there is nothing going on”, i.e. no change since 1995: \\(H_0: \\mu = 7.43~pounds\\). Our alternative hypothesis should reflect the research question, i.e., some change since 1995. Since the research question doesn’t state a direction for the change, we use a two-sided alternative hypothesis: \\(H_A: \\mu \\ne 7.43~pounds\\). Create a filtered data frame called ncbirths_white that contains data only from White mothers. Then, calculate the mean of the weights of their babies. Are the criteria necessary for conducting simulation-based inference satisfied? Explain your reasoning. Let’s discuss how this test would work. Our goal is to simulate a null distribution of sample means that is centered at the null value of 7.43 pounds. In order to do so, we: - take a bootstrap sample of from the original sample, - calculate this bootstrap sample’s mean, - repeat these two steps a large number of times to create a bootstrap distribution of means centered at the observed sample mean, - shift this distribution to be centered at the null value by subtracting / adding X to all bootstrap mean (X = difference between mean of bootstrap distribution and null value), and - calculate the p-value as the proportion of bootstrap samples that yielded a sample mean at least as extreme as the observed sample mean. Run the appropriate hypothesis test, visualize the null distribution, calculate the p-value, and interpret the results in the context of the data and the hypothesis test. 84.2.2 Baby weight vs. smoking Consider the relationship between a mother’s smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make side-by-side box plots displaying the relationship between habit and weight. What does the plot highlight about the relationship between these two variables? Before continuing, create a cleaned version of the dataset by removing any rows with missing values for habit or weight. Name this version ncbirths_clean. Calculate the observed difference in means between the baby weights of smoking and non-smoking mothers. ncbirths_clean %&gt;% group_by(habit) %&gt;% summarise(mean_weight = mean(weight)) We can see that there’s an observable difference, but is this difference meaningful? Is it statistically significant? We can answer this question by conducting a hypothesis test. Write the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different. \\(H_0\\): _________ (\\(\\mu_1 = \\mu_2\\)) \\(H_A\\): _________ (\\(\\mu_1 \\ne \\mu_2\\)) Run the appropriate hypothesis test, calculate the p-value, and interpret the results in context of the data and the hypothesis test. Construct a 95% confidence interval for the difference between the average weights of babies born to smoking and non-smoking mothers. 84.2.3 Mother’s age vs. baby weight In this portion of the analysis, we focus on two variables. The first one is maturemom. First, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. The other variable of interest is lowbirthweight. Conduct a hypothesis test evaluating whether the proportion of low birth weight babies is higher for mature mothers. Use \\(\\alpha = 0.05\\). State the hypotheses Verify the conditions Run the test and calculate the p-value State your conclusion within context of the research question Calculate a confidence interval for the difference between the proportions of low birth weight babies between mature and younger mothers. Interpret the interval in the context of the data and explain what it means. 84.3 Wrap up In this lab, you practiced using non-parametric inference to analyze the impact of maternal smoking during pregnancy on the weight of the baby. You also explored the relationships between other variables, such as the mother’s age and the baby’s birth weight. You’ve gained experience with data manipulation, visualization, hypothesis testing, and calculating confidence intervals. Remember to save your work, commit your changes, and push them to your Git repository! "],["welcome-to-base-r-and-simulating-data.html", "85 Welcome to Base R and Simulating Data 85.1 Module Materials 85.2 Estimated Video Length", " 85 Welcome to Base R and Simulating Data This module is designed to introduce you to the key ideas behind simulating data. In essence, we are making our own data instead of using someone else’s. Eventually, there will be video lectures. Once those videos exist, please watch the videos and work your way through the notes. The videos will eventually start on the next page. You will eventually be able to find the video playlist for this module here. The slides used to make the videos in this module will be able to be found in the slides repo. I just got an email from an interested PhD applicant asking about what else they can be doing to prepare. I ended up drafting up (what I think is) a nice recommendation for starting to learn Monte Carlo Sims. A ... (1/7)&mdash; Dr. Amanda Kay Montoya (@AmandaKMontoya) July 25, 2023 85.1 Module Materials Slides from Lectures Suggested Readings All subchapters of this module, including … R4DS TBD, including [TBD] [TBD] Activities [TBD] Lab Simulating on Mars 85.2 Estimated Video Length No of videos : TBD Average length of video : TBD Total length of playlist : TBD "],["lecture-getting-started-with-simulating-data-in-r.html", "86 Lecture: Getting started with simulating data in R", " 86 Lecture: Getting started with simulating data in R You can follow along with the slides here if they do not appear below. "],["data_simulations.html", "87 Getting Started with Data Simulations in R 87.1 Learning Goals 87.2 Generating Variables 87.3 Generate character vectors with rep() 87.4 Creating datasets with quantitative and categorical variables 87.5 Repeatedly simulate data with replicate() 87.6 What’s the next step?", " 87 Getting Started with Data Simulations in R This module introduces the basics of data simulation using R. We won’t cover every aspect of data simulation, but we’ll delve into some core functions. Drawing inspiration from Ariel Muldoon’s tutorial on simulation helper functions, this module presents practical methods for creating simulated datasets. Our focus will be on functions from the base R package, which is included with every installation of R. These functions are easy to use and are great for getting started with simulations. We will cover various approaches to generating both quantitative and categorical variables, enhancing your data science toolkit. 87.1 Learning Goals By the end of these notes, you will be able to: Generate quantitative variables using functions such as rnorm(), runif(), and rpois(). Generate categorical variables like group names using rep(), and explore various methods to replicate patterns. Integrate the creation of datasets that include both quantitative and categorical variables. Use replicate() to perform the data simulation process multiple times. 87.2 Generating Variables 87.2.1 Generating Random Numbers One method for generating numeric data in R involves using the “random deviate” functions to pull random numbers from statistical distributions. These functions, which all begin with the letter ‘r’, are designed to generate random numbers from specific statistical distributions. Here are some commonly used random deviate functions in R: Normal Distribution (rnorm()): Used for generating data that follows a Gaussian distribution. Uniform Distribution (runif()): Ideal for creating evenly distributed numbers over a specified range. Poisson Distribution (rpois()): Useful for modeling count-based data, such as the number of events happening within a fixed period. These functions are part of the core stats package. The syntax for these functions typically requires specifying the number of random numbers to generate (n) and defining the parameters for the distribution you want to draw from. For example, to generate random values from these distributions, you would use: # Generating five random normal values rnorm(5, mean = 0, sd = 1) #&gt; [1] 1.024 0.194 -0.568 -0.643 -0.592 # Generating five uniform values between 0 and 1 runif(5, min = 0, max = 1) #&gt; [1] 0.0235 0.2465 0.8360 0.9768 0.5828 # Generating five Poisson-distributed values with a mean of 3 rpois(5, lambda = 3) #&gt; [1] 1 3 4 3 6 The r functions for a chosen distribution all work similarly. Other distributions from the stats package. binomial, F, log-normal, beta, exponential, and gamma distributions. More exotic distributions are available through additional packages. 87.2.2 Deep Dive into the Normal Distribution (rnorm()) rnorm() is frequently used in statistical simulations to generate random numbers that follow a normal (Gaussian) distribution. The function rnorm() requires three main arguments: n: the number of observations to generate. mean: the mean of the normal distribution. sd: the standard deviation of the distribution. (note that sd is the standard deviation, not the variance) Generating five random numbers from a \\(Normal(0, 1)\\) (aka the standard normal) distribution can be done as follows: rnorm(n = 5, mean = 0, sd = 1) #&gt; [1] 2.278 -1.534 -0.313 -0.445 -0.307 Notice how I have written out the arguments explicitly. If I were coding “lazily”, I’d likely have written something like this… rnorm(5) # This uses the default values for mean and sd #&gt; [1] 0.540 0.227 -0.416 -0.552 -0.497 Lazy, concise, code can look pretty mysterious to someone reading my code (or to my future self reading my code). Because I used the default values for mean and sd, it’s not clear exactly what distribution I drew the numbers from. When future me revisits this code, she will have to remember what the defaults are. Is that a realistic request? Maybe, but future me has to remember a lot of things, so as a kindness to her, its worthwhile to make those arguments explicit. That way future me does not have to remember what the defaults are and she can quickly work her way through the code. In other words, specify the arguments explicitly, even if you are using default values. This practice makes your code self-documenting, meaning that anyone reading it can understand at a glance which distribution the data comes from, without needing to remember the default settings of the function. Here’s an even more readable version for generating five random numbers from a standard normal distribution, with explicit argument names: rnorm(n = 5, mean = 0, sd = 1) #&gt; [1] 0.432 0.331 -1.265 0.444 2.129 When I’m writing code for a project that I know will be shared with others, I always (try to) write out the arguments. When I’m writing code for myself, I’m more likely to use the defaults. It’s a balance between being kind to future me and being efficient in the moment. 87.2.2.1 Setting the random seed for reproducible random numbers As you’ve likely noticed, each time we run this code, we’ve gotten different numbers. To ensure reproducibility, we need to set the seed with set.seed(). Reproducibility means that whenwhen running the same code can be desirable Setting the seed will ensure that others can replicate your results. This ability to get the same result when running the same code is also helpful when making example datasets to demonstrate coding issues or when asking questions on forums like Stack Overflow. I also set seeds when I’m creating functions to ensure they work correctly. However, in most simulations, setting the seed isn’t necessary (and may be counter productive). By setting the seed before running rnorm(), we can reproduce the values we generate. set.seed(16) rnorm(n = 5, mean = 0, sd = 1) #&gt; [1] 0.476 -0.125 1.096 -1.444 1.148 If we reset the seed to the same number and run the code again, we get the same values. set.seed(16) rnorm(n = 5, mean = 0, sd = 1) #&gt; [1] 0.476 -0.125 1.096 -1.444 1.148 87.2.2.2 Change parameters in rnorm() To get a quick set of numbers, it’s easy to use the default parameter values in rnorm(). However, we can certainly change these values. For example, when exploring the long-run behavior of variance estimated from linear models, varying the standard deviation is useful. The sd argument specifies the standard deviation of the normal distribution. So drawing from a \\(Normal(0, 4)\\) can be done by setting sd to 2. rnorm(n = 5, mean = 0, sd = 2) #&gt; [1] -0.937 -2.012 0.127 2.050 1.146 I’ve seen other people make changes the mean and standard deviation to create variables that are within a specific range. For example, if the mean is large and the standard deviation is small in relation to the mean, we can generate strictly positive numbers. (I usually use runif() for this, which we’ll see below.) rnorm(n = 5, mean = 50, sd = 20) #&gt; [1] 86.9 52.2 35.1 83.2 64.4 87.2.2.3 Using vectors of values for the parameter arguments We can also use vectors for arguments! Using a vector allows us to pull random numbers from a normal distribution with distinct parameters. For example, simulating data with different group means but the same variance can be useful, especially when creating data that we’d use for an ANOVA. I’ll keep the standard deviation at 1 but will draw data from three distributions centered at three locations: 0, 5, and 20. I make 10 total draws by setting n to 10. Observe the repeating pattern: the function iteratively draws one value from each distribution until the total number requested is reached. This approach can lead to an imbalance in the sample size per distribution. rnorm(n = 10, mean = c(0, 5, 20), sd = 1) #&gt; [1] -1.663 5.576 20.473 -0.543 6.128 18.352 -0.314 4.817 21.470 -0.866 We can also pass a vector to sd, allowing both the means and standard deviations to vary among the three distributions. rnorm(n = 10, mean = c(0, 5, 20), sd = c(1, 5, 20) ) #&gt; [1] 1.527 10.271 40.601 0.840 6.085 6.549 0.133 4.645 1.146 -1.022 Things are different for the n argument. If a vector is passed to n, the length of that vector is taken to be the number required (see the Arguments section of the documentation for details). In the example below, we only get 3 values because the vector for n is length 3. rnorm(n = c(2, 10, 10), mean = c(0, 5, 20), sd = c(1, 5, 20) ) #&gt; [1] 0.281 7.724 22.617 87.2.3 Example of using the simulated numbers from rnorm() Up to this point, we’ve printed the results of each simulation. In practice, we’d save our vectors as objects in R to use them for some future task. For example, we might simulate two unrelated variables to see how correlated they appear. This fun exercise can show how variables can seem related by chance, especially with small sample sizes. (See Spurious correlations for numerous fun empirical examples of this phenomenon. Or Spurious scholar for some fun academic examples. Let’s generate two quantitative vectors of length 10, named x and y, and plot the results. I’m using the default mean and sd. x &lt;- rnorm(n = 10, mean = 0, sd = 1) y &lt;- rnorm(n = 10, mean = 0, sd = 1) data.frame(var1 = x, var2 = y) %&gt;% ggplot( aes( x = var1, y = var2 ) ) + geom_point(color = &quot;#56B4E9&quot;, size = 7) + theme_bw() + labs(title = &quot;Scatterplot of Two Random Variables&quot;) 87.2.4 runif() pulls from the uniform distribution Pulling random numbers from other distributions is very similar to using rnorm(), so we’ll go through them quickly. I’ve started using runif() regularly, especially when I want to generate strictly positive continuous numbers. The uniform distribution is continuous, with numbers uniformly distributed between some minimum and maximum. By default, runif() pulls random numbers between 0 and 1. The first argument,like all of these r functions, is the number of deviates to generate: runif(n, min = 0, max = 1) Let’s generate 5 numbers between 0 and 1. runif(n = 5, min = 0, max = 1) #&gt; [1] 0.999 0.943 0.250 0.648 0.113 What if we want to generate 5 numbers between 50 and 100? We change the parameter values. runif(n = 5, min = 50, max = 100) #&gt; [1] 81.6 66.9 82.7 64.3 54.2 87.2.5 Example of using the simulated numbers from runif() runif() is handy for demonstrating the effect of the relative size of variable values on the size of the estimated coefficient in a regression. For example, the size of the coefficient measured in kilometers is smaller than if that variable was converted into meters. Let’s generate data with the response variable (y) from a standard normal distribution and an explanatory variable with values between 1 and 2. The two variables are unrelated. Writing out argument names for clarity, we generate the data as follows: set.seed(16) y &lt;- rnorm(n = 100, mean = 0, sd = 1) x1 &lt;- runif(n = 100, min = 1, max = 2) head(x1) #&gt; [1] 1.96 1.08 1.71 1.33 2.00 1.45 Now, simulate a second explanatory variable with values between 200 and 300. This variable is also unrelated to the other two. x2 &lt;- runif(n = 100, min = 200, max = 300) head(x2) #&gt; [1] 220 263 210 245 265 257 We can fit a linear regression model via lm(). lm(y ~ x1 + x2) #&gt; #&gt; Call: #&gt; lm(formula = y ~ x1 + x2) #&gt; #&gt; Coefficients: #&gt; (Intercept) x1 x2 #&gt; 0.38089 0.10494 -0.00191 The coefficient for the second variable, with a larger relative size, is generally going to be smaller than the first since the change in y for a “1-unit increase” in x depends on the units of x. This is a good example of how the scale of the variables can affect the size of the estimated coefficients in a regression model. 87.2.6 Discrete counts with rpois() Let’s look at one last function for generating random numbers, rpois(), which generates discrete integers (including 0) from a Poisson distribution. The Poisson distribution is often used to model count data, such as the number of robo phone calls received in an hour or the number of fish caught in a day. I use rpois() for generating counts to explore generalized linear models. I’ve also found this function useful in gaining a better understanding of the shape of Poisson distributions with different means. The Poisson distribution is a single parameter distribution. The function looks like this: rpois(n, lambda) The single parameter, lambda, is the mean. It has no default setting, so it must always be defined by the user. Let’s generate five values from a Poisson distribution with a mean of 2.5. Note that the mean of the Poisson distribution can be any non-negative value (i.e., it doesn’t have to be an integer) even though the observed values will be discrete integers. rpois(n = 5, lambda = 2.5) #&gt; [1] 2 1 4 1 2 87.2.7 Example of using the simulated numbers from rpois() Let’s explore the Poisson distribution a little more, seeing how the distribution looks when changing the mean. This helps understand the distribution better, including why it so often does a poor job modeling ecological count data. We’ll draw 100 values from a Poisson distribution with a mean of 5. We’ll name this vector y and look at a summary of those values. y &lt;- rpois(100, lambda = 5) The values we simulated fall between 1 and 14. summary(y) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1.00 3.00 5.00 4.83 6.00 11.00 There is a mild right skew when we draw a histogram of the values. palette_OkabeIto &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#999999&quot;) data.frame(y) %&gt;% ggplot(aes(x=y)) + geom_histogram(binwidth=1,color=&quot;black&quot;, fill=&quot;#E69F00&quot;) + labs(title = &quot;Poisson distribution with a mean of 5&quot;, x = &quot;Value&quot;, y = &quot;Count&quot;) + theme_bw() Let’s do the same for a Poisson distribution with a mean of 100. The range of values is still pretty narrow; there are no values even remotely close to 0. y &lt;- rpois(100, lambda = 100) summary(y) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 76.0 94.8 102.0 101.3 108.0 124.0 The distribution is more symmetric compared to the distribution with the smaller mean. data.frame(y) %&gt;% ggplot(aes(x=y)) + geom_histogram(binwidth=5, color=&quot;black&quot;, fill= &quot;#CC79A7&quot;) + labs(title=&quot;Poisson distribution with a mean of 100&quot;,x= &quot;Value&quot;, y = &quot;Count&quot;) + theme_bw() An alternative to the Poisson distribution for discrete integers is the negative binomial distribution. The MASS package has a function called rnegbin() for generating random numbers from the negative binomial distribution. 87.3 Generate character vectors with rep() Quantitative variables are great, but in simulations, we often need categorical variables, as well. Categorical variables are useful for simulations that involve groups or treatments. The rep() function in R simplifies creating these kinds of variables by replicating elements of vectors or lists. This functionality is particularly useful to avoid manually writing out an entire vector, making it easier to construct large datasets. 87.3.1 Using letters and LETTERS The first argument of rep() is the vector to be repeated. One option is to write out the character vector you want to repeat. You can also get a simple character vector through the use of letters or LETTERS. These are built in constants in R. letters is the 26 lowercase letters of the Roman alphabet and LETTERS is the 26 uppercase letters. You can extract letters using brackets ([). These built-in constants are convenient for making basic categorical vectors when the form of the categories doesn’t matter. Typing out the word and brackets is often more straightforward than creating a vector of characters with quotes. Here are the first two letters. letters[1:2] #&gt; [1] &quot;a&quot; &quot;b&quot; And the last 17 LETTERS. LETTERS[10:26] #&gt; [1] &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; 87.3.2 Repeat each element of a vector with each There are three arguments to repeat the values in a vector in rep() with different patterns: each, times, and length.out. These approaches can be used individually or in combination. With each, we repeat each unique character in the vector the defined number of times. The replication is done “elementwise”, so the repeats of each unique character are all in a row. Let’s repeat two characters three times each. The resulting vector is 6 observations long. This is an example of how you can quickly make a grouping variable for simulating data to be used in a two-sample analysis. rep(letters[1:2], each = 3) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; 87.3.3 Repeat a whole vector with the times argument The times argument can be used when we want to repeat the whole vector rather than repeating it elementwise. We’ll make a two-group variable again, but this time we’ll change the repeating pattern of the values in the variable. rep(letters[1:2], times = 3) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;b&quot; 87.3.4 Set the output vector length with the length.out argument The length.out argument has rep() repeat the whole vector. However, it repeats the vector only until the defined length is reached. Using length.out is another way to get unbalanced groups. Rather than defining the number of repeats like we did with each and times we define the length of the output vector. Here we’ll make a two-group variable of length 5. This means the second group will have one less value than the first. rep(letters[1:2], length.out = 5) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;b&quot; &quot;a&quot; 87.3.5 Repeat each element a different number of times Unlike each and length.out, we can use times with a vector of values. This allows us to repeat each element of the character vector a different number of times. This is one way to simulate unbalanced groups. Using times with a vector repeats each element like each does, which can make it harder to remember which argument does what. Let’s repeat the first element twice and the second four times. rep(letters[1:2], times = c(2, 4) ) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; 87.3.6 Combining each with times As your simulation situation get more complicated, you may need more complicated patterns for your categorical variable. The each argument can be combined with times to first repeat each value elementwise (via each) and then repeat that whole pattern (via times). When using times this way it will only take a single value and not a vector. Let’s repeat each value twice, 3 times. rep(letters[1:2], each = 2, times = 3) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; 87.3.7 Combining each with length.out Similarly we can use each with length.out. This can lead to some imbalance. Here we’ll repeat the two values twice each but with a total final vector length of 7. rep(letters[1:2], each = 2, length.out = 7) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; Note you can’t use length.out and times together (if you try, length.out will be given priority and times ignored). 87.4 Creating datasets with quantitative and categorical variables We now have some tools for creating quantitative data as well as categorical. Which means it’s time to make some datasets! We’ll create several simple ones to get the general idea. 87.4.1 Simulate data with no differences among two groups Let’s start by simulating data for a simple two-sample analysis with no difference between the groups. We’ll create a dataset with six observations—three for each group. We will use the functions discussed earlier to create this dataset and store the results in a data frame for organization and ease of access. Unlike some beginners who might use cbind() for this purpose, it’s more efficient and standard to use data.frame() directly: We’ll be using the tools we reviewed above but will now name the output and combine them into a data.frame. This last step isn’t always necessary, but does help keep things organized in certain types of simulations. First, we’ll make separate vectors for the continuous and categorical data and then bind them together via data.frame(). Notice there is no need to use cbind() here, which is commonly done by R beginners (I know I did!). Instead we can use data.frame() directly. group &lt;- rep(letters[1:2], each = 3) response &lt;- rnorm(n = 6, mean = 0, sd = 1) data.frame(group, response) #&gt; group response #&gt; 1 a 0.493 #&gt; 2 a 0.523 #&gt; 3 a 1.237 #&gt; 4 b 0.356 #&gt; 5 b 0.575 #&gt; 6 b -0.422 Some people like to define the vectors and the data frame simultaneously to minimize clutter in the R environment. Here is how you can do it all at once: data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ) #&gt; group response #&gt; 1 a 0.402 #&gt; 2 a 0.959 #&gt; 3 a -1.876 #&gt; 4 b -0.212 #&gt; 5 b 1.437 #&gt; 6 b 0.386 (Personally, I like having longer code that is easier for future Mason to read. But it’s up to you!) Now, let’s add another layer to our dataset by introducing a second categorical variable. Assume we have two factors, not one, and that these factors are ‘crossed’—meaning every possible combination of the two factors appears at least once in the dataset. For the second factor, which we’ll call factor, let’s give it the values “C”, “D”, and “E”: LETTERS[3:5] #&gt; [1] &quot;C&quot; &quot;D&quot; &quot;E&quot; We need to repeat the values in a way that every combination of group and factor is present in the dataset at one time. Remember the group factor is repeated elementwise. rep(letters[1:2], each = 3) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; We need to repeat the three values twice. But what argument do we use in rep() to do so? rep(LETTERS[3:5], ?) Does each work? rep(LETTERS[3:5], each = 2) #&gt; [1] &quot;C&quot; &quot;C&quot; &quot;D&quot; &quot;D&quot; &quot;E&quot; &quot;E&quot; No, if we use each then each element is repeated twice and some of the combinations of group and factor are missing. This is a job for the times or length.out arguments, so the whole vector is repeated. We can repeat the whole vector twice using times, or use length.out = 6. I do the former. In the result below we can see every combination of the two factors is present once. data.frame(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) #&gt; group factor response #&gt; 1 a C 0.426 #&gt; 2 a D 0.290 #&gt; 3 a E -0.364 #&gt; 4 b C 1.978 #&gt; 5 b D 1.087 #&gt; 6 b E -0.587 87.4.2 Simulate data with a difference among groups The dataset above is one with “no difference” among groups. What if the means were different between groups? Let’s make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. Remembering how rnorm() works with a vector of means is key here. The function draws iteratively from each distribution. response &lt;- rnorm(n = 6, mean = c(5, 10), sd = 1) response #&gt; [1] 4.41 12.48 4.74 10.27 5.37 10.02 How do we get the group pattern correct? group &lt;- rep(letters[1:2], ?) We need to repeat the whole vector three times instead of elementwise. To get the groups in the correct order we need to use times or length.out in rep(). With length.out we define the output length of the vector, which is 6. Alternatively we could use times = 3 to repeat the whole vector 3 times. group &lt;- rep(letters[1:2], length.out = 6) group #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;b&quot; These can then be combined into a data.frame. Working out this process is another reason why sometimes we build each vector separately prior to combining together. data.frame(group, response) #&gt; group response #&gt; 1 a 4.41 #&gt; 2 b 12.48 #&gt; 3 a 4.74 #&gt; 4 b 10.27 #&gt; 5 a 5.37 #&gt; 6 b 10.02 87.4.3 Multiple quantitative variables with groups For our last dataset we’ll have two groups, with 10 observations per group. rep(LETTERS[3:4], each = 10) #&gt; [1] &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; #&gt; [20] &quot;D&quot; Let’s make a dataset that has two quantitative variables, unrelated to both each other and the groups. One variable ranges from 10 and 15 and one from 100 and 150. How many observations should we draw from each uniform distribution? runif(n = ?, min = 10, max = 15) We had 2 groups with 10 observations each and 2*10 = 20. So we need to use n = 20 in runif(). Here is the dataset made in a single step. data.frame(group = rep(LETTERS[3:4], each = 10), x = runif(n = 20, min = 10, max = 15), y = runif(n = 20, min = 100, max = 150)) #&gt; group x y #&gt; 1 C 13.2 127 #&gt; 2 C 13.9 137 #&gt; 3 C 12.7 135 #&gt; 4 C 14.3 123 #&gt; 5 C 11.7 118 #&gt; 6 C 14.6 108 #&gt; 7 C 12.8 142 #&gt; 8 C 13.5 104 #&gt; 9 C 13.9 107 #&gt; 10 C 12.2 145 #&gt; 11 D 12.0 117 #&gt; 12 D 13.7 121 #&gt; 13 D 14.8 145 #&gt; 14 D 11.7 120 #&gt; 15 D 13.3 140 #&gt; 16 D 10.8 107 #&gt; 17 D 14.0 148 #&gt; 18 D 14.9 113 #&gt; 19 D 13.5 105 #&gt; 20 D 14.4 120 What happens if we get this wrong? If we’re lucky we get an error. data.frame(group = rep(LETTERS[3:4], each = 10), x = runif(n = 15, min = 10, max = 15), y = runif(n = 15, min = 100, max = 150)) #&gt; Error in data.frame(group = rep(LETTERS[3:4], each = 10), x = runif(n = 15, : arguments imply differing number of rows: 20, 15 But if we get things wrong and the number we use goes into the number we need evenly, R will recycle the vector to the end of the data.frame(). This mistake is hard to catch. If you look carefully through the output below, you can see that the continuous variables start to repeat on line 10. data.frame(group = rep(LETTERS[3:4], each = 10), x = runif(n = 10, min = 10, max = 15), y = runif(n = 10, min = 100, max = 150)) #&gt; group x y #&gt; 1 C 12.3 108 #&gt; 2 C 13.8 115 #&gt; 3 C 12.4 105 #&gt; 4 C 10.1 125 #&gt; 5 C 10.8 130 #&gt; 6 C 11.0 129 #&gt; 7 C 11.5 149 #&gt; 8 C 13.5 139 #&gt; 9 C 11.6 120 #&gt; 10 C 12.9 120 #&gt; 11 D 12.3 108 #&gt; 12 D 13.8 115 #&gt; 13 D 12.4 105 #&gt; 14 D 10.1 125 #&gt; 15 D 10.8 130 #&gt; 16 D 11.0 129 #&gt; 17 D 11.5 149 #&gt; 18 D 13.5 139 #&gt; 19 D 11.6 120 #&gt; 20 D 12.9 120 87.5 Repeatedly simulate data with replicate() The replicate() function is a real workhorse when making repeated simulations. It is a member of the apply family in R, and is specifically made (per the documentation) for the repeated evaluation of an expression (which will usually involve random number generation). We want to repeatedly simulate data that involves random number generation, so that sounds like a useful tool. The replicate() function takes three arguments: n, which is the number of replications to perform. This is to set the number of repeated runs we want. expr, the expression that should be run repeatedly. This is often a function. simplify, which controls the type of output the results of expr are saved into. Use simplify = FALSE to get output saved into a list instead of in an array. 87.5.1 Simple example of replicate() Let’s say we want to simulate some values from a normal distribution, which we can do using the rnorm() function as above. But now instead of drawing some number of values from a distribution one time we want to do it many times. This could be something we’d do when demonstrating the central limit theorem, for example. Doing the random number generation many times is where replicate() comes in. It allows us to run the function in expr exactly n times. Here I’ll generate 5 values from a standard normal distribution three times. Notice the addition of simplify = FALSE to get a list as output. The output below is a list of three vectors. Each vector is from a unique run of the function, so contains five random numbers drawn from the normal distribution with a mean of 0 and standard deviation of 1. set.seed(16) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) #&gt; [[1]] #&gt; [1] 0.476 -0.125 1.096 -1.444 1.148 #&gt; #&gt; [[2]] #&gt; [1] -0.4684 -1.0060 0.0636 1.0250 0.5731 #&gt; #&gt; [[3]] #&gt; [1] 1.847 0.112 -0.746 1.658 0.722 Note if I don’t use simplify = FALSE I will get a matrix of values instead of a list. Each column in the matrix is the output from one run of the function. In this case there will be three columns in the output, one for each run, and 5 rows. This can be a useful output type for some simulations. I focus on list output throughout the rest of this post only because that’s what I have been using recently for simulations. set.seed(16) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1) ) #&gt; [,1] [,2] [,3] #&gt; [1,] 0.476 -0.4684 1.847 #&gt; [2,] -0.125 -1.0060 0.112 #&gt; [3,] 1.096 0.0636 -0.746 #&gt; [4,] -1.444 1.0250 1.658 #&gt; [5,] 1.148 0.5731 0.722 87.5.2 An equivalent for() loop example A for() loop can be used in place of replicate() for simulations. With time and practice I’ve found replicate() to be much more convenient in terms of writing the code. However, in my experience some folks find for() loops intuitive when they are starting out in R. I think it’s because for() loops are more explicit on the looping process: the user can see the values that i takes and the output for each i iteration is saved into the output object because the code is written out explicitly. In my example I’ll save the output of each iteration of the loop into a list called list1. I initialize this as an empty list prior to starting the loop. To match what I did with replicate() I do three iterations of the loop (i in 1:3), drawing 5 values via rnorm() each time. The result is identical to my replicate() code above. It took a little more code to do it but the process is very clear since it is explicitly written out. set.seed(16) list1 &lt;- list() # Make an empty list to save output in for (i in 1:3) { # Indicate number of iterations with &quot;i&quot; list1[[i]] &lt;- rnorm(n = 5, mean = 0, sd = 1) # Save output in list for each iteration } list1 #&gt; [[1]] #&gt; [1] 0.476 -0.125 1.096 -1.444 1.148 #&gt; #&gt; [[2]] #&gt; [1] -0.4684 -1.0060 0.0636 1.0250 0.5731 #&gt; #&gt; [[3]] #&gt; [1] 1.847 0.112 -0.746 1.658 0.722 87.5.3 Using replicate() to repeatedly make a dataset Earlier we were making datasets with random numbers and some grouping variables. Our code looked like data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ) #&gt; group response #&gt; 1 a -1.663 #&gt; 2 a 0.576 #&gt; 3 a 0.473 #&gt; 4 b -0.543 #&gt; 5 b 1.128 #&gt; 6 b -1.648 We could put this process as the expr argument in replicate() to get many simulated datasets. I would do something like this if I wanted to compare the long-run performance of two different statistical tools using the exact same random datasets. I’ll replicate things 3 times again to easily see the output. I still use simplify = FALSE to get things into a list. simlist &lt;- replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) We can see this result is a list of three data.frames. str(simlist) #&gt; List of 3 #&gt; $ :&#39;data.frame&#39;: 6 obs. of 2 variables: #&gt; ..$ group : chr [1:6] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; ... #&gt; ..$ response: num [1:6] -0.314 -0.183 1.47 -0.866 1.527 ... #&gt; $ :&#39;data.frame&#39;: 6 obs. of 2 variables: #&gt; ..$ group : chr [1:6] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; ... #&gt; ..$ response: num [1:6] 1.03 0.84 0.217 -0.673 0.133 ... #&gt; $ :&#39;data.frame&#39;: 6 obs. of 2 variables: #&gt; ..$ group : chr [1:6] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; ... #&gt; ..$ response: num [1:6] -0.943 -1.022 0.281 0.545 0.131 ... Here is the first one. simlist[[1]] #&gt; group response #&gt; 1 a -0.314 #&gt; 2 a -0.183 #&gt; 3 a 1.470 #&gt; 4 b -0.866 #&gt; 5 b 1.527 #&gt; 6 b 1.054 87.6 What’s the next step? I’m ending here, but there’s still more to learn about simulations. For a simulation to explore long-run behavior, some process is going to be repeated many times. We did this via replicate(). The next step would be to extract whatever results are of interest. This latter process is often going to involve some sort of looping. By saving our generated variables or data.frames into a list we’ve made it so we can loop via list looping functions like lapply() or purrr::map(). The family of map functions are newer and have a lot of convenient output types that make them pretty useful. Happy simulating! "],["lab12.html", "88 Lab: Simulating data Learning goals Getting started and warming up Exercises Exercise 2: Growing Our Colonists Exercise 4: Preparing for the Unexpected Stretch Tasks (Optional) Conclusion", " 88 Lab: Simulating data Welcome to the Mars Colony Simulation Lab! In this lab, we will embark on a journey to Mars, simulating potential colonists’ data to plan our future Mars colony. As potential planners of a future Mars settlement, it is crucial to understand the demographic and professional attributes that will help sustain a human presence on Mars. This lab teaches you to simulate these attributes using R. Learning goals Simulate Quantitative Variables: Learn to create simulated quantitative variables representing colonist attributes using rnorm(), runif(), and rpois(). Generate Character Variables: Use rep() to create categorical variables that represent different groups within the colonists, such as their professions or roles in the colony. Replication of Data Simulation: Utilize replicate() to repeat the data simulation process, representing multiple scenarios or batches of potential colonists. Getting started and warming up Packages In this lab, we will work with the tidyverse, ggplot2, and MASS packages. We can install and load them with the following code: if (!require(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;) library(ggplot2) if (!require(&quot;MASS&quot;)) install.packages(&quot;MASS&quot;) ## Warning: package &#39;MASS&#39; was built under R version 4.4.2 library(MASS) # Install and load the tidyverse package if (!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) ## Warning: package &#39;lubridate&#39; was built under R version 4.4.2 library(tidyverse) Exercises Instead of analyzing other people’s data, we’ll be generating our own! You will generate basic attributes for 100 potential colonists. These attributes include age, health, and fitness levels, which are critical for survival on Mars. Exercise 1: Simulating Our Colonists Imagine you’re choosing the first 100 colonists for Mars. Let’s simulate their ages assuming a mean age of 30 and a standard deviation of 5—aiming for youth but with sufficient experience. Setting Up Your Simulation. We’ll use the rnorm() function, which is perfect for this task because it generates normally distributed random numbers. Let’s examine the arguments for rnorm() using the ?rnorm command in R. Parameters: n = 100: The number of random ages we want to generate. mean = 30: The average age of our colonists. We’re targeting a young, but experienced group of individuals. sd = 5: The standard deviation, representing age variability around the mean. Here’s code to generate an age distribution: set.seed(123) age &lt;- rnorm(100, mean = 30, sd = 5) 1.1. Create a dataframe to store your colonists’ attributes. I’ve already gotten you started, but you’ll need to add their ages. df_colonists &lt;- data.frame(id = 1:100, # this is our id column ) # add the age column here ## Error in data.frame(id = 1:100, ): argument is missing, with no default Now, let’s visualize the age distribution of our colonists using a histogram. This will help us understand the diversity within our potential Mars colony. 1.2. Consider the histograms of the age distribution that we’ve generated (as well as two more I added). What do you notice about the age distribution of our colonists? How does the seed affect the spread of the distribution? What roles would these colonists have? Let’s decide….. We need engineers, scientists, and medics in equal numbers. We’ll create a variable, role, with three categories: engineer, scientist, and medic. We’ll use the rep() function to simulate this categorical variable. I’ve demonstrated several ways you can use rep() to create the role variable. Try them out. set.seed(1) roleA &lt;- rep(c(&quot;engineer&quot;, &quot;scientist&quot;, &quot;medic&quot;), length.out = 100) # this works if we want to set a specific number of each role roleB &lt;- rep(c(&quot;engineer&quot;, &quot;scientist&quot;, &quot;medic&quot;), each = 34, length.out = 100) roleC &lt;- rep(c(&quot;engineer&quot;, &quot;scientist&quot;, &quot;medic&quot;), times = c(33, 33, 33), length.out = 100) # if you want to use sampling weights roleD &lt;- sample(c(&quot;engineer&quot;, &quot;scientist&quot;, &quot;medic&quot;), replace = TRUE, size = 100, prob = c(1,1,1)) # if you want to randomly shuffle roleE &lt;- sample(roleB, size = 100, replace = FALSE) If you want to see the raw data as a table, click here df &lt;- data.frame(row = rep(1:100, 5), role = c(roleA, roleB, roleC, roleD, roleE), method = c(rep(&quot;Method A&quot;, length(roleA)), rep(&quot;Method B&quot;, length(roleB)), rep(&quot;Method C&quot;, length(roleC)), rep(&quot;Method D&quot;, length(roleD)), rep(&quot;Method E&quot;, length(roleE)))) df %&gt;% DT::datatable( rownames = FALSE, class = &quot;cell-border stripe&quot;, filter = list(position = &quot;top&quot;), options = list( pageLength = nrow(df)/5, autoWidth = TRUE, bInfo = FALSE, paging = FALSE ) ) 1.3. Create a role variable that suits the needs of your colony, and explain why you chose that method. Next, we’ll simulate the health and fitness levels of our colonists. Health and fitness are essential for the physical and mental well-being of our colonists, ensuring they can adapt to the harsh Martian environment. Setting Up Your Simulation. We’ll use the runif() function to generate random numbers drawn from a uniform distribution. This distribution is ideal for simulating health and fitness levels, as it ensures equal probability across the range of values. Examine the arguments for runif(), using the ?runif command in R. Notice that although some of the arguments are the same, such as n, and parameters for the distribution are different. On Mars, health is measured using the MARSGAR (Martian Adult Resilience Score Gauging Astronaut Readiness; Musk, 2026) scale, which ranges from 0 to 100. Like the suspiciously-similar Apgar score for newborns, a score of 0 indicates that a colonist has no signs of life: no heartbeat, no breathing, no response to stimulation, no muscle tone, and central cyanosis/pallor. A score of 100 indicates a colonist in perfect health. Take this scoring into consideration when you simulate the health of your colonists. 1.4. Add a uniformally-distributed MARSGAR variable to your colony. set.seed(123) df_colonists$marsgar &lt;- 1 # &quot;replace this with your code&quot; Now, let us examine our colonists. We will create a scatter plot to visualize the relationship between age and health. This will help us understand the health distribution of our potential Mars colony. ## Warning: package &#39;viridis&#39; was built under R version 4.4.2 What a contrived plot! It seems that there’s no relationship between age and health in the data. How odd! (Or is it?) Spoiler: It’s not odd. We generated the data univariately, so there’s no reason for there to be a relationship between age and health. Exercise 2: Growing Our Colonists Next, we’ll simulate the relationship among several attributes of our colonists. We’ll consider technical skills, problem-solving abilities, psychological resilience, teamwork, and adaptability. These attributes are crucial for the success of our Mars colony, ensuring that our colonists can work together effectively and overcome challenges. There are several ways to simulate variables with specific correlations. I will show two approaches: - using multiple steps with rnorm, which is a less coputationally efficient, but more accessible approach and - using mvrnorm, which generates data from a multivariate distribustion. Basic method To start, let’s illustrate a simpler method using rnorm to simulate how certain attributes like age could influence others such as technical skills—assuming that older colonists, with more experience, likely have higher technical skills. To model this, we define the variable technical_skills as a linear function of age, incorporating random noise for variability. Specifically, we use the regression equation technical_skills = 2 * age + noise, where noise is normally distributed with a mean of zero and a standard deviation of one. This implies that for each additional year of age, a colonist’s technical skills increase by two points, with some random noise added in. 2.1. Simulate technical skills based on age using the equation technical_skills = 2 * age + noise. # Simulate technical skills based on age # Set the seed for reproducibility set.seed(1235) # Create a variable for technical skills df_colonists$technical_skills &lt;- 2 * df_colonists$age + rnorm(100, mean = 0, sd = 1) Now, let’s visualize the relationship between age and technical skills using a scatter plot. 2.2. Simulate problem-solving abilities based on their assigned role Now its your turn! Simulate problem-solving abilities based on their assigned role. Recall that the options are “engineer”, “scientist”, “medic”. First you need to think about the relationship between the role AND the variable being simulated. Then you can write up the equation that would create that relationship. And then you can write the code to simulate the variable. df_colonists$problem_solving &lt;- 1 # replace this with your code Click this to see a hint If you’re stuck, you can check the source code for the solution by examining the following chuck in this corresponding rmd file exercise2.2solution. Click this to see a solution df_colonists$problem_solving[df_colonists$role == &quot;engineer&quot;] &lt;- rnorm(sum(df_colonists$role == &quot;engineer&quot;), mean = 100, sd = 10) df_colonists$problem_solving[df_colonists$role == &quot;scientist&quot;] &lt;- rnorm(sum(df_colonists$role == &quot;scientist&quot;), mean = 80, sd = 10) df_colonists$problem_solving[df_colonists$role == &quot;medic&quot;] &lt;- rnorm(sum(df_colonists$role == &quot;medic&quot;), mean = 60, sd = 10) Exploring Correlations with mvrnorm Now, this approach works well if you already know what you want your model to look like. But what if you want to simulate multiple variables with specific correlations? That’s when the mvrnorm function comes in handy from the MASS package. This function generates data from a multivariate normal distribution, where we can specify means, standard deviations, and the correlation between variables. This is ideal for simulating more complex interdependencies that might exist among colonists’ traits. Setting Up Your Simulation. As you’ve hopefully gathered, we’ll use the `mvrnorm function to generate random numbers drawn from a multivariate distribution. I encourage you to examine the arguments for mvrnorm(), using the ?mvrnorm command in R. Examine the arguments for mvrnorm(), using the ?mvrnorm command in R. Notice that the arguments are similar to rnorm(), but with the addition of the Sigma parameter, which represents the covariance matrix. This matrix allows us to specify the relationships between the variables we’re simulating. (Remember, the covariance matrix is a square matrix that shows the variances of each variable on the diagonal and the covariances between variables on the off-diagonals. A helpful trick is that the correlation matrix is just a standardized covariance matrix, where the diagonals are all 1s, and the off-diagonals are the correlations between variables. You can convert between the two using the cov2cor() and cor2cov() functions in R.) Parameters for Simulation: n = 100: We are simulating attributes for 100 colonists. mu = c( 50, 50): Represents the average scores for Resilience and Agreeableness. Sigma = matrix(c(100, 50, 50, 100), ncol = 2): The covariance matrix, where diagonals represent variances and the off-diagonals represent the covariance between the traits. 3.2. Simulate Resilience and Agreeableness library(MASS) # Define mean and covariance matrix mean_traits &lt;- c(50, 50) cov_matrix &lt;- matrix(c(100, 50, 50, 100), ncol = 2) # Generate correlated data traits_data &lt;- mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix, empirical = FALSE) colnames(traits_data) &lt;- c(&quot;resilience&quot;, &quot;agreeableness&quot;) df_colonists &lt;- cbind(df_colonists, traits_data) But, now, we need to simulate more attributes for our colonists that don’t all have the same summary statistics. We need to simulate the big five personality traits: openness, conscientiousness, extraversion, agreeableness, and neuroticism. These traits are essential for understanding how colonists will interact with each other and cope with the challenges of living on Mars. Your task to simulate these traits using the mvrnorm function, using parameters that would make sense for these traits, and examine how closely your colonists match up with your population parameters. 3.3. Simulate Big Five Traits using the Correlation Matrix provided and examine how closely your colonists match up with your parameters. Population Parameters for Big Five Traits I’ve provided a correlation matrix from Park et al. (2020)6 7 that you can use to get you started on simulating these traits. Click this to see the code that built the matrix # Define the bigfive bigfive &lt;- c(&quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;) # Create an empty matrix with dimensions equal to the number of bigfive cor_matrix &lt;- matrix(0, ncol = length(bigfive), nrow = length(bigfive), dimnames = list(bigfive, bigfive)) # Initialize the diagonal to 1 diag(cor_matrix) &lt;- 1 # Function to set correlation values ensuring symmetry set_correlation &lt;- function(matrix, row, column, value) { matrix &lt;- as.matrix(matrix) matrix[row, column] &lt;- value matrix[column, row] &lt;- value return(matrix) } #for(i in 1:10) { # print(paste0(&quot;set_correlation(cor_matrix, &#39;&quot;,ma_res$construct_x[i],&quot;&#39; ,&#39;&quot;, ma_res$construct_y[i],&quot;&#39;,&quot;,round(ma_res[[6]][[i]][[&quot;barebones&quot;]][[&quot;mean_r&quot;]],4),&quot;)&quot;))} cor_matrix_bigfive &lt;- cor_matrix &lt;- cor_matrix %&gt;% set_correlation(&quot;ES&quot;, &quot;AG&quot;, 0.1576) %&gt;% set_correlation(&quot;ES&quot;, &quot;CO&quot;, 0.2306) %&gt;% set_correlation(&quot;ES&quot;, &quot;EX&quot;, 0.2599) %&gt;% set_correlation(&quot;ES&quot;, &quot;OP&quot;, 0.072) %&gt;% set_correlation(&quot;AG&quot;, &quot;CO&quot;, 0.2866) %&gt;% set_correlation(&quot;AG&quot;, &quot;EX&quot;, 0.1972) %&gt;% set_correlation(&quot;AG&quot;, &quot;OP&quot;, 0.1951) %&gt;% set_correlation(&quot;CO&quot;, &quot;EX&quot;, 0.186) %&gt;% set_correlation(&quot;CO&quot;, &quot;OP&quot;, 0.1574) %&gt;% set_correlation(&quot;EX&quot;, &quot;OP&quot;, 0.2949) print(cor_matrix_bigfive) ## EX ES AG CO OP ## EX 1.00 0.260 0.20 0.19 0.295 ## ES 0.26 1.000 0.16 0.23 0.072 ## AG 0.20 0.158 1.00 0.29 0.195 ## CO 0.19 0.231 0.29 1.00 0.157 ## OP 0.29 0.072 0.20 0.16 1.000 Click this to get a matrix you can quickly paste cor_matrix_bigfive &lt;- matrix(c( 1.0000, 0.2599, 0.1972, 0.1860, 0.2949, 0.2599, 1.0000, 0.1576, 0.2306, 0.0720, 0.1972, 0.1576, 1.0000, 0.2866, 0.1951, 0.1860, 0.2306, 0.2866, 1.0000, 0.1574, 0.2949, 0.0720, 0.1951, 0.1574, 1.0000 ), nrow=5, ncol=5, byrow=TRUE, dimnames=list(c(&quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;), c(&quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;))) Because this task is a tad more complex than the previous simulation, I’ve created some optional scaffolding by structured hints. Click here to see the hints Step 1 Hint Begin by defining the parameters for your simulation. This includes the number of colonists, the mean and standard deviation for each of the Big Five personality traits, and the correlation matrix that models the relationships between these traits. . Step 2 Hint Simulate the Big Five personality traits using the mvrnorm function. This function will use your defined means, standard deviations, and correlation matrix to generate a dataset that reflects the psychological profiles of your simulated colonists. . Step 3 Hint Calculate the means and standard deviations of the Big Five personality traits in your simulated dataset. This will allow you to compare the simulated data to the population parameters you defined earlier. Great! We have successfully simulated a dataset of 100 colonists with interdependent skills. The summary statistics show that the mean and standard deviation of the simulated data match the population parameters. The correlation matrix also aligns (fairly well) with the specified values. But did we just get lucky? Let’s run the simulation multiple times to ensure the results are consistent across different scenarios. Click this text for a solution # Set seed for reproducibility seed &lt;- 123 set.seed(seed) # Load the Mass, Matrix, and then tidyverse (otherwise have to use conflicted package to handle conflict ## (note to self that MASS has a select function library(MASS); library(Matrix); library(tidyverse); library(conflicted) conflicts_prefer(dplyr::select()) # Number of colonists n_colonists &lt;- 100 # Define mean and covariance matrix ## &quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot; var_names &lt;- c( &quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;) mean_traits &lt;- c(-.5, .5, .25, .5, 0) # Mean of each trait sd_traits &lt;- c(1, .9, 1, 1, 1) # Standard deviation of each trait # Get correlation matrix cor_matrix_bigfive &lt;- matrix(c( 1.0000, 0.2599, 0.1972, 0.1860, 0.2949, 0.2599, 1.0000, 0.1576, 0.2306, 0.0720, 0.1972, 0.1576, 1.0000, 0.2866, 0.1951, 0.1860, 0.2306, 0.2866, 1.0000, 0.1574, 0.2949, 0.0720, 0.1951, 0.1574, 1.0000 ), nrow=5, ncol=5, byrow=TRUE, dimnames=list(c(&quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;), c(&quot;EX&quot;, &quot;ES&quot;, &quot;AG&quot;, &quot;CO&quot;, &quot;OP&quot;))) # Check if correlation matrix is positive definite (if all eigenvalues are positive) eigen_values &lt;- eigen(cor_matrix_bigfive)$values is_positive_definite &lt;- all(eigen_values &gt; 0) print(is_positive_definite) ## [1] TRUE # Convert correlation matrix to covariance matrix cov_matrix_bigfive &lt;- cor_matrix_bigfive * (sd_traits %*% t(sd_traits)) # Print the covariance matrix print(cov_matrix_bigfive) ## EX ES AG CO OP ## EX 1.00 0.234 0.20 0.19 0.295 ## ES 0.23 0.810 0.14 0.21 0.065 ## AG 0.20 0.142 1.00 0.29 0.195 ## CO 0.19 0.208 0.29 1.00 0.157 ## OP 0.29 0.065 0.20 0.16 1.000 simulated_data &lt;- mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix_bigfive) # Add colonist_id and seed simulated_data &lt;- cbind.data.frame(colonist_id = 1:n_colonists, # add colonist_id seed = seed, # add seed simulated_data) # add simulated data # Print the first few rows of the simulated data print(head(simulated_data)) ## colonist_id seed EX ES AG CO OP ## 1 1 123 0.90 1.806 -1.00 0.82 0.141 ## 2 2 123 -0.55 1.933 -0.48 0.92 0.023 ## 3 3 123 -2.01 -0.095 -1.18 -0.23 -0.218 ## 4 4 123 -0.40 0.642 -0.72 0.83 0.262 ## 5 5 123 -0.18 -0.346 -0.22 0.31 0.585 ## 6 6 123 -2.46 0.393 -0.42 -1.11 -0.413 summary_stats_mean &lt;- simulated_data %&gt;% select(-colonist_id, -seed) %&gt;% summarise(across(everything(), list(mean = mean))) %&gt;% rbind(mean_traits) # compare with population parameters from mean_traits summary_stats_sd &lt;- simulated_data %&gt;% select(-colonist_id, -seed) %&gt;% summarise(across(everything(), list(sd = sd))) %&gt;% rbind(sd_traits) # compare with population parameters from sd_traits # compare with population parameters from mean_traits and sd_traits summary_stats &lt;- cbind(summary_stats_mean, summary_stats_sd) # summary_stats_cor &lt;- simulated_data %&gt;% select(-colonist_id, -seed) %&gt;% cor() # compare with population parameters from cor_matrix_bigfive summary_stats ## EX_mean ES_mean AG_mean CO_mean OP_mean EX_sd ES_sd AG_sd CO_sd OP_sd ## 1 -0.43 0.45 0.081 0.43 -0.053 1 0.88 0.97 0.97 0.85 ## 2 -0.50 0.50 0.250 0.50 0.000 1 0.90 1.00 1.00 1.00 Exercise 4: Preparing for the Unexpected Generate the big five for 100 colonies of 100 colonists, repeating this process multiple times to how much our colony might look if we settled on 100 different planets. We’ve already made the colonists for one planet, so we’ll just need to replicate that process 99 more times. There are two major approaches to take here. You can either use replicate, which is easier, or a for loop, which is more flexible. 4.1. Generate 100 colonies and extract the mean and standard deviation for extraversion. Although you can get the summary statistic for each variable, let’s focus on the mean and standard deviation for extraversion as well as its correlation with openness. Please plot the distribution of the correlation coefficients to see how consistent the relationship between extraversion and openness is across different planets. Consider how this distribution might differ across colonists. How large of a sample size would you need to get a stable estimate of the correlation between extraversion and openness? 4.2. Plot the distributions of those statistics from your new empire of colonies, and include an annotation in the plot to show the population parameters. Click this text for a hint about implementing with replicate You can use the replicate function to repeat the simulation process multiple times. Remember to append the results of each simulation to a single dataset. After running the simulations, you can use the group_by and summarize functions to get the mean and standard deviation for extraversion and its correlation with openness. Click this text for a hint about implementing with a forloop You can use a for loop to repeat the simulation process multiple times. Remember to append the results of each simulation to a single dataset. After running the simulations, you can use the group_by and summarize functions to get the mean and standard deviation for extraversion and its correlation with openness. . Click this text for forloop based solution library(dplyr) conflicts_prefer(dplyr::select()) set.seed(124) num_simulations &lt;- 100 # Number of times to simulate the colonist data all_simulations &lt;- data.frame() # Create an empty data frame to store the results for (i in 1:num_simulations) { # Simulate the big five personality traits simulated_data &lt;- mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix_bigfive) # Add colonist_id and seed simulated_data &lt;- cbind.data.frame(colonist_id = 1:n_colonists, # add colonist_id rep = i, simulated_data) # add simulated data # Append the results all_simulations &lt;- rbind(all_simulations, simulated_data) } summary_stats &lt;- all_simulations %&gt;% group_by(rep) %&gt;% select(-colonist_id) %&gt;% summarise(across(everything(), list(mean = mean, sd = sd))) # compare with population parameters from mean_traits summary_stats_mean &lt;- all_simulations %&gt;% select(-colonist_id) %&gt;% summarise(across(everything(), list(mean = mean))) %&gt;% rbind(mean_traits) # compare with population parameters from mean_traits summary_stats_sd &lt;- all_simulations %&gt;% select(-colonist_id) %&gt;% summarise(across(everything(), list(sd = sd))) %&gt;% rbind(sd_traits) # compare with population parameters from sd_traits # compare with population parameters from mean_traits and sd_traits summary_stats &lt;- cbind(summary_stats_mean, summary_stats_sd) # summary_stats_cor &lt;- simulated_data %&gt;% select(-colonist_id) %&gt;% cor() # compare with population parameters from cor_matrix_bigfive Stretch Tasks (Optional) In space colonization, just like in any complex project management, it’s essential to prepare for variability and uncertainty. To test the resilience of our simulated Mars colony, we’ll generate multiple sets of potential colonists at different sample sizes. By examining these various batches, we can assess how many people we actually need in our colony and how that affects are summary statistics. Click this text for forloop based solution library(dplyr) set.seed(124) sample_sizes &lt;- seq(30, 300, by = 15) # Varying sample sizes repetitions_per_condition &lt;- 20 # Number of repetitions for each sample size mean_skills &lt;- c(50, 50) # Mean technical skills and problem-solving abilities cov_skills &lt;- matrix(c(100, 50, 50, 100), ncol = 2) # Covariance matrix for skills # Initialize a DataFrame to store results simulation_results &lt;- data.frame( Condition = integer(), SampleSize = integer(), Repetition = integer(), Covariance = numeric() ) # Nested loop for simulations for (size in sample_sizes) { for (rep in 1:repetitions_per_condition) { skills_data &lt;- mvrnorm(n = size, mu = mean_skills, Sigma = cov_skills, empirical = FALSE) current_covariance &lt;- cov(skills_data[, 1], skills_data[, 2]) # Append results simulation_results &lt;- rbind(simulation_results, data.frame( SampleSize = size, Repetition = rep, Covariance = current_covariance )) } } num_simulations &lt;- 100 # Number of times to simulate the colonist data all_simulations &lt;- replicate(100, mvrnorm(n = 100, mu = mean_skills, Sigma = cov_skills, empirical = FALSE)) # Initialize a DataFrame to store results set.seed(124) mean_skills &lt;- c(50, 50) # Mean technical skills and problem-solving abilities cov_skills &lt;- matrix(c(100, 50, 50, 100), ncol = 2) # Covariance matrix for skills num_simulations &lt;- 100 # Number of times to simulate the colonist data all_simulations &lt;- replicate(num_simulations, mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix_bigfive, empirical = FALSE), simplify = FALSE) sample_sizes &lt;- seq(30, 300, by = 15) # Varying sample sizes repetitions_per_condition &lt;- 20 # Number of repetitions for each sample size # Initialize a DataFrame to store results simulation_results_cov &lt;- data.frame( Condition = integer(), SampleSize = integer(), Repetition = integer(), Covariance = numeric() ) # Nested loop for simulations for (size in sample_sizes) { for (rep in 1:repetitions_per_condition) { skills_data &lt;- mvrnorm(n = size, mu = mean_skills, Sigma = cov_skills, empirical = FALSE) current_covariance &lt;- cov(skills_data[, 1], skills_data[, 2]) # Append results simulation_results_cov &lt;- rbind(simulation_results_cov, data.frame( SampleSize = size, Repetition = rep, Covariance = current_covariance )) } } # Plotting the average covariance for each sample size average_covariances &lt;- simulation_results_cov %&gt;% group_by(SampleSize) %&gt;% summarize(AverageCovariance = mean(Covariance)) simulation_results_cov$SampleSize_factor &lt;- as.factor(simulation_results_cov$SampleSize) ggplot(simulation_results_cov, aes(x = Covariance, fill = SampleSize_factor)) + # geom_histogram(position = position_dodge()) + geom_density(position = position_dodge(), alpha=.6) + labs(title = &quot;Average Covariance by Sample Size&quot;, ) + scale_fill_viridis_d() + # This adds a nice color gradient based on the &#39;method&#39; theme_minimal() Conclusion Congratulations on completing the Mars Colony Simulation Lab! Today, you’ve applied your skills in R to simulate and analyze potential attributes of Mars colonists, preparing you for complex data analysis tasks in both theoretical and practical scenarios. Through various exercises, you practiced using R to create distributions, generate categorical variables, and explore relationships between simulated traits. Final Checklist Ensure that you have executed all the codes and documented your findings in this lab. Review the simulations you have created and confirm that all data frames and plots are correctly generated and annotated. Reflect on Your Learning What insights have you gained about the potential challenges and needs of a Mars colony? How can the simulation techniques you practiced today be applied to other areas of research or data science? Next Steps Commit all your changes to your project repository. Use a comprehensive commit message that reflects the completion of this lab. Push your changes to ensure everything is updated in your remote repository. I encourage you to explore further with the stretch tasks provided and utilize the resources listed to deepen your understanding of statistical simulations. OSF repo↩︎ Park, H. H., Wiernik, B. M., Oh, I. S., Gonzalez-Mulé, E., Ones, D. S., &amp; Lee, Y. (2020). Meta-analytic five-factor model personality intercorrelations: Eeny, meeny, miney, moe, how, which, why, and where to go. The Journal of applied psychology, 105(12), 1490–1529. https://doi.org/10.1037/apl0000476↩︎ "],["welcome-to-large-language-models.html", "89 Welcome to Large Language Models 89.1 Module Materials 89.2 Estimated Video Length", " 89 Welcome to Large Language Models This module introduces Large Language Models (LLMs) and their applications in data science. We’ll explore basic concepts, use cases, ethical considerations, and hands-on applications of LLMs. Eventually, there will be video lectures. Once those videos exist, please watch the videos and work your way through the notes. The videos will eventually start on the next page. You will eventually be able to find the video playlist for this module [here][pl_llm]. The slides used to make the videos in this module will be able to be found in the slides repo. This module is in active development, as part of an AI teaching workshop sponsored by the Wake Forest University’s Center for Teaching. 89.1 Module Materials Slides from Lectures: Intro to LLMs, LLMs in Data Science, [LLM Ethics][d33_llmethics] Suggested Readings All subchapters of this module, including … R4DS TBD, including [TBD] [TBD] Key Papers: Attention Is All You Need by Vaswani et al., NeurIPS 2017. This paper introduces the Transformer model, a fundamental architecture for modern LLMs. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Devlin et al., NAACL 2019. BERT introduces bidirectional training and significantly improves NLP tasks. Language Models are Few-Shot Learners by Brown et al., NeurIPS 2020. This paper details GPT-3 and its few-shot learning capabilities. Training language models to follow instructions with human feedback by Ouyang et al., NeurIPS 2022. Discusses the integration of human feedback to improve LLM performance and ethical considerations. Activities Exploring LLM Capabilities with OpenAI’s API [TBD] Lab TBD 89.2 Estimated Video Length No of videos : TBD Average length of video : TBD Total length of playlist : TBD "],["lecture-what-are-large-language-models.html", "90 Lecture: What are Large Language Models? 90.1 Data Science and LLMs", " 90 Lecture: What are Large Language Models? You can follow along with the slides here if they do not appear below. 90.1 Data Science and LLMs Although we have covered some of the basic concepts of LLMs in previous lectures, we will review them here. We primarily used them in the context of APIs and debugging. However, I’d be remiss if I didn’t mention them in the context of data science. 90.1.1 What are Large Language Models? Large Language Models (LLMs) are a type of artificial intelligence designed to understand and generate human language. These advanced AI systems are trained on vast amounts of text data. Key characteristics of LLMs include: Massive scale (billions of parameters), self-supervised learning and the ability to generate human-like text. There are versatile applications of LLMs in data science, including text generation, summarization, sentiment analysis, question answering, and more. Some popular LLMs include GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and T5 (Text-to-Text Transfer Transformer). 90.1.2 History of LLMs: Early models include rule-based systems such as ELIZA (1966) and statistical models like IBM’s Watson (2011). Modern models include OpenAI’s LLM ChatGPT (Chat Generative Pre-Trained Transformer; November 2022), Stanford CRFM and MosaicML’s BioMedLM (Biomedical Language Model; December/January 2023), Meta AI’s LLaMA (Large Language Model Meta AI; February 2023), OpenAI’s GPT-4 (Generative Pre-trained Transformer; March 2023), and Google’s LaMDA (Language Model for Dialogue Applications; May 2023). Alan D. Thompson has a really nice timeline of AI timelineof AI and table of models that you might find interesting. 90.1.3 How do LLMs work? The architecture of LLMs is based on the transformer model, which includes key components such as the encoder, decoder, attention mechanisms, and layers. The transformer model is a neural network architecture that uses self-attention mechanisms to process sequential data. Other components include positional encoding, layer normalization, and feed-forward neural networks. The training process involves pre-training on a large corpus of text data, fine-tuning for specific tasks, and prompt engineering for task-specific performance. 90.1.4 Applications in Data Science LLMs can be used for various tasks in data science, including: text generation and summarization, sentiment analysis, named entity recognition (NER), question answering, text classification, and language translation. These applications are used in a wide range of industries, including healthcare, finance, marketing, and customer service. LLMs have the potential to automate and improve many aspects of data science, from data processing to decision-making. We’ve seen some of these applications in our previous lectures, and we’ll continue to explore them in future lectures. But for now, let’s dive into the world of LLMs and explore their capabilities and limitations. It’s also important to note that many of the models we’ve discussed are still in development, and new models are being released regularly. So, stay tuned for the latest updates on LLMs and their applications in data science. Nevertheless, it looks like we’re just scratching the surface of what LLMs can do. However, as I’ve written elsewhere (Garrison &amp; Tyson, 2026), we need to use these tools in a way that doesn’t inhibit your creativity, critical thinking, or development. My advice is to use these tools as a way to enhance your work, not replace it.And that means understanding the limitations of these tools and using them in conjunction with your own expertise and judgment. LLMs can greatly enhance productivity at both the novice and expert level. But in order to leave the novice stage, you need to understand the underlying principles of the models you are using. If you can’t explain how a model works or why it’s making a particular prediction, then you probably shouldn’t be using it. My general rule of thumb is to use these tools to help you think, not to do the thinking for you. "],["lecture-applications-of-large-language-models-in-data-science.html", "91 Lecture: Applications of Large Language Models in Data Science 91.1 Use Cases in Data Science", " 91 Lecture: Applications of Large Language Models in Data Science You can follow along with the slides here if they do not appear below. 91.1 Use Cases in Data Science 91.1.1 R Example: Text Classification (Sentiment Analysis) library(tidytext) #&gt; Warning: package &#39;tidytext&#39; was built under R version 4.4.2 library(dplyr) library(ggplot2) # Sample text data texts &lt;- c( &quot;I love this product! It&#39;s amazing.&quot;, &quot;This is terrible. I hate it.&quot;, &quot;It&#39;s okay, nothing special.&quot;, &quot;Wow, absolutely fantastic experience!&quot;, &quot;Disappointed with the quality.&quot; ) # Create a tibble df &lt;- tibble(text = texts) # show the data df %&gt;% unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) #&gt; Joining with `by = join_by(word)` #&gt; # A tibble: 7 × 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 love positive #&gt; 2 amazing positive #&gt; 3 terrible negative #&gt; 4 hate negative #&gt; 5 wow positive #&gt; 6 fantastic positive #&gt; 7 disappointed negative ## Tokenize and Perform Sentiment Analysis sentiment_scores &lt;- df %&gt;% unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment_score = positive - negative) #&gt; Joining with `by = join_by(word)` sentiment_scores #&gt; # A tibble: 1 × 3 #&gt; negative positive sentiment_score #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3 4 1 ## Visualize Sentiment Analysis Results df %&gt;% unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% ggplot(aes(x = sentiment)) + geom_bar()+ coord_flip() + theme_classic()+ labs(title = &quot;Overall Vibes&quot;, x = &quot;&quot;, y = &quot;Count&quot;) #&gt; Joining with `by = join_by(word)` 91.1.2 Text Generation (Simple Markov Chain) library(tidytext) library(dplyr) library(stringr) # Sample text text &lt;- &quot;The quick brown fox jumps over the lazy dog. The dog barks at the fox. The fox runs away quickly.&quot; # Tokenize and create word pairs word_pairs &lt;- tibble(text = text) %&gt;% unnest_tokens(word, text) %&gt;% mutate(next_word = lead(word)) %&gt;% na.omit() # Create a simple Markov chain markov_chain &lt;- word_pairs %&gt;% group_by(word) %&gt;% summarise(next_words = list(next_word)) # Generate text generate_text &lt;- function(start_word, length = 10) { result &lt;- start_word current_word &lt;- start_word for (i in 1:length) { next_word_options &lt;- markov_chain %&gt;% filter(word == current_word) %&gt;% pull(next_words) %&gt;% unlist() if (length(next_word_options) == 0) break next_word &lt;- sample(next_word_options, 1) result &lt;- c(result, next_word) current_word &lt;- next_word } str_c(result, collapse = &quot; &quot;) } # Generate a sentence generate_text(&quot;the&quot;) #&gt; [1] &quot;the quick brown fox runs away quickly&quot; library(tidytext) library(dplyr) context &lt;- &quot;The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the Tower.&quot; question &lt;- &quot;Who designed the Eiffel Tower?&quot; # Tokenize context and question context_tokens &lt;- tibble(text = context) %&gt;% unnest_tokens(word, text) question_tokens &lt;- tibble(text = question) %&gt;% unnest_tokens(word, text) # Simple word overlap for demonstration matching_words &lt;- intersect(context_tokens$word, question_tokens$word) # Find sentence with most matching words sentences &lt;- tibble(text = context) %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) best_sentence &lt;- sentences %&gt;% mutate(matches = sapply(sentence, function(s) { sum(matching_words %in% unlist(strsplit(s, &quot; &quot;))) })) %&gt;% arrange(desc(matches)) %&gt;% slice(1) print(best_sentence$sentence) #&gt; [1] &quot;the eiffel tower is a wrought-iron lattice tower on the champ de mars in paris, france.&quot; "],["hands-on-with-openais-api.html", "92 Hands-on with OpenAI’s API", " 92 Hands-on with OpenAI’s API #&gt; #&gt; Attaching package: &#39;jsonlite&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; flatten #&gt; Warning in readLines(&quot;admin/secrets/openai_api_key.txt&quot;): incomplete final line #&gt; found on &#39;admin/secrets/openai_api_key.txt&#39; #&gt; $id #&gt; [1] &quot;chatcmpl-B25kzdjFuxC6kqIoY3OnUbCra98Nh&quot; #&gt; #&gt; $object #&gt; [1] &quot;chat.completion&quot; #&gt; #&gt; $created #&gt; [1] 1739838485 #&gt; #&gt; $model #&gt; [1] &quot;gpt-3.5-turbo-0125&quot; #&gt; #&gt; $choices #&gt; index message.role #&gt; 1 0 assistant #&gt; message.content #&gt; 1 1. Identify the problem: Define the research question or problem that needs to be addressed.\\n\\n2. Data collection: Gather relevant data from various sources, ensuring that the data is clean and reliable.\\n\\n3. Data preprocessing: Clean and preprocess the data, handling missing values, outliers, and inconsistencies.\\n\\n4. Exploratory data analysis: Explore the data to understand patterns, relationships, and trends using visualizations and statistical techniques.\\n\\n5. Feature engineering: Create new features or transform existing features to improve model performance.\\n\\n6. Model selection: Choose the appropriate machine learning algorithm or statistical model based on the nature of the problem and the data.\\n\\n7. Model training: Train the selected model using the training data to learn the patterns and relationships within the data.\\n\\n8. Model evaluation: Evaluate the performance of the model using validation data and metrics such as accuracy, precision, recall, and F1 score.\\n\\n9. Model optimization: Fine-tune the model parameters and hyperparameters to improve performance.\\n\\n10. Deployment: Deploy the model in a production environment to make predictions on new data and monitor its performance over time. #&gt; message.refusal logprobs finish_reason #&gt; 1 NA NA stop #&gt; #&gt; $usage #&gt; $usage$prompt_tokens #&gt; [1] 19 #&gt; #&gt; $usage$completion_tokens #&gt; [1] 222 #&gt; #&gt; $usage$total_tokens #&gt; [1] 241 #&gt; #&gt; $usage$prompt_tokens_details #&gt; $usage$prompt_tokens_details$cached_tokens #&gt; [1] 0 #&gt; #&gt; $usage$prompt_tokens_details$audio_tokens #&gt; [1] 0 #&gt; #&gt; #&gt; $usage$completion_tokens_details #&gt; $usage$completion_tokens_details$reasoning_tokens #&gt; [1] 0 #&gt; #&gt; $usage$completion_tokens_details$audio_tokens #&gt; [1] 0 #&gt; #&gt; $usage$completion_tokens_details$accepted_prediction_tokens #&gt; [1] 0 #&gt; #&gt; $usage$completion_tokens_details$rejected_prediction_tokens #&gt; [1] 0 #&gt; #&gt; #&gt; #&gt; $service_tier #&gt; [1] &quot;default&quot; #&gt; #&gt; $system_fingerprint #&gt; NULL "],["lecture-ethical-considerations-of-llms.html", "93 Lecture: Ethical Considerations of LLMs 93.1 Bias and Fairness 93.2 Privacy and Security 93.3 Environmental Impact", " 93 Lecture: Ethical Considerations of LLMs You can follow along with the slides [here][d31_llm] if they do not appear below. 93.1 Bias and Fairness [Content on potential biases in LLMs and mitigation strategies] 93.2 Privacy and Security [Content on data privacy concerns and best practices] 93.3 Environmental Impact [Content on computational resources and environmental implications] "],["rshiny.html", "94 Welcome to interactive web apps 94.1 Module Materials", " 94 Welcome to interactive web apps This module will introduce you to interactive web apps. We’ll focus on rshiny. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 94.1 Module Materials Slides Interactive web apps Videos Curated Videos Readings Rstudio tutorial Written Lecture Notes Lab No Lab! "],["rshiny-overview.html", "95 RShiny Overview", " 95 RShiny Overview You can follow along with the slides here if they do not appear below. "],["practical-advice-from-the-data-professor.html", "96 Practical Advice from the Data Professor 96.1 Web Apps in R: Building your First Web Application in R 96.2 Web Apps in R: Build Interactive Histogram Web Application in R 96.3 Web Apps in R: Building Data-Driven Web Application in R 96.4 Web Apps in R: Building the Machine Learning Web Application in R 96.5 Web Apps in R: Build BMI Calculator web application in R for health monitoring", " 96 Practical Advice from the Data Professor I’ve outsourced the practical videos because, frankly, I’m a novice when it comes to rshiny. I know enough to be dangerous, but not enough to wield that power for teaching. However, my good twitter friend, the Data Professor has that power. You should check out his youtube channel 96.1 Web Apps in R: Building your First Web Application in R 96.2 Web Apps in R: Build Interactive Histogram Web Application in R 96.3 Web Apps in R: Building Data-Driven Web Application in R 96.4 Web Apps in R: Building the Machine Learning Web Application in R 96.5 Web Apps in R: Build BMI Calculator web application in R for health monitoring "],["shiny-overview.html", "97 All the Shiny things 97.1 Building Slides 97.2 Building Shiny apps 97.3 Build the basic UI 97.4 Checkpoint: what our app looks like after implementing the UI 97.5 Ideas to improve our app", " 97 All the Shiny things These notes are adapted from Jenny Bryan and Dean Attali. 97.1 Building Slides See “Building Shiny Apps” slides by Dean Attali. 97.2 Building Shiny apps Shiny is a package from RStudio that can be used to build interactive web pages with R. While that may sound scary because of the words “web pages”, Shiny is geared to R users who have zero experience with web development, and you do not need to know any HTML/CSS/JavaScript. You can do quite a lot with Shiny: think of it as an easy way to make an interactive web page, and that web page can seamlessly interact with R and display R objects (plots, tables, or anything else you do in R). To get a sense of the wide range of things you can do with Shiny, you can visit my Shiny server (https://daattali.com/shiny/), which hosts some of my own Shiny apps. This tutorial is a hands-on activity complement to a set of presentation slides for learning how to build Shiny apps. In this activity, we’ll walk through all the steps of building a Shiny app using a dataset that lets you explore the products available at the BC Liquor Store. The final version of the app, including a few extra features that are left as exercises for the reader, can be seen here: https://daattali.com/shiny/bcl/. Any activity deemed as an exercise throughout this tutorial is not mandatory for building our app, but they are good for getting more practice with Shiny. This tutorial should take approximately an hour to complete. If you want even more practice, another great tutorial is the official Shiny tutorial. RStudio also provides a handy cheatsheet to remember all the little details after you already learned the basics. 97.2.1 Before we begin You’ll need to have the shiny package, so install it. install.packages(&quot;shiny&quot;) To ensure you successfully installed Shiny, try running one of the demo apps. library(shiny) runExample(&quot;01_hello&quot;) If the example app is running, press Escape to close the app, and you are ready to build your first Shiny app! Exercise: Visit https://www.showmeshiny.com/, which is a gallery of user-submitted Shiny apps, and click through some of the showcased apps. Get a feel for the wide range of things you can do with Shiny. 97.2.2 Shiny app basics Every Shiny app is composed of a two parts: a web page that shows the app to the user, and a computer that powers the app. The computer that runs the app can either be your own laptop (such as when you’re running an app from RStudio) or a server somewhere else. You, as the Shiny app developer, need to write these two parts (you’re not going to write a computer, but rather the code that powers the app). In Shiny terminology, they are called UI (user interface) and server. UI is just a web document that the user gets to see, it’s HTML that you write using Shiny’s functions. The UI is responsible for creating the layout of the app and telling Shiny exactly where things go. The server is responsible for the logic of the app; it’s the set of instructions that tell the web page what to show when the user interacts with the page. If you look at the app we will be building (http://daattali.com/shiny/bcl/), the page that you see is built with the UI code. You’ll notice there are some controls that you, as the user, can manipulate. If you adjust the price or choose a country, you’ll notice that the plot and the table get updated. The UI is responsible for creating these controls and telling Shiny where to place the controls and where to place the plot and table, while the server is responsible for creating the actual plot or the data in the table. 97.2.3 Create an empty Shiny app All Shiny apps follow the same template: library(shiny) ui &lt;- fluidPage() server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) This template is by itself a working minimal Shiny app that doesn’t do much. It initializes an empty UI and an empty server, and runs an app using these empty parts. Copy this template into a new file named app.R in a new folder. It is very important that the name of the file is app.R, otherwise it would not be recognized as a Shiny app. It is also very important that you place this app in its own folder, and not in a folder that already has other R scripts or files, unless those other files are used by your app. After saving the file, RStudio should recognize that this is a Shiny app, and you should see the usual Run button at the top change to Run App. Figure 97.1: Shiny run app button If you don’t see the Run App button, it means you either have a very old version of RStudio, don’t have Shiny installed, or didn’t follow the file naming conventions. Click the Run App button, and now your app should run. You won’t see much because it’s an empty app, but you should see that the R Console has some text printed in the form of Listening on http://127.0.0.1:5274 and that a little stop sign appeared at the top of the R Console You’ll also notice that you can’t run any commands in the R Console This is because R is busy - your R session is currently powering a Shiny app and listening for user interaction (which won’t happen because the app has nothing in it yet). Click the stop button to stop the app, or press the Escape key. Figure 97.2: Shiny stop app button You may have noticed that when you click the Run App button, all it’s doing is just running the function shiny::runApp() in the R Console You can run that command instead of clicking the button if you prefer. Exercise: Try running the empty app using the runApp() function instead of using the Run App button. 97.2.3.1 Alternate way to create a Shiny app: separate UI and server files Another way to define a Shiny app is by separating the UI and server code into two files: ui.R and server.R. This is the preferable way to write Shiny apps when the app is complex and involves more code, but in this tutorial we’ll stick to the simple single file. If you want to break up your app into these two files, you simply put all code that is assigned to the ui variable in ui.R and all the code assigned to the server function in server.R. When RStudio sees these two files in the same folder, it will know you’re writing a Shiny app. Exercise: Try making a new Shiny app by creating the two files ui.R and server.R. Remember that they have to be in the same folder. Also remember to put them in a new, isolated folder (not where your app.R already exists). 97.2.3.2 Let RStudio fill out a Shiny app template for you You can also create a new Shiny app using RStudio’s menu by selecting File &gt; New File &gt; Shiny Web App…. If you do this, RStudio will let you choose if you want a single-file app (app.R) or a two-file app (ui.R+server.R). RStudio will initialize a simple functional Shiny app with some code in it. I personally don’t use this feature because I find it easier to simply type the few lines of a Shiny app and save the files. 97.2.4 Load the dataset The dataset we’ll be using contains information about all the products sold by BC Liquor Store and is provided by OpenDataBC. They provide a direct link to download a CSV version of the data, and this data has the rare quality that it is immediately clean and useful. You can view the raw data they provide, but I have taken a few steps to simplify the dataset to make it more useful for our app. I removed some columns, renamed other columns, and dropped a few rare factor levels. The processed dataset we’ll be using in this app is available here. Download it now and place this file in the same folder as your Shiny app. Make sure the file is named bcl-data.csv. Add a line in your app to load the data into a variable called bcl. It should look something like this bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) Place this line in your app as the second line, just after library(shiny). Make sure the file path and file name are correct, otherwise your app won’t run. Try to run the app to make sure the file can be loaded without errors. If you want to verify that the app can successfully read the data, you can add a print() statement after reading the data. This won’t make anything happen in your Shiny app, but you will see a summary of the dataset printed in the R Console, which should let you know that the dataset was indeed loaded correctly. You can place the following line after reading the data: print(str(bcl)) Once you get confirmation that the data is properly loaded, you can remove that line. In case you’re curious, the code Jenny and Dean used to process the raw data into the data we’ll be using is available as a gist. Exercise: Load the data file into R and get a feel for what’s in it. How big is it, what variables are there, what are the normal price ranges, etc. 97.3 Build the basic UI Let’s start populating our app with some elements visually. This is usually the first thing you do when writing a Shiny app - add elements to the UI. 97.3.1 Add plain text to the UI You can place R strings inside fluidPage() to render text. fluidPage(&quot;BC Liquor Store&quot;, &quot;prices&quot;) Replace the line in your app that assigns an empty fluidPage() into ui with the one above, and run the app. The entire UI will be built by passing comma-separated arguments into the fluidPage() function. By passing regular text, the web page will just render boring unformatted text. Exercise: Add several more strings to fluidPage() and run the app. Nothing too exciting is happening yet, but you should just see all the text appear in one contiguous block. 97.3.1.1 Add formatted text and other HTML elements If we want our text to be formatted nicer, Shiny has many functions that are wrappers around HTML tags that format text. We can use the… h1() function for a top-level header (&lt;h1&gt; in HTML) h2() for a secondary header (&lt;h2&gt; in HTML) strong() to make text bold (&lt;strong&gt; in HTML) em() to make text italicized (&lt;em&gt; in HTML) …and many more. There are also functions that are wrappers to other HTML tags, such as br() for a line break, img() for an image, a() for a hyperlink, and others. All of these functions are actually just wrappers to HTML tags with the equivalent name. You can add any arbitrary HTML tag using the tags object, which you can learn more about by reading the help file via ?tags. Just as a demonstration, try replacing the fluidPage() function in your UI with the following: fluidPage( h1(&quot;My app&quot;), &quot;BC&quot;, &quot;Liquor&quot;, br(), &quot;Store&quot;, strong(&quot;prices&quot;) ) Run the app with this code as the UI. Notice the formatting of the text and understand why it is rendered that way. For people who know basic HTML: any named argument you pass to an HTML function becomes an attribute of the HTML element, and any unnamed argument will be a child of the element. That means that you can, for example, create blue text with div(\"this is blue\", style = \"color: blue;\"). Exercise: Experiment with different HTML-wrapper functions inside fluidPage(). Run the fluidPage(...) function in the R Console and see the HTML that it creates. 97.3.1.2 Add a title We could add a title to the app with h1(), but Shiny also has a special function titlePanel(). Using titlePanel() not only adds a visible big title-like text to the top of the page, but it also sets the “official” title of the web page. This means that when you look at the name of the tab in the browser, you’ll see this title. Overwrite the fluidPage() that you experimented with so far, and replace it with the simple one below, that simply has a title and nothing else. fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;) ) Exercise: Look at the documentation for the titlePanel() function and notice it has another argument. Use that argument and see if you can see what it does. 97.3.1.3 Add a layout You may have noticed that so far, by just adding text and HTML tags, everything is unstructured and the elements simply stack up one below the other in one column. We’ll use sidebarLayout() to add a simple structure. It provides a simple two-column layout with a smaller sidebar and a larger main panel. We’ll build our app such that all the inputs that the user can manipulate will be in the sidebar, and the results will be shown in the main panel on the right. Add the following code after the titlePanel() sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) Remember that all the arguments inside fluidPage() need to be separated by commas. So far our complete app looks like this (hopefully this isn’t a surprise to you): library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 97.3: Our Shiny app so far If you want to be a lot more flexible with the design, you can have much more fine control over where things go by using a grid layout. We won’t cover that here, but if you’re interested, look at the documentation for ?column and ?fluidRow. Exercise: Add some UI into each of the two panels (sidebar panel and main panel) and see how your app now has two columns. 97.3.1.4 All UI functions are simply HTML wrappers It’s important to remember: the entire UI is just HTML, and Shiny simply gives you easy tools to write it without having to know HTML. To convince yourself of this reality, look at the output when printing the contents of the ui variable. print(ui) ## &lt;div class=&quot;container-fluid&quot;&gt; ## &lt;h2&gt;BC Liquor Store prices&lt;/h2&gt; ## &lt;div class=&quot;row&quot;&gt; ## &lt;div class=&quot;col-sm-4&quot;&gt; ## &lt;form class=&quot;well&quot;&gt;our inputs will go here&lt;/form&gt; ## &lt;/div&gt; ## &lt;div class=&quot;col-sm-8&quot;&gt;the results will go here&lt;/div&gt; ## &lt;/div&gt; ## &lt;/div&gt; This output should make you appreciate Shiny for not making you write horrendous HTML by hand. 97.3.2 Add inputs to the UI Inputs are what gives users a way to interact with a Shiny app. Shiny provides many input functions to support many kinds of interactions that the user could have with an app. For example, textInput() is used to let the user enter text, numericInput() lets the user select a number, dateInput() is for selecting a date, and selectInput() is for creating a select box (a.k.a. a dropdown menu). Figure 97.4: Shiny inputs All input functions have the same first two arguments: inputId and label. The inputId will be the name that Shiny will use to refer to this input when you want to retrieve its current value. It is important to note that every input must have a unique inputId. If you give more than one input the same inputId, Shiny will unfortunately not give you an explicit error, but your app won’t work correctly. The label argument specifies the text in the display label that goes along with the input widget. Every input can also have multiple other arguments specific to that input type. The only way to find out what arguments you can use with a specific input function is to look at its help file. Exercise: Read the documentation of numericInput (via ?numericInput) and try adding a numeric input to the UI. Experiment with the different arguments. Run the app and see how you can interact with this input. Then try different inputs types. 97.3.2.1 Input for price The first input we want to have is for specifying a price range (minimum and maximum price). The most sensible types of input for this are either numericInput() or sliderInput() since they are both used for selecting numbers. If we use numericInput(), we’d have to use two inputs, one for the minimum value and one for the maximum. Looking at the documentation for sliderInput(), you’ll see that by supplying a vector of length two as the value argument, it can be used to specify a range rather than a single number. This sounds like what we want in this case, so we’ll use sliderInput(). To create a slider input, a maximum value needs to be provided. We could use the maximum price in the dataset, which is $30,250, but I doubt I’d ever buy something that expensive. I think $100 is a more reasonable max price for me, and about 85% of the products in this dataset are below $100, so let’s use that as our max. By looking at the documentation for the slider input function, the following piece of code can be constructed. sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, min = 0, max = 100, value = c(25, 40), pre = &quot;$&quot;) Place the code for the slider input inside sidebarPanel() (replace the text we wrote earlier with this input). Exercise: Run the code of the sliderInput() in the R Console and see what it returns. Change some of the parameters of sliderInput(), and see how that changes the result. It’s important to truly understand that all these functions in the UI are simply a convenient way to write HTML, as is apparent whenever you run these functions on their own. 97.3.2.2 Input for product type Usually when going to the liquor store you know whether you’re looking for beer or wine, and you don’t want to waste your time in the wrong section. The same is true in our app, we should be able to choose what type of product we want. For this we want some kind of a text input. But allowing the user to enter text freely isn’t the right solution because we want to restrict the user to only a few choices. We could either use radio buttons or a select box for our purpose. Let’s use radio buttons for now since there are only a few options, so take a look at the documentation for radioButtons() and come up with a reasonable input function code. It should look like this: radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;) Add this input code inside sidebarPanel(), after the previous input (separate them with a comma). If you look at that input function and think “what if there were 100 types, listing them by hand would not be fun, there’s got to be a better way!”, then you’re right. This is where uiOutput() comes in handy, but we’ll talk about that later. 97.3.2.3 Input for country Sometimes Dean and Jenny like to feel fancy and only look for wines imported from France. We should add one last input, to select a Country. The most appropriate input type in this case is probably the select box. Look at the documentation for selectInput() and create an input function. For now let’s only have \"CANADA\", \"FRANCE\", \"ITALY\" as options, and later we’ll see how to include all countries. selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) Add this function as well to your app. If you followed along, your entire app should have this code: library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 97.5: Adding inputs to our Shiny app 97.3.3 Add placeholders for outputs After creating all the inputs, we should add elements to the UI to display the outputs. Outputs can be any object that R creates and that we want to display in our app - such as a plot, a table, or text. We’re still only building the UI, so at this point we can only add placeholders for the outputs that will determine where an output will be and what its ID is, but it won’t actually show anything. Each output needs to be constructed in the server code later. Shiny provides several output functions, one for each type of output. Similarly to the input functions, all the output functions have a outputId argument that is used to identify each output, and this argument must be unique for each output. 97.3.3.1 Output for a plot of the results At the top of the main panel we’ll have a plot showing some visualization of the results. Since we want a plot, the function we use is plotOutput(). Add the following code into the mainPanel() (replace the existing text): plotOutput(&quot;coolplot&quot;) This will add a placeholder in the UI for a plot named coolplot. Exercise: To remind yourself that we are still merely constructing HTML and not creating actual plots yet, run the above plotOutput() function in the R Console to see that all it does is create some HTML. 97.3.4 Output for a table summary of the results Below the plot, we will have a table that shows all the results. To get a table, we use the tableOutput() function. Here is a simple way to create a UI element that will hold a table output: tableOutput(&quot;results&quot;) Add this output to the mainPanel() as well. Maybe add a couple br() in between the two outputs, just as a space buffer so that they aren’t too close to each other. 97.4 Checkpoint: what our app looks like after implementing the UI If you’ve followed along, your app should now have this code: library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 97.4.1 Implement server logic to create outputs So far we have only written code inside that was assigned to the ui variable (or code that was written in ui.R). That’s usually the easier part of a Shiny app. Now we have to write the server function, which will be responsible for listening to changes to the inputs and creating outputs to show in the app. If you look at the server function, you’ll notice that it is always defined with two arguments: input and output. You must define these two arguments! Both input and output are list-like objects. As the names suggest, input is a list you will read values from and output is a list you will write values to. input will contain the values of all the different inputs at any given time, and output is where you will save output objects (such as tables and plots) to display in your app. 97.4.1.1 Building an output Recall that we created two output placeholders: coolplot (a plot) and results (a table). We need to write code in R that will tell Shiny what kind of plot or table to display. There are three rules to build an output in Shiny: Save the output object into the output list (remember the app template - every server function has an output argument). Build the object with a render* function, where * is the type of output. Access input values using the input list (every server function has an input argument). The third rule is only required if you want your output to depend on some input, so let’s first see how to build a very basic output using only the first two rules. We’ll create a plot and send it to the coolplot output. output$coolplot &lt;- renderPlot({ plot(rnorm(100)) }) This simple code shows the first two rules: we’re creating a plot inside the renderPlot() function, and assigning it to coolplot in the output list. Remember that every output created in the UI must have a unique ID, now we see why. In order to attach an R object to an output with ID x, we assign the R object to output$x. Since coolplot was defined as a plotOutput, we must use the renderPlot function, and we must create a plot inside the renderPlot function. If you add the code above inside the server function, you should see a plot with 100 random points in the app. Exercise: The code inside renderPlot() doesn’t have to be only one line, it can be as long as you’d like as long as it returns a plot. Try making a more complex plot using ggplot2. The plot doesn’t have to use our dataset, it could be anything, just to make sure you can use renderPlot(). 97.4.1.2 Making an output react to an input Now we’ll take the plot one step further. Instead of always plotting the same plot (100 random numbers), let’s use the minimum price selected as the number of points to show. It doesn’t make too much sense, but it’s just to learn how to make an output depend on an input. Replace the previous code in your server function with the code below, and run the app. output$coolplot &lt;- renderPlot({ plot(rnorm(input$priceInput[1])) }) Whenever you choose a new minimum price range, the plot will update with a new number of points. Notice that the only thing different in the code is that instead of using the number 100 we are using input$priceInput[1]. What does this mean? Just like the variable output contains a list of all the outputs (and we need to assign code into them), the variable input contains a list of all the inputs that are defined in the UI. input$priceInput return a vector of length two containing the minimum and maximum price. Whenever the user manipulates the slider in the app, these values are updated, and whatever code relies on it gets re-evaluated. This is a concept known as reactivity, which we will get to soon. Notice that these short three lines of code are using all the three rules for building outputs: We are saving to the output list (output$coolplot &lt;-). We are using a render* function to build the output (renderPlot({})). We are accessing an input value (input$priceInput[1]). 97.4.2 Building the plot output Now we have all the knowledge required to build a plot visualizing some aspect of the data. We’ll create a simple histogram of the alcohol content of the products by using the same three rules to create a plot output. First we need to make sure ggplot2 is attached, so add a library(ggplot2) at the top. Next we’ll return a histogram of alcohol content from renderPlot(). Let’s start with just a histogram of the whole data, unfiltered. output$coolplot &lt;- renderPlot({ ggplot(bcl, aes(Alcohol_Content)) + geom_histogram() }) If you run the app with this code inside your server, you should see a histogram in the app. But if you change the input values, nothing happens yet, so the next step is to actually filter the dataset based on the inputs. Recall that we have 3 inputs: priceInput, typeInput, and countryInput. We can filter the data based on the values of these three inputs. We’ll use dplyr functions to filter the data, so be sure to include dplyr at the top. Then we’ll plot the filtered data instead of the original data. Place this code in your server function and run the app: output$coolplot &lt;- renderPlot({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) ggplot(filtered, aes(Alcohol_Content)) + geom_histogram() }) If you change any input, you should see the histogram update. The way I know the histogram is correct is by noticing that the alcohol content is about 5% when I select beer, 40% for spirits, and 13% for wine. That sounds right. Read this code and understand it. You’ve successfully created an interactive app - the plot is changing according to the user’s selection. To make sure we’re on the same page, here is what your code should look like at this point: library(shiny) library(ggplot2) library(dplyr) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) { output$coolplot &lt;- renderPlot({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) ggplot(filtered, aes(Alcohol_Content)) + geom_histogram() }) } shinyApp(ui = ui, server = server) Figure 43.1: Adding a plot to our Shiny app Exercise: The current plot doesn’t look very nice, you could enhance the plot and make it much more pleasant to look at. 97.4.2.1 Building the table output Building the next output should be much easier now that we’ve done it once. The other output we have was called results (as defined in the UI) and should be a table of all the products that match the filters. Since it’s a table output, we should use the renderTable() function. We’ll do the exact same filtering on the data, and then simply return the data as a data.frame. Shiny will know that it needs to display it as a table because it’s defined as a tableOutput. The code for creating the table output should make sense to you without too much explanation: output$results &lt;- renderTable({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) filtered }) Add this code to your server. Don’t overwrite the previous definition of output$coolplot, just add this code before or after that, but inside the server function. Run your app, and be amazed! You can now see a table showing all the products at the BC Liquor Store that match your criteria. Exercise: Add a new output. Either a new plot, a new table, or some piece of text that changes based on the inputs. For example, you could add a text output (textOutput() in the UI, renderText() in the server) that says how many results were found. If you choose to do this, I recommend first adding the output to the UI, then building the output in the server with static text to make sure you have the syntax correct. Only once you can see the text output in your app you should make it reflect the inputs. Pro-tip: since textOutput() is written in the UI, you can wrap it in other UI functions. For example, h2(textOutput(...)) will result in larger text. 97.4.3 Reactivity 101 Shiny uses a concept called reactive programming. This is what enables your outputs to react to changes in inputs. Reactivity in Shiny is complex, but as an extreme oversimplification, it means that when the value of a variable x changes, then anything that relies on x gets re-evaluated. Notice how this is very different from what you are used to in R. Consider the following code: x &lt;- 5 y &lt;- x + 1 x &lt;- 10 What is the value of y? It’s 6. But in reactive programming, if x and y are reactive variables, then the value of y would be 11 because it would be updated whenever x is changed. This is a very powerful technique that is very useful for creating the responsiveness of Shiny apps, but it might be a bit weird at first because it’s a very different concept from what you’re used to. Only reactive variables behave this way, and in Shiny all inputs are automatically reactive. That’s why you can always use input$x in render functions, and you can be sure that whatever output depends on x will use the updated value of x whenever x changes. You might be wondering what it means to “depend” on a variable. This is not the official terminology, but it simply means that the variable is referenced in the code. So by merely accessing the value of a reactive variable, it causes the current code block to “depend” on that variable. Consider the following sample code to create a plot with a specific number of points in a specific colour: output$someoutput &lt;- renderPlot({ col &lt;- input$mycolour num &lt;- input$mynumber plot(rnorm(num), col = col) }) The above render function accesses two different inputs: input$mycolour and input$mynumber. This means that this code block depends on both of these variables, so whenever either one of the two inputs is updated, the code gets re-executed with the new input values and output$someoutput is updated. 97.4.3.1 Creating and accessing reactive variables One very important thing to remember about reactive variables (such as the input list) is that they can only be used inside reactive contexts. Any render* function is a reactive context, so you can always use input$x or any other reactive variable inside render functions. There are two other common reactive contexts that we’ll get to in a minute: reactive({}) and observe({}). To show you what this means, let’s try accessing the price input value in the server function, without explicitly being inside a reactive context. Simply add print(input$priceInput) inside the server function, and you will get an error when running the app: Operation not allowed without an active reactive context. (You tried to do something that can only be done from inside a reactive expression or observer.) Shiny is very clear about what the error is: we are trying to access a reactive variable outside of a reactive context. To fix this, we can use the observe({}) function to access the input variable. Inside the server, replace print(input$priceInput) with observe({ print(input$priceInput) }), and now the app should run fine. Note that this observe({}) statement depends on input$priceInput, so whenever you change the value of the price, the code inside this observe({}) will run again, and the new value will be printed. This is actually a very simple yet useful debugging technique in Shiny: often you want to know what value a reactive variable holds, so you need to remember to wrap the cat(input$x) or print(input$x) by an observe({}). So far we only saw one reactive variable: the input list. You can also create your own reactive variables using the reactive({}) function. The reactive({}) function is similar to observe({}) in that it is also a reactive context, which means that it will get re-run whenever any of the reactive variables in it get updated. The difference between them is that reactive({}) returns a value. To see it in action, let’s create a variable called priceDiff that will be the difference between the maximum and minimum price selected. If you try to naively define priceDiff &lt;- diff(input$priceInput), you’ll see the same error as before about doing something outside a reactive context. This is because input$priceInput is a reactive variable, and we can’t use a reactive variable outside a reactive context. Since we want to assign a value, we use the reactive({}) function. Try adding the following line to your server: priceDiff &lt;- reactive({ diff(input$priceInput) }) Now your app will run. If you want to access a reactive variable defined with reactive({}), you must add parentheses after the variable name, as if it’s a function. To demonstrate this, add observe({ print(priceDiff()) }) to your server function. Notice that we use priceDiff() rather than priceDiff. It’s very important to remember this, because you can get confusing unclear errors if you simply try to access a custom reactive variable without the parentheses. You can think of reactivity as causing a chain reaction: when one reactive value changes, anything that depends on it will get updated. If any of the updated values are themselves reactive variables, then any reactive contexts that depend on those variables will also get updated in turn. As a concrete example, let’s think about what happens when you change the value of the priceInput on the page. Since input$priceInput is a reactive variable, any expression that uses it will get updated. This means the two render functions from earlier will execute because they both depend on input$priceInput, as well as the priceDiff variable because it also depends on it. But since priceDiff is itself a reactive variable, Shiny will check if there is anything that depends on priceDiff, and indeed there is - the observe({}) function that prints the value of priceDiff. So once priceDiff gets updated, the observe({}) function will run, and the value will get printed. Reactivity is usually the hardest part about Shiny to understand, so if you don’t quite get it, don’t feel bad. Try reading this section again, and I promise that with time and experience you will get more comfortable with reactivity. Once you do feel more confident with reactivity, it may be a good idea to read more advanced documentation describing reactivity, since this section greatly simplifies ideas to make them more understandable. A great resource is RStudio’s tutorial on reactivity. Before continuing to the next section, you can remove all the observe({}) and reactive({}) functions we wrote in this section since they were all just for learning purposes. Exercise: Read this section again and really understand what a reactive variable means, what the three main reactive contexts are, how you can define reactive variables, and how a reactivity chain of events works. 97.4.3.2 Using reactive variables to reduce code duplication You may have noticed that we have the exact same code filtering the dataset in two places, once in each render function. We can solve that problem by defining a reactive variable that will hold the filtered dataset, and use that variable in the render functions. The first step would be to create the reactive variable. The following code should be added to the server function. filtered &lt;- reactive({ bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) The variable filtered is being defined exactly like before, except the body is wrapped by a reactive({}), and it’s defined in the server function instead of inside the individual render functions. Now that we have our reactive variable, we can use it in the output render functions. Try it yourself, and when you think you’re done, check the code below. Don’t forget that in order to access the value of a reactive expression, you must follow the name of the variable with parentheses! This is how your server function should look now: server &lt;- function(input, output) { filtered &lt;- reactive({ bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) output$coolplot &lt;- renderPlot({ ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) output$results &lt;- renderTable({ filtered() }) } As a reminder, Shiny creates a dependency tree with all the reactive expressions to know what value depends on what other value. For example, when the price input changes, Shiny looks at what values depend on price, and sees that filtered is a reactive expression that depends on the price input, so it re-evaluates filtered. Then, because filtered is changed, Shiny now looks to see what expressions depend on filtered, and it finds that the two render functions use filtered. So Shiny re-executes the two render functions as well. 97.4.4 Using uiOutput() to create UI elements dynamically One of the output functions you can add in the UI is uiOutput(). According to the naming convention (e.g. plotOutput() is an output to render a plot), this is an output used to render more UI. This may sound a bit confusing, but it’s actually very useful. It’s usually used to create inputs (or any other UI) from the server, or in other words - you can create inputs dynamically. Any input that you normally create in the UI is created when the app starts, and it cannot be changed. But what if one of your inputs depends on another input? In that case, you want to be able to create an input dynamically, in the server, and you would use uiOutput(). uiOutput() can be used to create any UI element, but it’s most often used to create input UI elements. The same rules regarding building outputs apply, which means the output (which is a UI element in this case) is created with the function renderUI(). 97.4.4.1 Basic example of uiOutput() As a very basic example, consider this app: library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Maximum slider value&quot;, 5), uiOutput(&quot;slider&quot;) ) server &lt;- function(input, output) { output$slider &lt;- renderUI({ sliderInput(&quot;slider&quot;, &quot;Slider&quot;, min = 0, max = input$num, value = 0) }) } shinyApp(ui = ui, server = server) If you run that tiny app, you will see that whenever you change the value of the numeric input, the slider input is re-generated. This behavior can come in handy often. 97.4.5 Use uiOutput() in our app to populate the countries We can use this concept in our app to populate the choices for the country selector. The country selector currently only holds 3 values that we manually entered, but instead we could render the country selector in the server and use the data to determine what countries it can have. First we need to replace the selectInput(\"countryInput\", ...) in the UI with: uiOutput(&quot;countryOutput&quot;) Then we need to create the output (which will create a UI element - yeah, it can be a bit confusing at first), so add the following code to the server function: output$countryOutput &lt;- renderUI({ selectInput(&quot;countryInput&quot;, &quot;Country&quot;, sort(unique(bcl$Country)), selected = &quot;CANADA&quot;) }) Now if you run the app, you should be able to see all the countries that BC Liquor stores import from. 97.4.5.1 Errors showing up and quickly disappearing You might notice that when you first run the app, each of the two outputs are throwing an error message, but the error message goes away after a second. The problem is that when the app initializes, filtered is trying to access the country input, but the country input hasn’t been created yet. After Shiny finishes loading fully and the country input is generated, filtered tries accessing it again, this time it’s successful, and the error goes away. Once we understand why the error is happening, fixing it is simple. Inside the filtered reactive function, we should check if the country input exists, and if not then just return NULL. filtered &lt;- reactive({ if (is.null(input$countryInput)) { return(NULL) } bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) Now when the render function tries to access the data, it will get a NULL value before the app is fully loaded. You will still get an error, because the ggplot() function will not work with a NULL dataset, so we also need to make a similar check in the renderPlot() function. Only once the data is loaded, we can try to plot. output$coolplot &lt;- renderPlot({ if (is.null(filtered())) { return() } ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) The renderTable() function doesn’t need this fix applied because Shiny doesn’t have a problem rendering a NULL table. Exercise: Change the product type radio buttons to get generated in the server with the values from the dataset, instead of being created in the UI with the values entered manually. If you’re feeling confident, try adding an input for “subtype” that will get re-generated every time a new type is chosen, and will be populated with all the subtype options available for the currently selected type (for example, if \"WINE\" is selected, then the subtypes are white wine, red wine, etc.). 97.4.6 Final Shiny app code In case you got lost somewhere, here is the final code. The app is now functional, but there are plenty of features you can add to make it better. library(shiny) library(ggplot2) library(dplyr) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), uiOutput(&quot;countryOutput&quot;) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) { output$countryOutput &lt;- renderUI({ selectInput(&quot;countryInput&quot;, &quot;Country&quot;, sort(unique(bcl$Country)), selected = &quot;CANADA&quot;) }) filtered &lt;- reactive({ if (is.null(input$countryInput)) { return(NULL) } bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) output$coolplot &lt;- renderPlot({ if (is.null(filtered())) { return() } ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) output$results &lt;- renderTable({ filtered() }) } shinyApp(ui = ui, server = server) 97.4.7 Share your app with the world Remember how every single app is a web page powered by an R session on a computer? So far, you’ve been running Shiny locally, which means your computer was used to power the app. It also means that the app was not accessible to anyone on the internet. If you want to share your app with the world, you need to host it somewhere. 97.4.7.1 Host on shinyapps.io RStudio provides a service called shinyapps.io which lets you host your apps for free. It is integrated seamlessly into RStudio so that you can publish your apps with the click of a button, and it has a free version. The free version allows a certain number of apps per user and a certain number of activity on each app, but it should be good enough for most users. It also lets you see some basic stats about the usage of your app. Hosting your app on shinyapps.io is the easy and recommended way of getting your app online. Go to www.shinyapps.io and sign up for an account. When you’re ready to publish your app, click on the “Publish Application” button in RStudio and follow the instructions. You might be asked to install a couple of packages if it’s your first time. Figure 97.6: Shiny publish application button After a successful deployment to [shinyapps.io], you will be redirected to your app in the browser. You can use that URL to show off to your family what a cool app you wrote. 97.4.7.2 Host on a Shiny Server The other option for hosting your app is on your own private Shiny Server. Shiny Server is also a product by RStudio that lets you host apps on your own server. This means that instead of RStudio hosting the app for you, you have it on your own private server. This gives you a lot more freedom and flexibility, but it also means you need to have a server and be comfortable administering a server. Some people, such as Dean Attali, host all their apps on their own Shiny Server just because they like having the extra control, but when they first learned about Shiny they used [shinyapps.io] for several months. If you’re feeling adventurous and want to host your own server, you can follow this tutorial for hosting a Shiny Server. 97.4.8 More Shiny features to check out Shiny is extremely powerful and has lots of features that we haven’t covered. Here’s a sneak peek of just a few other common Shiny features that are not too advanced. 97.4.8.1 Shiny in R Markdown You can include Shiny inputs and outputs in an R Markdown document! This means that your R Markdown document can be interactive. Learn more here. Here’s a simple example of how to include interactive Shiny elements in an R Markdown: --- output: html_document runtime: shiny --- ```{r echo=FALSE} sliderInput( &quot;num&quot;, &quot;Choose a number&quot;, 0, 100, 20 ) renderPlot({ plot(seq(input$num)) }) ``` 97.4.8.2 Use conditionalPanel() to conditionally show UI elements You can use conditionalPanel() to either show or hide a UI element based on a simple condition, such as the value of another input. Learn more via ?conditionalPanel. library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Number&quot;, 5, 1, 10), conditionalPanel( &quot;input.num &gt;=5&quot;, &quot;Hello!&quot; ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 97.4.8.3 Use navbarPage() or tabsetPanel() to have multiple tabs in the UI If your apps requires more than a single “view”, you can have separate tabs. Learn more via ?navbarPage or ?tabsetPanel. library(shiny) ui &lt;- fluidPage( tabsetPanel( tabPanel(&quot;Tab 1&quot;, &quot;Hello&quot;), tabPanel(&quot;Tab 2&quot;, &quot;there!&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 97.4.8.4 Use DT for beautiful, interactive tables Whenever you use tableOutput() + renderTable(), the table that Shiny creates is a static and boring-looking table. If you download the DT package, you can replace the default table with a much sleeker table by just using DT::dataTableOutput() + DT::renderDataTable(). It’s worth trying. Learn more on DT’s website. 97.4.8.5 Use isolate() function to remove a dependency on a reactive variable When you have multiple reactive variables inside a reactive context, the whole code block will get re-executed whenever any of the reactive variables change because all the variables become dependencies of the code. If you want to suppress this behavior and cause a reactive variable to not be a dependency, you can wrap the code that uses that variable inside the isolate() function. Any reactive variables that are inside isolate() will not result in the code re-executing when their value is changed. Read more about this behavior via ?isolate. 97.4.8.6 Use update*Input() functions to update input values programmatically Any input function has an equivalent update*Input function that can be used to update any of its parameters. library(shiny) ui &lt;- fluidPage( sliderInput(&quot;slider&quot;, &quot;Move me&quot;, value = 5, 1, 10), numericInput(&quot;num&quot;, &quot;Number&quot;, value = 5, 1, 10) ) server &lt;- function(input, output, session) { observe({ updateNumericInput(session, &quot;num&quot;, value = input$slider) }) } shinyApp(ui = ui, server = server) Note that we used an additional argument session when defining the server function. While the input and output arguments are mandatory, the session argument is optional. You need to define the session argument when you want to use functions that need to access the session. The session parameter actually has some useful information in it, you can learn more about this via ?shiny::session. 97.4.9 Scoping rules in Shiny apps Scoping is very important to understand in Shiny once you want to support more than one user at a time. Since your app can be hosted online, multiple users can use your app simultaneously. If there are any variables (such as datasets or global parameters) that should be shared by all users, then you can safely define them globally. But any variable that should be specific to each user’s session should be not be defined globally. You can think of the server function as a sandbox for each user. Any code outside of the server function is run once and is shared by all the instances of your Shiny app. Any code inside the server is run once for every user that visits your app. This means that any user-specific variables should be defined inside server. If you look at the code in our BC Liquor Store app, you’ll see that we followed this rule: the raw dataset was loaded outside the server and is therefore available to all users, but the filtered object is constructed inside the server so that every user has their own version of it. If filtered was a global variable, then when one user changes the values in your app, all other users connected to your app would see the change happen. You can learn more about the scoping rules in Shiny here. 97.4.9.1 Use global.R to define objects available to both ui.R and server.R If there are objects that you want to have available to both ui.R and server.R, you can place them in global.R. You can learn more about global.R and other scoping rules here. 97.4.10 Add images You can add an image to your Shiny app by placing an image under the www/ folder and using the UI function img(src = \"image.png\"). Shiny will know to automatically look in the www/ folder for the image. 97.4.10.1 Add JavaScript/CSS If you know JavaScript or CSS you are more than welcome to use some in your app. library(shiny) ui &lt;- fluidPage( tags$head(tags$script(&quot;alert(&#39;Hello!&#39;);&quot;)), tags$head(tags$style(&quot;body{ color: blue; }&quot;)), &quot;Hello&quot; ) server &lt;- function(input, output) { } shinyApp(ui = ui, server = server) If you do want to add some JavaScript or use common JavaScript functions in your apps, you might want to check out shinyjs. 97.5 Ideas to improve our app The app we developed is functional, but there are plenty of improvements that can be made. You can compare the app we developed to Dean’s version of this app to get an idea of what a (slightly) more functional app could include. Here are some suggestions of varying difficulties. Each idea also has a hint, Dean recommends only reading the hint if you’re stuck for 10 minutes. Split the app into two separate files: ui.R and server.R. Hint: All the code assigned into the ui variable goes into ui.R and all the code for the server function goes into server.R. You do not need to explicitly call the shinyApp() function. Add an option to sort the results table by price. Hint: Use checkboxInput() to get TRUE/FALSE values from the user. Add an image of the BC Liquor Store to the UI. Hint: Place the image in a folder named www, and use img(src = \"imagename.png\") to add the image. Share your app with everyone on the internet by deploying to shinyapps.io. Hint: Go to shinyapps.io, register for an account, then click the “Publish App” button in RStudio. Use the DT package to turn the current results table into an interactive table. Hint: Install the DT package, replace tableOutput() with DT::dataTableOutput() and replace renderTable() with DT::renderDataTable(). Add parameters to the plot. Hint: You will need to add input functions that will be used as parameters for the plot. You could use shinyjs::colourInput() to let the user decide on the colours of the bars in the plot. The app currently behaves strangely when the user selects filters that return 0 results. For example, try searching for wines from Belgium. There will be an empty plot and empty table generated, and there will be a warning message in the R Console. Try to figure out why this warning message is appearing, and how to fix it. Hint: The problem happens because renderPlot() and renderTable() are trying to render an empty data frame. To fix this issue, the filtered reactive expression should check for the number of rows in the filtered data, and if that number is 0 then return NULL instead of a 0-row data frame. Place the plot and the table in separate tabs. Hint: Use tabsetPanel() to create an interface with multiple tabs. If you know CSS, add CSS to make your app look nicer. Hint: Add a CSS file under www and use the function includeCSS() to use it in your app. Experiment with packages that add extra features to Shiny, such as shinyjs, leaflet, shinydashboard, shinythemes, ggvis. Hint: Each package is unique and has a different purpose, so you need to read the documentation of each package in order to know what it provides and how to use it. Show the number of results found whenever the filters change. For example, when searching for Italian wines $20-$40, the app would show the text “We found 122 options for you”. Hint: Add a textOutput() to the UI, and in its corresponding renderText() use the number of rows in the filtered() object. Allow the user to download the results table as a .csv file. Hint: Look into the downloadButton() and downloadHandler() functions. When the user wants to see only wines, show a new input that allows the user to filter by sweetness level. Only show this input if wines are selected. Hint: Create a new input function for the sweetness level, and use it in the server code that filters the data. Use conditionalPanel() to conditionally show this new input. The condition argument of conditionalPanel should be something like input.typeInput == \"WINE\". Allow the user to search for multiple alcohol types simultaneously, instead of being able to choose only wines/beers/etc. Hint: There are two approaches to do this. Either change the typeInput radio buttons into checkboxes (checkboxGroupInput()) since checkboxes support choosing multiple items, or change typeInput into a select box (selectInput()) with the argument multiple = TRUE to support choosing multiple options. If you look at the dataset, you’ll see that each product has a “type” (beer, wine, spirit, or refreshment) and also a “subtype” (red wine, rum, cider, etc.). Add an input for “subtype” that will let the user filter for only a specific subtype of products. Since each type has different subtype options, the choices for subtype should get re-generated every time a new type is chosen. For example, if “wine” is selected, then the subtypes available should be white wine, red wine, etc. Hint: Use uiOutput() to create this input in the server code. Provide a way for the user to show results from all countries (instead of forcing a filter by only one specific country). Hint: There are two ways to approach this. You can either add a value of “All” to the dropdown list of country options, you can include a checkbox for “Filter by country” and only show the dropdown. "],["shiny-resources.html", "98 Shiny Resources 98.1 Awesome add-on packages to Shiny", " 98 Shiny Resources Shiny is a very popular package and has lots of resources on the web. Here’s a compiled list of a few resources I recommend, which are all fairly easy to read and understand. Shiny official website Shiny official tutorial Shiny cheatsheet Lots of short useful articles about different topics in Shiny - highly recommended Shiny in R Markdown Get help from the Shiny Google group or StackOverflow Publish your apps for free with shinyapps.io Host your app on your own Shiny server Learn about how reactivity works Learn about useful debugging techniques 98.1 Awesome add-on packages to Shiny Many people have written packages that enhance Shiny in some way or add extra functionality. Here is a list of several popular packages that people often use together with Shiny: shinythemes - Easily alter the appearance of your app (cran). shinyjs - Enhance user experience in Shiny apps using JavaScript functions without knowing JavaScript (cran; GitHub). leaflet - Add interactive maps to your apps (cran; GitHub). ggvis - Similar to ggplot2, but the plots are focused on being web-based and more interactive (cran). shinydashboard - Gives you tools to create visual “dashboards” (cran; GitHub). "],["special-topics-reproducible-reports.html", "99 Special Topics: Reproducible reports 99.1 Module Materials", " 99 Special Topics: Reproducible reports This bonus module is designed to introduce reproducible documents. Please watch the curated videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module [here][pl_16]. Most of the slides used to make the videos in this module can be found in the slides repo. 99.1 Module Materials Slides from Lectures [TBD][TBD] Suggested Readings All subchapters of this module, including TOPIC R4DS Activities Activities Lab LAB –&gt; "],["efficient-workflow-with-r-projects-and-r-markdown.html", "100 Efficient Workflow with R Projects and R Markdown 100.1 Overview 100.2 R Projects: Your Workspace Anchor 100.3 Navigate Between Projects 100.4 Recommended Workflow 100.5 Your Turn 100.6 Rmd Creation 100.7 Compile the Document 100.8 Document Types 100.9 Your Turn", " 100 Efficient Workflow with R Projects and R Markdown 100.1 Overview Are you ready to make your data analysis smoother and more efficient? R Projects and R Markdown provide a potent combination that streamlines data analysis and reporting in R. Combining R Projects with R Markdown is like having a Swiss Army knife for your R environment—it integrates perfectly with version control systems like Git and keeps your file paths consistent, no matter where you move your projects. This lesson will show you how to set up an R Project, organize your files, and create an R Markdown document to streamline your data analysis workflow. This section takes heavy inspiration from Zachary M. Smith. 100.2 R Projects: Your Workspace Anchor R Projects establish a self-contained environment for any data analysis project. They help in organizing your work and ensuring that file paths are consistent, making your work portable and easier to share with collaborators. 100.2.1 Create a New R Project Start New Project: In RStudio, create a new R project by clicking on the drop-down menu in top right of your window and selecting “New Project” Select “New Directory” within the “Create Project” window Select “New Project” within the “Project Type” window Enter a project name (below I have given the name “new_project”), the project directory (where the project should live), and select “Create Project” Tip: Create a “project” folder that will act as your parent directory for all R projects. This will make it much easier to navigate to and between projects. A new session specific to your R project will start within RStudio There are a number of ways to tell which project is open… 100.3 Navigate Between Projects Quickly navigate between recently opened R projects by clicking on the drop-down menu at the top right of RStudio and selecting the project of interest. 100.4 Recommended Workflow Set up a GitHub repository. Create an R-project connected to the GitHub repository. Develop R-scripts. Push and pull project changes to and from GitHub. 100.5 Your Turn Open R Studio. Create a new Project. Add an “data” folder and add the example data set to this folder. Create a new R Script (Ctrl + Shift + N) and save the script as “lesson_1.R” to your project root folder. Add the following script and run it (Ctrl + Enter) Notice that the entire file path to CSV of interest (“zms_thesis_metrics.csv”) was not specified just file.path(\"data\", \"zms_thesis_metrics.csv\"). df_thesis &lt;- read.csv( file.path( &quot;data&quot;, &quot;zms_thesis_metrics.csv&quot; ), stringsAsFactors = FALSE ) head(df_thesis) View df_thesis in the Environment panel How many rows and columns does our data frame have? Click on the drop-down menu (the blue circle to the left of df_thesis). Can you identify the column type (e.g., Character, Numeric, Integer, or Factor) of the columns unique_id, substrate_size_d50, richness, and pct_diptera? What are the first, second, and third unique_id’s represented in the data frame and which richness values are associated with each unique_id? Click on the button to view the data (the small girdded table to the far-right of df_thesis) Filter the data frame to only see data from Onondaga Lake (lake = “onon”) 100.6 Rmd Creation 100.6.1 Create a New Document Click on the new document button: Click on R Markdown: Provide a “Title:”, select the “Default Output Format:”, and click “OK” A new R Markdown document will appear with some instructions and example text/code. 100.7 Compile the Document To view the html document, you must compile the document using Knit. The easiest way to knit a document is to navigate to and click on the Knit button (it looks like a ball of yarn) in the toolbar above the editor window. If a window appears saying “Install Required Packages” for R Markdown, install the necessary packages for knitting the document. The compiled file will be saved in the same directory as your Rmd file (your R Markdown file). I generally store the R Markdown file(s) in a sub-directory labeled “rmarkdown” within the R-project folder (rproject/markdown), which prevents the project directory from becoming cluttered– this is NOT necessary. You can click on the small triangle to the right of the Knit button to view a drop-down menu of knitting options. Here you have the option to update the document type you want to knit (examples listed below). Knit to HTML Knit to PDF Knit to Word By clicking one of these options your YAML header updates automatically. We will discuss YAML headers in a later section, but in short the YAML header defines some of the formatting options associated with the document, including the output type. You can have multiple document types render at once (e.g., a separate HTML and Word document are created with each knit call). 100.8 Document Types There are multiple document types available which designate how the .Rmd file will be rendered (knit). 100.8.1 HTML Knitting to a Hyper Text Markup Language (HTML) document is the default for R Markdown. Using HTML will enable you to add interactive features to your document. To view HTML documents, simply open them in your internet browser of choice– by default, your machine should open HTML documents in your default browser automatically. 100.8.1.1 R Markdown vs. R Notebook R Markdown documents (html_documents) and R Notebook documents (html_notebook) are very similar; in fact, an R Notebook document is a special type of R Markdown document. The main difference is using R Markdown document (html_documents) you have to knit (render) the entire document each time you want to preview the document, even if you have made a minor change. However, using an R Notebook document (html_notebook) you can view a preview of the final document without rendering the entire document. For all document types you can specify how you want to preview the rendered documents by clicking on the cog in the Source window toolbar to reveal a drop-down menu of options. The recommended setup for use of an R Notebook document would be to specify you want to view the preview in the Viewer pane (i.e., the bottom right pane shared with Files, Plots, Packages, and Help). Then click on the “Preview” button in the Source window toolbar to render a preview of the final document. Keep the Viewer pane open and do not close the preview of the document. Now begin to edit the markdown syntax in the document. Each time you save the document the preview window will render the changes made to the markdown syntax. If you edit R code chunks, you must run the code chunks within the notebook file prior to saving to see the changes in the Viewer window. 100.8.2 PDF You have the ability to knit to Portable Document Format (PDF) but you will not be able include interactive features in the document. Knitting to a PDF requires you to have the program LaTeX installed. You will get the following error message if you try to knit a PDF document without LaTeX installed on your machine: &gt; No LaTeX installation detected (LaTeX is required to create PDF output). You should install a LaTeX distribution for your platform: https://www.latex-project.org/get/ If you are not sure, you may install TinyTeX in R: tinytex::install_tinytex() Otherwise consider MiKTeX on Windows - http://miktex.org MacTeX on macOS - https://tug.org/mactex/ (NOTE: Download with Safari rather than Chrome strongly recommended) Linux: Use system package manager 100.8.3 Word You have the ability to knit to Microsoft Word if you have Microsoft Word installed on your machine. Similarly to PDFs, you will not be able to include interactive features in your Word documents. 100.8.4 Templates 100.8.4.1 Provide Templates There are a number of templates you can install or will come with packages you install, which you can view in the pop-up window that appears when you select that you want to create a new R Markdown file. These templates may give you a jump-start to provide a large portion of the formatting specification to make specific documents. Some example templates from the package pagedown are: Business Card Letter Paged HTML Documents Resume Journal Article Poster Thesis Package Vignette 100.8.4.2 Custom Templates You also have the ability to specify custom templates in your YAML header. We will discuss YAML headers in a later section (Lesson 4: YAML Headers). I have only used this feature for Microsoft Word documents. I created a new document within Microsoft Word and selected all of the formatting options I wanted for my document. I then added the template to the YAML header of my R Markdown document to specify that when the document is knit (rendered) to apply the formatting specifications found in the template Microsoft Word document. 100.8.5 Other Types 100.8.5.1 Presentations You have the ability to create presentation slides using R Markdown. Below are the different format options available. Examples can be found in the following link under the “Presentation” section: https://rmarkdown.rstudio.com/gallery.html ioslides (HTML) Slidy (HTML) Beamer (PDF) Requires LaTeX Microsoft PowerPoint Requires Microsoft PowerPoint I have tried to create presentations with R Markdown on several occasions but I always revert back to using Microsoft PowerPoint. I have not found these slides to be very flexible and seem to only be able to generate simple slides without much content. My recommendation would be to try these slides out if you are making a presentation demonstrating how to use simple R code. 100.8.5.2 Shiny Documents Shiny documents enable you to embed a shiny application into an R Markdown document. This will enable you to insert custom interactive features into your document. The final document will need to be an HTML file. 100.9 Your Turn Create a new HTML .Rmd file. Save the document to your project root folder. Knit the default document produced by RStudio. Using the Knit drop-down menu, specify that you want to create a Word document. This will only be possible if you have Microsoft Word installed on your machine. Bonus: Try creating new R Markdown documents and selecting Presentation, Shiny, and other templates available. Each selection will provide you with an example .Rmd file that you can knit (render) and explore. "],["basic-syntax.html", "101 Basic Syntax 101.1 Heading Text 101.2 Plain Text 101.3 Bold and Italicized Text 101.4 Lists 101.5 Link to a Section 101.6 Hyperlink 101.7 Insert Images 101.8 Tabbed Sections 101.9 Your Turn 101.10 Lesson 4: YAML Headers 101.11 Table of Contents (TOC) 101.12 Themes 101.13 Code Folding 101.14 output 101.15 Custom Template 101.16 ymlthis 101.17 Your Turn 101.18 Lesson 5: Code Chunks and Inline Code 101.19 Inline Code 101.20 Your Turn (Part 1) 101.21 Interactive Features 101.22 Your Turn (Part 2)", " 101 Basic Syntax Source: https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf 101.1 Heading Text Heading text follows one or more hash-sign(s) (#). The number of hash-signs determines the hierarchy of headings. For example, “# Heading 1” would represent the primary heading, “## Heading 2” would represent the secondary heading, “### Heading 3” would represent the tertiary heading, and so forth. 101.2 Plain Text Simply add text below the YAML header. To start a new paragraph, you will need to end your plain text line with two (2) spaces. 101.3 Bold and Italicized Text There are two ways to format your text to be bold or italicized. Bold Surround your text with two (2) asterisks (*) on each side. Surround your text with two (2) underscores (_) on each side. Italicized Surround your text with one (1) asterisk (*) on each side. Surround your text with one (1) underscore (_) on each side. The markdown syntax… which renders to… Bold Text Example Bold Text Example Italicize Text Example Italicize Text Example 101.4 Lists 101.4.1 Unordered Lists Unordered or bulleted lists can be generated by using a single asterisk (*) followed by a space and the plain text intended to represent the bullet. To add another bullet, just hit enter or return and follow the previous steps. To add sub-bullets, insert two spaces and a plus sign (+) followed by a space and the plain text intended to represent the sub-bullet. You can do this at multiple levels; meaning you can add four spaces and a plus sign (+) to insert a sub-sub-bullet and six spaces and a plus sign (+) to add a sub-sub-sub-bullet. The markdown syntax… which renders to… Bullet 1 Bullet 2 Sub-Bullet 1 Sub-Bullet 2 Sub-Sub-Bullet 1 Sub-Sub-Bullet 2 101.4.2 Ordered Lists Follow the steps in Unordered Lists but instead of using asterisk (*) use sequential numbers followed by a period (.)– this cannot be done for sub-bullets. For example, “1.”, “2.”, and “3.”. The markdown syntax… which renders to… Bullet 1 Sub-Bullet 1 Bullet 2 Sub-Bullet 1 Sub-Bullet 2 Sub-Sub-Bullet 1 101.5 Link to a Section You can link to a section within the document by surrounding the name of the section in square brackets. The markdown syntax… [Lesson 3: Basic Syntax] would render to this… [Lesson 3: Basic Syntax] If you click on the “Lesson 3: Basic Syntax” above, it will jump you to the head of this section. 101.6 Hyperlink You simply need to paste in a full URL into your document and it will automatically be recognized as a hyperlink. For example… https://bookdown.org/yihui/rmarkdown If you want to hide the full link and represent it with text, then surround the text with square brackets followed by the URL in parenthesis. The markdown syntax… [link](https://bookdown.org/yihui/rmarkdown/) would render to this… link. 101.7 Insert Images You can insert images into your document using syntax similar to the hyperlink syntax above. In the code below, you can see the file path the to the image. This image will render without a caption ![](img/hex_symbols/rmarkdown.png) You can add a caption for the image in between the square brackets. ![This is my caption for the image.](img/hex_symbols/rmarkdown.png) This is my caption for the image. You can control the size of the image like this… ![This is my caption for the image.](img/hex_symbols/rmarkdown.png){width=200px} This is my caption for the image. You can center the image with HTML like this… This is my caption for the image. 101.8 Tabbed Sections Tabbed sections can be added to section of an HTML documents that will fold all of the subsequent sub-sections into separate tabs. To do this you specify a section header followed by {.tabset}. For example, ## My Section Header {.tabset}. Note this feature is not available for use with bookdown. 101.9 Your Turn Using the .Rmd file created in [Lesson 2: Rmd Creation], delete everything below the YAML header. Add the following level-1 headers and knit the document. Introduction Workflow Import Data Preprocessing Study Area Plot Conclusions Add two level-2 headers below the Plot header and knit the document. Scatter Plot LOESS Plot Add plain text below the Introduction header and knit the document. Be sure to include some bold and italicized text. Explain the purpose of the document (e.g., “This document was created during the 2020 NABs R Markdown workshop and will serve as an example workflow for my future use of R Markdown.”) Include the hyperlink to R Markdown: The Definitive Guide (https://bookdown.org/yihui/rmarkdown/). This will be a helpful resource in the future. Add unordered or ordered list below the Workflow header and knit the document. Describe the workflow we have talked about thus far (e.g., create an R project, add folders, create an rmarkdown document, knit the document to render). Add sub-bullets with helpful details. Add the image provided with the data to the Introduction. Make the Plot section tabbed section with {.tabset}. 101.10 Lesson 4: YAML Headers YAML: YAML Ain’t Markup Language A YAML header contains YAML arguments, such as “title”, “author”, and “output”, demarcated by three dashes (—) on either end. 101.10.1 Title The specified title (title: \"My Title\") will appear at the head of the document with a larger font size than the rest of the document. 101.10.1.1 Subtitle You can specify subtitle (subtitle: \"My Subtitle\"), which will appear below and in slightly smaller font size than the title of the document. 101.10.2 Author(s) An author can be specified (author: \"S. Mason Garrison\") and will appear at the head of the document but below the title(s). You can specify multiple authors by separating the names of the authors by a comma (author: \"S. Mason Garrison, Tukey Garrison\"). 101.10.3 Date You can supply a date (date: “2025-02-17”) that will be added to the head of the document. You can specify a static date (date: \"March 3rd, 2020\") or a dynamic date (see image below) that will update each time you knit (render) the document– we will discuss this further in a subsequent section. 101.11 Table of Contents (TOC) 101.11.1 Floating Table of Contents (TOC) 101.12 Themes You can modify the YAML to specify the .theme of the document, which will change document styling (e.g., font type, color, size) The following link provides examples of some of the available R Markdown themes: https://www.datadreaming.org/post/r-markdown-theme-gallery/ 101.13 Code Folding Code folding refers to the HTML YAML option to hide code chunks by default, but enable the reader to click a button to show the underlying code chunk. 101.13.1 Example without Code Folding 101.13.2 Examples with Code Folding 101.13.2.1 Show 101.13.2.2 Hide 101.14 output The output option allows you to specify the type of document you want to create. This will be auto-populated if you generate the .Rmd file in RStudio by creating a new R Markdown file through the toolbar. You can manually modify the output type, but you must specify valid arguments. Some valid arguments include: html_document html_notebook pdf_document word_document You can click on the small triangle to the right of the Knit button to view a drop-down menu of knitting options. Here you have the option to update the document type you want to knit (examples listed below). Knit to HTML Knit to PDF Knit to Word By clicking one of these options, your YAML header updates automatically. You can have multiple document types render at once (e.g., a separate HTML and Word document are created with each knit call). 101.15 Custom Template You have the ability to specify custom templates in your YAML header. I have only used this feature for Microsoft Word documents. I created a new document within Microsoft Word and selected all of the formatting options I wanted for my document. I then added the template to the YAML header of my R Markdown document to specify that when the document is knit (rendered) to apply the formatting specifications found in the template Microsoft Word document. 101.16 ymlthis ymlthis is an R package intended to make it easier for you to generate YAML headers. The package also includes an interactive addin that provides an intuitive user-interface for generating YAML headers. You can visit the following link to learn more about how to use ymlthis: []https://ymlthis.r-lib.org/ Addins can be accessed by navigating to “Tools” in the RStudio banner, scrolling down to “Addins”, and selecting “Browse Addins.” A pop-up window will appear. If you have ymlthis installed on your machine, you will see the ymlthis addin in the pop-up window table. Once you select the ymlthis addin, you will see a pop-up window like this… 101.17 Your Turn Edit the YAML title and author and knit the document. Specify in the YAML that you want to include a table of contents and knit the document. Specify in the YAML that you want a floating table of contents and knit the document. Specify in the YAML that you want to change the theme to “journal” and knit the document. If you have Microsoft Word installed on your computer, change the YAML output to word_document and the knit the document. Bonus: If you finish early, install ymlthis (install.packages(\"ymlthis\")), navigate to the ymlthis addin, and try to re-create the YAML header from steps 1-4. 101.18 Lesson 5: Code Chunks and Inline Code Up to this point in the workshop we have mostly just written a standard markdown document. However, we want to integrate R code into our document to create reproducible objects, such as figures, tables, and text. 101.18.1 Code Chunks To insert a code chunk, press Ctrl + Alt + I in the source pane (top left pane in the default settings of RStudio). A code chunk will appear: Inside the code chunk you can write and run R-code. If you print the output of your R-code it will appear below the code chunk in the source pane and the printed output will appear in the final compiled document. This is useful for producing figures and tables. On the far right of the code chunk you will see three buttons. The cog makes it easy to specify code chunk options– discussed below. The middle button will execute all code chunks prior to a given function (Ctrl + Alt + P) The far right button will execute all of the code in a code chunk. 101.18.2 Shortcuts You will want to learn these ASAP. Ctrl + Alt + I - Insert a new code chunk Ctrl + Enter - Run the line of code your cursor is on Ctrl + Alt + R - Run all of the code in the document Ctrl + Alt + P - Run all code chunks above your cursor Ctrl + Shift + F10 - Restart R 101.18.3 Options There are a number of code chunk options that can be specified in the code chunk header to modify how the code chunk is rendered while kniting. These options will not be visible in the rendered document– even if you choose not to hide the code chunks in your rendered document. A comprehensive list of chunk options can be found here: https://yihui.org/knitr/options/ 101.18.3.1 name Best practice is to include a short and unique name within each code chunk header. This will be especially beneficial if you need to troubleshoot an error in your document as you will be able to track the source of the error down by code chunk name. The “name” is not specified as an argument within the header; it is simply separated from the “r” by a space. This is exemplified in the image below on line 9 where “addition” is supplied as the name of the code chunk ({r addition}). 101.18.3.2 echo A logical value (true or false) indicating if the code chunk should be visible in the rendered document. By default the code chunk will be visible upon knitting (echo=true). To hide a specific code chunk set echo to “false” (echo=false). Include the code chunk in the rendered document with echo=true. Exclude the code chunk in the rendered document with echo=false. 101.18.3.3 fig.width, fig.height, and fig.cap The fig.width and fig.height options allow you to control the size of the figure generated from a given code chunk. fig.cap provides the ability to add a caption below the figure generated from a given code chunk. Here is an example of the plot output form a code chunk. Here I have specified fig.width, fig.height, and fig.cap. Notice that plot dimensions have changed from the previous image and a caption has been added to the bottom of the figure. In many cases, you will want the captions to be numbered and you will want this to occur automatically. To this you will first need to install the package bookdown (install.packages(\"bookdown\")). Specify in the YAML header that the output should be “html_document2” from the bookdown package (output: bookdown::html_document2). When you render the document the figures will now be automatically numbered by the order they appear in the document. To cross reference a figure by name in the text use \\@ref(fig:figure_name). Using the example from the figure below, you might write “In \\@ref(fig:IrisScatter) we can see that the species setosoa tend to have shorter sepal lengths than the other two species.” For more information visit https://bookdown.org/yihui/rmarkdown-cookbook/figure-number.html. 101.18.3.4 message and warning Using message=FALSE and/or warning=FALSE will suppress messages and warnings preventing them from printing in the rendered document. Warning: Suppress these messages/warnings at your own risk. Without these messages/warnings you may make it very difficult to track down any issues in your code. An example of messages that are produced with the default message=TRUE. The messages above can be suppressed by setting message=FALSE in the code chunk header. 101.18.3.5 include Code chunk headers with include=FALSE will be ignored when the document is rendered. This can be useful during document development. For example, you may have a code chunk producing an error that prevents you from rendering the rest of your document or you may be questioning the need to keep a given code chunk. include=FALSE will exclude the code from the rendered document without deleting the code entirely from the .Rmd file. The default is include=TRUE. In the example below, include is not altered from the default and therefore all of the code chunks are present in the rendered document. In the example below I added include=FALSE to the header of the second code chunk named “iris-sepal-scatter.” The second code chunk is excluded from the rendered document. 101.18.3.6 Setting Global Options It can be helpful to establish global options to produce a standard format throughout the document. You can set this with the knitr function opts_chunk$set(). For instance, you may want to set a standard dimensions for figures produced by code chunks throughout your document. In the code chunk below, you can see that I set fig.width to 8 and fig.height to 4. knitr::opts_chunk$set(fig.width = 8, fig.height = 4) The options specified in opts_chunk$set() become the default when rendering the document. However, you still have the ability to overwrite these options for specific code chunks. Building off of the previous example, you may find that one out of ten of your plots is not aesthetically pleasing with fig.width = 8 and fig.height = 4. You could specify in the code chunk producing that plot that you want to set fig.width = 6 and fig.height = 6, for example. 101.19 Inline Code Inline code enables you to insert R code into your document to dynamically updated portions of your text. To insert inline code you need to encompass your R code within: . For example, you could write: Which would render to: The mean sepal length found in the iris data set is 5.8433333. I frequently use inline code to make my YAML header date update to the date the document was last rendered. You can reformat the date using the format() function, as depicted in the image below. 101.20 Your Turn (Part 1) The point of this exercise is to get you familiar with code chunks and inline code, NOT to test your R knowledge; therefore, I have included the necessary code below each task. Add a code chunk using Ctrl + Alt + i to import the example data set. read.csv(file = file.path(\"data\", \"zms_thesis_metrics.csv\"), stringsAsFactors = FALSE) Add a name to this code chunk header and all subsequent code chunk headers. Add plain text description above the code chunk. Add the following unordered list describing the contents of the imported data. unique_id (character) unique sample ID lake (character) lake code: caz = Cazenovia lake, onon = Onondaga, and ot = Otisco lat (numeric) latitude of sampling location long (numeric) longitude of sampling location substrate_size_d50 (numeric) median particle size from pebble count conductivity (numeric) specific conductivity (µS/cm) richness (numeric) taxonomic richness shannon (numeric) Shannon-Wiener diversity index values pct_ephemeroptera (numeric) relative abundance of Ephemeroptera taxa pct_amphipoda (numeric) relative abundance of Amphipoda taxa pct_diptera (numeric) relative abundance of Diptera taxa dom_1 (numeric) relative abundance of the most dominant taxon observed in each sample Add a code chunk to import the tidyverse packages. library(tidyverse) Add plain text description above the code chunk. Add message=FALSE to the code chunk header to prevent the package start-up messages Add a code chunk below libary(tidyverse) to preprocess lake name (lake) associated with each sample. df_thesis &lt;- df_thesis %&gt;% mutate( lake = case_when( lake %in% &quot;caz&quot; ~ &quot;Cazenovia&quot;, lake %in% &quot;onon&quot; ~ &quot;Onondaga&quot;, lake %in% &quot;ot&quot; ~ &quot;Otisco&quot;, TRUE ~ &quot;ERROR&quot; ), lake = factor(lake, levels = c( &quot;Onondaga&quot;, &quot;Otisco&quot;, &quot;Cazenovia&quot; )) ) Add a code chunk just below the YAML header to specify global code chunk options specify fig.width and fig.height in the header specify echo=FALSE to hide this code chunk– your reader does not need to see this knitr::opts_chunk$set() Add a code chunk to generate a scatter plot. Add a caption ggplot(df_thesis, aes(substrate_size_d50, pct_diptera, color = lake)) + geom_point() Add a code chunk generate a boxplot. Modify the figure size Add a caption ggplot(df_thesis, aes(lake, richness, fill = lake)) + geom_boxplot() Add a brief interpretation of the figures into the “Conclusions” section. Bonus: Try using the cog at the top right of the code chunk to update a code chunks options. 101.21 Interactive Features Interactive tables and figures are a great way to make your document more interesting and can be very useful for data exploration. I will warn you that if you use these features too frequently your document will become cluttered and will likely load and run slowly. 101.21.1 DT The DT package enables you and your readers to interact with tables in your document. You can filter and sort the table to view data that you are most interested in exploring or understanding. For more information visit https://rstudio.github.io/DT/ library(DT) datatable(iris) 101.21.2 Plotly Plotly makes it easy to create interactive figures. Some of the most useful features are the ability to… rollover figure objects and see a pop-up information related to specific features Zoom-in and zoom-out take a picture of the figure For more information visit: https://plot.ly/ggplot2/ I generally use plotly in conjunction with ggplot2. The plotly function, ggplotly(), makes it simple to convert ggplot2 figures to interactive plotly figures. library(plotly) p_scatter &lt;- ggplot(iris, aes( x = Petal.Width, y = Petal.Length, color = Species )) + geom_point() ggplotly(p_scatter) 101.21.3 Leaflet Leaflet is a great resource for creating interactive maps with minimal amount of coding. For more information visit: https://rstudio.github.io/leaflet/ library(leaflet) data(&quot;quakes&quot;) leaflet( data = quakes, options = leafletOptions( minZoom = 4, maxZoom = 18 ) ) %&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat, fillOpacity = 0.75, stroke = FALSE, popup = paste( &quot;Sample ID:&quot;, quakes$unique_id, &quot;&lt;br/&gt;&quot;, &quot;Magnitude:&quot;, quakes$mag, &quot;&lt;br/&gt;&quot;, &quot;Latitude:&quot;, quakes$lat, &quot;&lt;br/&gt;&quot;, &quot;Longitude:&quot;, quakes$long ) ) Another neat feature is the ability to cluster points to better visualize density. You can do this by setting clusterOptions = markerClusterOptions(). library(leaflet) data(&quot;quakes&quot;) leaflet( data = quakes, options = leafletOptions( minZoom = 4, maxZoom = 18 ) ) %&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat, fillOpacity = 0.75, stroke = FALSE, popup = paste( &quot;Sample ID:&quot;, quakes$unique_id, &quot;&lt;br/&gt;&quot;, &quot;Magnitude:&quot;, quakes$mag, &quot;&lt;br/&gt;&quot;, &quot;Latitude:&quot;, quakes$lat, &quot;&lt;br/&gt;&quot;, &quot;Longitude:&quot;, quakes$long ), clusterOptions = markerClusterOptions() ) 101.22 Your Turn (Part 2) Add a code chunk below the “Preproccessing” header and add the following DT code add this link (https://rstudio.github.io/DT/) to your description above the code chunk to describe where you can find more information on DT library(DT) datatable(df_thesis, options = list(scrollX = TRUE)) Add a code chunk below the “Study Area” header and add the following leaflet code add this link (https://rstudio.github.io/leaflet/) to your description above the code chunk to describe where you can find more information on leaflet library(leaflet) pal &lt;- colorFactor(c(&quot;#619Cff&quot;, &quot;#F8766D&quot;, &quot;#00BA38&quot;), domain = c(&quot;Cazenovia&quot;, &quot;Onondaga&quot;, &quot;Otisco&quot;) ) leaflet( data = df_thesis, options = leafletOptions( minZoom = 7, maxZoom = 13 ) ) %&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat, fillOpacity = 0.75, fillColor = ~ pal(lake), stroke = FALSE, popup = paste( &quot;Sample ID:&quot;, df_thesis$unique_id, &quot;&lt;br/&gt;&quot;, &quot;Lake:&quot;, df_thesis$lake, &quot;&lt;br/&gt;&quot;, &quot;Latitude:&quot;, df_thesis$lat, &quot;&lt;br/&gt;&quot;, &quot;Longitude:&quot;, df_thesis$long ) ) Edit the scatter plot code chunk to make the figure interactive with plotly add this link (https://plot.ly/ggplot2/) to your description above the code chunk play around with the interactive plot and the tools provided at the top right of the plot For example, you can zoom in, click on points to get more information, take a snapshot of the current plot, etc. library(plotly) p_scatter &lt;- ggplot( df_thesis, aes( substrate_size_d50, pct_diptera ) ) + geom_point(aes(color = lake)) + geom_smooth(method = &quot;lm&quot;) ggplotly(p_scatter) "],["child-documents.html", "102 Child Documents 102.1 Extract and Run R-Code from R Markdown Files 102.2 Your Turn", " 102 Child Documents In general, I find that a single R Markdown file quickly becomes unwieldy. I recommend breaking the document up into multiple “child” documents and sourcing these child documents in a parent document. My child documents generally represent major subsections of the document. I prefer to store the parent R Markdown file in a folder labeled “markdown” (rproject/markdown) and the child R Markdown files in a sub-directory of my “markdown” folder called “sections” (rproject/markdown/sections). In the parent file, the child files are sourced within the code chunk header using child = 'sections/example.Rmd'. After sourcing all the child chunks, the parent file can be knit (compiled) like a normal R markdown document. The child documents cannot be run in the parent file. 102.1 Extract and Run R-Code from R Markdown Files The parent file is great for organizing sections of your document, but the child documents cannot be executed within R Studio like a normal code chunk. Without the ability to easily execute the R code within the child documents it can become very difficult to develop new child documents because new child documents often depend on upstream code execution. Imagine you have a parent document that sources child sections which import your data and clean your data. You now want to visualize your data; accordingly, you begin to develop a visualization child document, which depends on information from the upstream child sections. It would be inefficient and inappropriate to perform all the steps in the upstream child sections within the visualization section. Therefore, you need an effective way to execute the upstream child sections while you continue to develop the visualization section. The inefficient way of doing this is to open each child Rmd file in R Studio and execute them manually in the correct sequence. This becomes tedious after you have three or more documents (imagine doing this for 10+ child sections). The most efficient way that I have found to run upstream child sections is to extract the R-code chunks from each Rmd file, save them in a “raw_scripts” folder, and then source/execute the scripts within a regular R script file (.R). 102.1.1 R Code In this section we establish the file path to the folder that contains all the child documents. The names of the child documents are extracted and stored as a vector. The grepl() function is used to retain only the Rmd files stored in the vector. sections_path &lt;- file.path( rprojroot::find_root(&quot;r_in_practice.Rproj&quot;), &quot;markdown&quot;, &quot;sections&quot; ) r_files_vec &lt;- list.files(sections_path) r_files_vec &lt;- r_files_vec[grepl(&quot;.Rmd&quot;, r_files_vec)] Next, a file path is specified for the R-scripts that will be extracted from the R Markdown documents; I place these files within a “raw_script/extracted” folder. The map() function from the purrr package is used to loop through each file in the vector (r_files_vec). Within the map() loop, the purl() function from knitr is used to extract the R-code from the R Markdown documents and save the code to the specified folder. extracted_path &lt;- file.path( rprojroot::find_root(&quot;r_in_practice.Rproj&quot;), &quot;markdown&quot;, &quot;raw_scripts&quot;, &quot;extracted&quot; ) purrr::map(r_files_vec, function(file.i) { # print(file.i) file_name &lt;- gsub(&quot;.Rmd&quot;, &quot;&quot;, file.i) extracted_file &lt;- paste0(file_name, &quot;.R&quot;) knitr::purl( file.path(sections_path, file.i), file.path(extracted_path, extracted_file) ) }) Finally, create a vector of file names (vec_source) stored in the “raw_script/extracted” folder. You will want to type these out manually (do not use list.files() functions) because in this format you can easily comment out certain scripts and only run the scripts of interest. map() is then used to loop through each specified file in vec_source. Keep in mind that the order of the file names specified in vec_source will determine the order that these files are executed in the map() function; therefore, order the files in vec_source from furthest upstream to furthest downstream. Each iteration of the loop, executes (sources) the specified R-script. vec_source &lt;- c( &quot;introduction.R&quot;, &quot;quick_reference.R&quot;, &quot;installation_updates.R&quot;, &quot;r_project.R&quot;, &quot;version_control.R&quot; ) purrr::map(vec_source, function(source_i) { source(file.path(extracted_path, source_i)) }) Once all the R-scripts extracted from the upstream child R Markdown files have been executed, you can begin or continue work on a new child R Markdown document. I keep all the above code in a single R-script and execute the entire script each time I use this file to make sure all of the files are up-to-date. 102.2 Your Turn Create an R Markdown document for each of the sections, designated by Header level 1s, we have created thus far. save the documents within your project root (where your .Rproj file lives) Copy the Header and the relevant content below the header. Paste the content into the appropriate R Markdown document. Create a parent R Markdown document. create a code chunk for each section. add child = 'insert-file-name-here.Rmd' to the header of the appropriate code chunk Knit the document "],["parameterized-reports.html", "103 Parameterized Reports 103.1 params 103.2 Knitting 103.3 rmarkdown::render() 103.4 Your Turn", " 103 Parameterized Reports Parameterized reports allow you to create a template for generating reports based on specific data. For example, imagine you are tasked with generating 100 one page fact sheets for lakes in your state. The fact sheets will include boilerplate language but for each lake’s fact sheet you will need to update the name of the lake used through out the document, provide a list of metrics relevant to the lake (e.g., lake area, lat/longs, and max depth), and map of the lake. Think about how long it would take you to compile of the necessary components (e.g., lake metrics and lake maps), the amount of time it would take you to manually add these components to each fact sheet, and the struggles you would likely run into with formatting. Now imagine you complete all 100 fact sheets, hand the fact sheets to your boss, and they ask you to change the format of all the maps in the fact sheets. You would likely need to manually re-generate all of the maps and manually replace the maps in each document. Here is a link to a great lecture on creating parameterized reports: https://www.coursera.org/lecture/reproducible-templates-analysis/adding-parameters-in-a-document-template-6fQwc 103.1 params To add a parameter or parameters to your document, add the params: argument below output: html_document. Below params: you can specify any parameter name you want to use and any default value you want to supply. In this example I will be using the iris data set, and therefore I specified that the name of my parameter would be “species” and the default value would be “setosa.” The specified parameter can be used throughout the document with the syntax params$species. params$species will act as a place holder throughout the document until the document is rendered and, in this case, the species is specified; this specified species will be used throughout the document where params$species was acting as a placeholder. In the example below, I use inline code to insert the species name into the title of the document. Therefore, the title will automatically update during rendering depending on the specified parameter. The parameters can also be used to filter the original data frame to a subset of interest. For example, the iris data frame can be filtered to only include the specified species of interest using the code below. data(iris) iris_sub &lt;- iris[iris$Species %in% params$species, ] The parameter can be referenced frequently to update the text in the document or to filter to the data of interest. In the image below, the iris data set will be filtered by the specified parameter to generate iris_sub. iris_sub will be used to calculate the mean value for each flower characteristic. 103.2 Knitting To the knit the document, click on the knit drop-down menu (the small triangle to the right of the Knit button) and select “Knit with Parameters…” A pop-up window will appear specifying the list of available parameters and their default values. These default values can be edited prior to clicking “knit” at the bottom right of the window. The document will then render. 103.3 rmarkdown::render() The rmarkdown function render() can also be used to compile the document. The input specifies the parameterized .Rmd file. The params argument species the parameter values to be used when rendering the document. Note that the value supplied to params must be wrapped by list(). rmarkdown::render( input = &quot;parameterized/param_template.Rmd&quot;, params = list(species = &quot;setosa&quot;) ) It would not be efficient to specify each parameter of interest one-by-one. Instead a loop can be written to iteratively generate parameterized documents. Below the function lapply() is used to create the loop. unique(iris$Species) supplies a vector of unique species names (i.e., “setosa”, “versicolor”, and “virginica”) to generate parameterized reports. species.i is specified as a placeholder to represent each unique species in the iris data set; therefore species.i is used to specify what the species parameter should equal and it is used to generate a unique and easy to interpret file name output_file = paste0(species.i, \".HTML\"). lapply(unique(iris$Species), function(species.i) { rmarkdown::render(&quot;parameterized/param_template.Rmd&quot;, params = list(species = species.i), output_file = paste0(species.i, &quot;.HTML&quot;) ) }) 103.4 Your Turn Create a new .Rmd file and copy/paste the content of the .Rmd file created in Lesson 5: Code Chunks and Inline Code add params: and specify lake as a parameter with Onondaga as the default (lake: Onondaga) to the YAML header Use inline code to have the YAML title update to include the specified parameter (params$lake) In your introductory paragraph, use inline code to specify that this document is specific to X Lake. Filter df_thesis to only represent the specified lake parameter df_thesis &lt;- df_thesis %&gt;% filter(lake %in% params$lake) Update your conclusion section text to specify that the results are from X Lake and use inline code to specify: the minimum relative abundance of Diptera taxa min(df_thesis$pct_diptera) the maximum relative abundance of Diptera taxa max(df_thesis$pct_diptera) the median richness value observed in the lake median(df_thesis$richness) Render the document using the “Knit with Parameters…” button Use the default parameter Specify a different parameter (e.g., lake = Cazenovia) Bonus: Try to write a loop with lapply() and render() to automate the process to generate a report for each of the three lakes in the data set. "],["special-topics-machine-learn.html", "104 Special Topics: Machine, Learn 104.1 Module Materials", " 104 Special Topics: Machine, Learn This bonus module is designed to introduce machine learning and neural networks. Please watch the curated videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. me, a statistician: *interviewing for a machine learning job* as i always say &quot;machine learning is basically statistics!&quot; pic.twitter.com/OBY9KKDSRv&mdash; Kareem Carr, Statistics Person (@kareem_carr) March 2, 2021 I hate to tell you this but most deep learning models are just three or more logistic regressions in a trench coat.&mdash; Kareem Carr, Statistics Person (@kareem_carr) January 19, 2021 Here&#39;s my new experiment with AI-generated Pokémon: I finetuned an AI on *only one image* of Pikachu and had it generate new images of Pikachu.Merry Christmas! pic.twitter.com/u4GQFC0ASY&mdash; Max Woolf (@minimaxir) December 26, 2021 104.1 Module Materials Readings A visual introduction to machine learning "],["neural-networks.html", "105 Neural Networks 105.1 What is a Neural Network? 105.2 How does it learn?", " 105 Neural Networks 105.1 What is a Neural Network? This is what #Tesla Autopilot sees using #neuralnetworks that take 70.000 GPU hours to train and output 1,000 tensors at each timestep#AI #DeepLearning #AutonomousVehicles@mvollmer1 @Hana_ElSayyed @JeroenBartelse @kalydeoo @WSWMUC @pascal_bornet @CurieuxExplorer @fogle_shane pic.twitter.com/h6ZOUSsoNF&mdash; Franco Ronconi  (@FrRonconi) February 6, 2021 105.2 How does it learn? The following videos by Youtuber and game developer Dani illustrate the iterative learning process. Specifically, I think his video on “Teaching A.I. to Play my Game!” introduces the basic ideas behind neural networks well and shows the dynamic developement process. His video on “Stickman A.I. Learns To Walk” builds upon these ideas and illustrates how the iterative learning process requires thoughtful interventions. 105.2.1 Teaching A.I. to Play My Game 105.2.2 Stickman A.I. Learns To Walk "],["natural-language-processing.html", "106 Natural Language Processing", " 106 Natural Language Processing Resources: https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing https://app.inferkit.com/demo "],["dont-miss-the-last-module.html", "Don’t Miss The Last Module 106.1 Important Wake Forest Stuff 106.2 What Next?", " Don’t Miss The Last Module This course was designed to be a starting point. You have learned so much in a short span. I am so proud of each and every one of you!!!!! # library(embedr) # embed_audio(&quot;assets/audio/SoLongFarewellAllParts.mid&quot;) Before you go – I have some important practical things to walk you thru. Most of these are Wake Forest Specific… such as making sure to connect your GitHub account to a non-WFU email. As well as well other things… 106.1 Important Wake Forest Stuff For those of you who are graduating from Wake Forest University, you may not be aware that your email address gets shut off very soon after you graduate. Why do they do this? I do not know and this it is a silly policy. Regardless, before you lose access to your WFU email, you need to add a 2nd email address to your GitHub account. Otherwise, you will not be able to get access to your materials after you graduate. GitHub has some incredibly useful guides to do this… How to add an email address to your GitHub account How to change your primary email address How to set up a backup email address Please do not procrastinate this! It is really important to do this before you lose access!!!! 106.2 What Next? Industry Data Analyst Resume Templates Examples of Good vs Bad Resumes 106.2.1 Industry Transition Stories 106.2.1.1 Joe Hilgard I started a new job in industry as a data scientist. Smell you later. https://t.co/PkXniG7Mz5&mdash; Joe Hilgard, data guy (@JoeHilgard) May 29, 2021 106.2.1.2 Michael Mullarkey "],["thoughts-from-hadley-wickham-on-tidyverse.html", "Thoughts from Hadley Wickham on Tidyverse 106.3 Dive into Hadley Wickham’s Tidyverse 106.4 Current State of the Tidyverse (2020)", " Thoughts from Hadley Wickham on Tidyverse Although I’m perpetually reluctant to embed videos that I don’t host, I think hearing from Hadley Wickham is worthwhile. 106.3 Dive into Hadley Wickham’s Tidyverse 106.4 Current State of the Tidyverse (2020) "],["good-resources.html", "107 Good Resources 107.1 Cheatsheets", " 107 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ 107.1 Cheatsheets Rstudio has a glorious number of cheatsheets, including: Data Wrangling "],["media-without-a-home-yet.html", "108 Media without a home yet 108.1 SIPS Resources 108.2 Visualizing Linear Models: An R Bag of Tricks 108.3 For new programmers learning keyboard shortcuts 108.4 Are you a student? If yes, this is the best data science project for you 108.5 rstudio is magic 108.6 automation quote 108.7 How computer memory works 108.8 Is Coding a Math Skill or a Language Skill? Neither? Both? 108.9 Quantum Computers Explained 108.10 The Rise of the Machines – Why Automation is Different this Time 108.11 Emergence – How Stupid Things Become Smart Together 108.12 How not to ask for help 108.13 The Birthday Paradox 108.14 Why can’t you divide by zero? 108.15 Yea he’s chewing up my stats homework but that face though 108.16 Coding Kitty 108.17 Democratic databases: science on GitHub 108.18 Ten simple rules for getting started on Twitter as a scientist 108.19 NYT data ethics stuff 108.20 ", " 108 Media without a home yet 108.1 SIPS Resources Data Management Hackathon - Syllabus SIPS Products 108.2 Visualizing Linear Models: An R Bag of Tricks Visualizing Linear Models: An R Bag of Tricks I&#39;m starting a 3-week #rstats short course, Visualizing Linear Models: An R Bag of Tricks.One week on univariate models, two weeks on models for multivariate responses. Lectures notes, examples and exercises are at: https://t.co/LF1iVPZOPs&mdash; Michael Friendly @datavisfriendly.bsky.social (@datavisFriendly) February 27, 2021 108.3 For new programmers learning keyboard shortcuts https://www.shortcutfoo.com/ 108.4 Are you a student? If yes, this is the best data science project for you 108.5 rstudio is magic Multiple cursors in @RStudio are so handy! Holding down the option key and drag gives me multiple synced cursors ️️️ pic.twitter.com/nQKzqIwsou&mdash; Emil Hvitfeldt (@Emil_Hvitfeldt) February 2, 2021 108.6 automation quote &quot;I’ve always objected to doing anything over again if I had already done it once.&quot; – Grace Hopper&mdash; Programming Wisdom (@CodeWisdom) February 8, 2021 108.7 How computer memory works 108.8 Is Coding a Math Skill or a Language Skill? Neither? Both? 108.9 Quantum Computers Explained 108.10 The Rise of the Machines – Why Automation is Different this Time 108.11 Emergence – How Stupid Things Become Smart Together 108.12 How not to ask for help 108.13 The Birthday Paradox 108.14 Why can’t you divide by zero? 108.15 Yea he’s chewing up my stats homework but that face though Yea he’s chewing up my stats homework but that face though… from r/CatsBeingCats 108.16 Coding Kitty https://hostrider.com/ I’ve been using R since 2008. Today I learned about stats::reformulate(). https://t.co/7xm4CiHkW9&mdash; James E. Pustejovsky (@jepusto) December 16, 2021 108.17 Democratic databases: science on GitHub Nature: “Democratic databases: science on GitHub” (Perkel, 2016). 108.18 Ten simple rules for getting started on Twitter as a scientist https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007513 108.19 NYT data ethics stuff https://www.nytimes.com/2021/01/31/technology/facial-recognition-photo-tool.html 108.20 Art! https://t.co/XuDToJAmnp&mdash; Prof. Mason Garrison ✨ (@SMasonGarrison) March 18, 2021 "],["r-commands.html", "109 R Commands", " 109 R Commands Below is a list of R commands along with links to their help pages: summary(): Provides summary statistics for numeric data. plot(): Creates various types of plots. lm(): Fits linear regression models. read.csv(): Reads a CSV file into a data frame. install.packages(): Installs R packages from CRAN. "],["references.html", "References", " References Fox, John. 2016. Applied Regression Analysis and Generalized Linear Models. Third edition. Los Angeles: SAGE Publications, Inc. Spector, P. 2008. Data Manipulation with r. Use r! Springer. https://books.google.com/books?id=grfuq1twFe4C. White, Ethan P., Elita Baldridge, Zachary T. Brym, Kenneth J. Locey, Daniel J. McGlinn, and Sarah R. Supp. 2013. “Nine Simple Ways to Make It Easier to (Re)use Your Data.” PeerJ PrePrints 1 (July): e7v2. https://doi.org/10.7287/peerj.preprints.7v2. Wickham, H. 2015. Advanced r. Chapman &amp; Hall/CRC the r Series. CRC Press. https://books.google.com/books?id=FfsYCwAAQBAJ. Wickham, Hadley. 2011a. “Testthat: Get Started with Testing.” The R Journal 3 (1): 5–10. ———. 2011b. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software, Articles 40 (1): 1–29. https://doi.org/10.18637/jss.v040.i01. ———. 2014. “Tidy Data.” Journal of Statistical Software, Articles 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10. Wickham, Hadley, and Jenny Bryan. In progress. R Packages. 2nd ed. https://r-pkgs.org. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. https://books.google.com/books?id=vfi3DQAAQBAJ. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R Welcome to PSY 703 Mason Notes Dont Miss Module 00", " Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R S. Mason Garrison 2021-01-25 Welcome to PSY 703 Welcome to class! This website is designed to accompany Mason Garrisons Data Science for Psychologists (DS4P). DS4P is a graduate-level quantitative methods course at Wake Forest University. This website hosts the course notes. All the embedded lecture videos can be found on a youtube playlist. You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. Mason Notes This website is constantly changing. This new course is active development, and approximately 60% done. I have made this process explicitly transparent because I want you to see how you can use R to produce some pretty neat things. Indeed, Ive included the source code for this website in the class github. I encourage you to contribute to the course code. If you catch typos, errors, please issue a pull request with the fixes. If you find cool / useful resources, please add them. By the end of the semester, I would love for everyone to have contributed to the course materials. It can be as simple as adding a course request to the wishlist. I believe that it is useful skill to be able to do. In terms of timing, I will have each module completed by the start of the week. In our first class, well decide what day of the week that will be. It is likely that I will get ahead of this deadline. You can see the current status of the course below. Status of course This table provides the current status of the course. It lists proportions of specific components by module. Overall it is 60% complete. Course Wishlist Although there will be hiccups and snafus along the way, one major advantage of this process is that you(!) can have a major input on what we cover. Some of these inputs have already been incorporated (such as github, Rshiny). So take advantage! Add your requests to the markdown list below! As I incorporate those requests, Ill move them into the Wish Granted Section. Wish Granted Github Rshiny Wish Not Yet Granted Data Science and the Law Computational neuroscience Dont Miss Module 00 This overview is designed to orient you to the class. Please watch the videos from this playlist and work your way through the notes. Although the module-level playlists are embedded in the course, you can find the full-course video playlist here. Data Science for Psychologists (DS4P) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. In this class, we link those principles to psychological methods and open science practices by emphasizing exploratory analyses and description, rather than confirmatory analyses and prediction. Through the semester we will work our way thru Wickham and Grolemunds R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. DS4P is a practical skilled-based class and should be useful to students aiming for academia as well as those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g, eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., valence analysis). 0.0.1 Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. 0.0.2 Materials 0.0.2.1 Hardware This class is requires that you have a laptop that can run R. 0.0.2.2 Required Texts The text is intended to supplement the videos, lecture notes, and in-class tutorials. You probably need to consume all four in order to be successful in this class. R for Data Science text 0.0.2.3 Software 0.0.2.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface for R. 0.0.2.4 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files  called a repository  in a structured way. Think of it like the Track Changes features from Microsoft Word. Github is a free IDE and hosting service for Git. As a Wake Forest student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account 0.0.3 Knowledge is Power This brief video is covers the icebreaker I do in all of my classes. I encourage you to watch it. In it, I discuss stereotype threats and statistics anxiety. 0.0.4 Course Modality This class is a blended class. The online portions are asynchronous. Ive created a video highlighting how to be a successful asynchronous learner. Much of this information comes from Northeastern Universitys Tips for Taking Online Classes "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major Attributions Jenny Bryans (jennybryan.org) STAT 545; Joe Rodgerss PSY 8751 Exploratory and Graphical Data Analysis Course Mine Çetinkaya-Rundels Data Science in a Box. Additional Attributions Academic.ios AWESOME DATA SCIENCE Julia Fukuyamas EXPLORATORY DATA ANALYSIS Benjamin Soltoffs Computing for the Social Sciences Grant McDermotts course materials on environmental economics and data science "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon Sitemap", " Colophon This book was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from GitHub. The book style was designed by Desirée De Leon. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-01-25 Along with these packages: Sitemap html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lpufrodcbd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lpufrodcbd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lpufrodcbd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lpufrodcbd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #lpufrodcbd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lpufrodcbd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lpufrodcbd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lpufrodcbd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lpufrodcbd .gt_column_spanner_outer:first-child { padding-left: 0; } #lpufrodcbd .gt_column_spanner_outer:last-child { padding-right: 0; } #lpufrodcbd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #lpufrodcbd .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #lpufrodcbd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lpufrodcbd .gt_from_md > :first-child { margin-top: 0; } #lpufrodcbd .gt_from_md > :last-child { margin-bottom: 0; } #lpufrodcbd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lpufrodcbd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #lpufrodcbd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lpufrodcbd .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #lpufrodcbd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lpufrodcbd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lpufrodcbd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lpufrodcbd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lpufrodcbd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lpufrodcbd .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #lpufrodcbd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lpufrodcbd .gt_sourcenote { font-size: 90%; padding: 4px; } #lpufrodcbd .gt_left { text-align: left; } #lpufrodcbd .gt_center { text-align: center; } #lpufrodcbd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lpufrodcbd .gt_font_normal { font-weight: normal; } #lpufrodcbd .gt_font_bold { font-weight: bold; } #lpufrodcbd .gt_font_italic { font-style: italic; } #lpufrodcbd .gt_super { font-size: 65%; } #lpufrodcbd .gt_footnote_marks { font-style: italic; font-size: 65%; } title link additional media that doesnt have a home yet website attribution website basic data care website building plots for various data types website colophon website cross validation website data and ethics website data and visualization website data types and data transformations website dont forget module 00 website fitting and interpreting models website functions website good resources website grammar of data wrangling website index website lab work on portfolios website lab01 website lab02 website lab03 website lab04 website lab05 website lab06 website lab07 website lab08 website lab09 website lab10 website lab11 website lab13 website license website media without a home yet website meet our toolbox website orientation module 00 website prediction and overfitting website prediction and overfittinga website quantifying uncertainty website r basics website references website rshiny website scientific studies and confounding website secrets website special topics website thoughtful workflow website tips for effective data visualization website visualization examples website web scraping website welcome to data science website "],["welcome-to-data-science.html", "1 Welcome to Data Science 1.1 Module Materials 1.2 What is Data Science? 1.3 Course structure and some other useful things", " 1 Welcome to Data Science This module is designed to introduce you to data science. Please watch the videos and work your way through the notes. You can find the module playlist here. Most of the slides used to make the videos in this module can be found in the slides repo. 1.1 Module Materials Slides Welcome Slides Meet the toolkit Activities UN Voting Covid Data Bechdal Test Lab Hello R 1.2 What is Data Science? You can follow along with the slides here if they do not appear below. 1.2.1 See for yourselves! Ive embedded a few examples below. 1.2.1.1 Shiny App 1.2.1.2 Hans Rosling The below video is the shorter version. Hans Roslings 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. 1.2.1.3 Social Media Social media contains a ton of great (and terrible examples of data science in action. These examples range from entire subreddits, such as /r/DataisBeautiful (be sure to check out the highest voted posts) to celebrity tweets about data scientists. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones  (@Lesdoggg) January 5, 2021 Good reasons to not be a Data Scientist:- It is a lot of work- Literally nobody will know what you&#39;re talking about- In the end, your computer will be your only real friend&mdash; Kareem Carr  (@kareem_carr) January 22, 2021 1.2.1.4 Read for yourselves! Link Preview What is Data Science @ Oreilly Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdiscplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: heres a lot of data, what can you make from it? What is Data Science @ Quora Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyse it, and find innovative solutions to difficult problems. Basically Data Science is all about Analysing data and driving for business growth by finding creative ways. The sexiest job of 21st century Data scientists today are akin to Wall Street quants of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed masters programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs. Wikipedia Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. How to Become a Data Scientist Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientists role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations. a very short history of #datascience The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young onecomputer science. The term Data Science has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term Data Science and its use, attempts to define it, and related terms. 1.3 Course structure and some other useful things You can follow along with the slides here if they do not appear below. 1.3.1 Activity 01 You can do either activity. The choice is yours. 1.3.1.1 UN Votes You can find the materials for the UN activity here. The compiled version should look something like the following 1.3.1.2 Covid Data You can find the materials for the Covid version of this activity here. The compiled version should look something like the following "],["meet-our-toolbox.html", "2 Meet our toolbox! 2.1 R and RStudio 2.2 Activity 02: Bechdel", " 2 Meet our toolbox! You can follow along with the slides here if they do not appear below. I recommend installing R, Rstudio, git, and github before starting activity 02 2.1 R and RStudio 2.1.1 Install R and RStudio library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 2.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. 2.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage 2.1.4 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual 2.2 Activity 02: Bechdel You can find the materials for the Bechdel activity here. The compiled version should look something like the following "],["thoughtful-workflow.html", "3 Thoughtful Workflow 3.1 R Markdown 3.2 Git and Github 3.3 Getting Help with R", " 3 Thoughtful Workflow At this point, I recommend you pause and think about your workflow. I give you permission to spend some time and energy sorting this out! It can be as or more important than learning a new R function or package. The experts dont talk about this much, because theyve already got a workflow; its something they do almost without thinking. Working through subsequent material in R Markdown documents, possibly using Git and GitHub to track and share your progress, is a great idea and will leave you more prepared for your future data analysis projects. Typing individual lines of R code is but a small part of data analysis and it pays off to think holistically about your workflow. If you want a lot more detail on workflows, you can wander over to the optional chapter on r basics and workflow. 3.1 R Markdown If you are in the mood to be entertained, start the video from the beginning. But if youd rather just get on with it, start watching at 6:52. You can follow along with the slides here if they do not appear below. R Markdown is a very accessible way to create computational documents that combine prose and tables and figures produced by R code. An introductory R Markdown workflow, including how it intersects with Git, GitHub, and RStudio, is now maintained within the Happy Git site: Test drive R Markdown 3.2 Git and Github First, its important to realize that Git and GitHub are distinct things. GitHub is an online hosting platform that provides an array of services built on top of the Git system. (Similar platforms include Bitbucket and GitLab.) Just like we dont need Rstudio to run R code, we dont need GitHub to use Git But, it will make our lives so much easier. I recommend checking out Jenny Bryans instructions around installation, setup, and early Git usage. Eventually, it grew so extensive that she created a dedicated website. This content can now be found here: https://happygitwithr.com You can follow along with the slides here if they do not appear below. 3.2.1 What is Github? 3.2.2 Git Git is a distributed version control system. (Wait, what?) Okay, try this: Imagine if Dropbox and the Track changes feature in MS Word had a baby. Git would be that baby. In fact, its even better than that because Git is optimized for the things that economists and data scientists spend a lot of time working on (e.g. code). There is a learning curve, but I promise you its worth it. Git and GitHubs role in global software development is not in question. - Theres a high probability that your favorite app, program or package is built using Git-based tools. (RStudio is a case in point.) Scientists and academic researchers are cottoning on too. Benefits of VC and collaboration tools aside, Git(Hub) helps to operationalize the ideals of open science and reproducibility. Journals have increasingly strict requirements regarding reproducibility and data access. GH makes this easy (DOI integration, off-the-shelf licenses, etc.). I run my entire lab on GH; this entire course is running on github; these lecture notes are github Nature: Democratic databases: science on GitHub (Perkel, 2016). 3.3 Getting Help with R "],["r-basics.html", "4 Optional Deep Dive: R basics and workflows 4.1 Basics of working with R at the command line and RStudio goodies 4.2 Workspace and working directory 4.3 RStudio projects 4.4 Stuff", " 4 Optional Deep Dive: R basics and workflows 4.1 Basics of working with R at the command line and RStudio goodies Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects  assignments  have this form: objectName &lt;- value and in my head I hear, e.g., x gets 12. You will make lots of assignments and the operator &lt;- is a pain to type. Dont be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudios keyboard shortcut: Alt + - (the minus sign). Notice that RStudio auto-magically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this, try out RStudios completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2 ^ 3 Lets try to inspect: masonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;masonrocks&#39; not found masn_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;masn_rocks&#39; not found Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Get better at typing. R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Lets try using seq() which makes regular sequences of numbers and, while were at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a functions arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Since we didnt specify step size, the default value of by in the function definition is used, which ends up being 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you dont get to see the value, so then youre tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and print to screen to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Mon Jan 25 22:36:29 2021&quot; Now look at your workspace  in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;install_quietly&quot; #&gt; [3] &quot;mason_rocks&quot; &quot;pretty_install&quot; #&gt; [5] &quot;shhh_check&quot; &quot;this_is_a_really_long_name&quot; #&gt; [7] &quot;x&quot; &quot;y&quot; #&gt; [9] &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;install_quietly&quot; #&gt; [3] &quot;mason_rocks&quot; &quot;pretty_install&quot; #&gt; [5] &quot;shhh_check&quot; &quot;this_is_a_really_long_name&quot; #&gt; [7] &quot;x&quot; &quot;y&quot; #&gt; [9] &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudios Environment pane. 4.2 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is real, i.e. will you save it as your lasting record of what happened? Where does your analysis live? 4.2.1 Workspace, .RData As a beginning R user, its OK to consider your workspace real. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as real. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as real (saving and reloading all the time), if you need to redo analysis  youre going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your good R code in a script for future reuse. Because it can be useful sometimes, note the commands youve recently run appear in the History pane. But you dont have to choose right now and the two strategies are not incompatible. Lets demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. Youll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click Save. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and youll see the same objects as before. In the History tab of the same pane, you should also see your command history. Youre back in business. This way of starting and stopping analytical work will not serve you well for long but its a start. 4.2.2 Working directory Any process running on your computer has a notion of its working directory. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, its OK let your home directory or any other weird directory on your computer be Rs working directory. Very soon, I urge you to evolve to the next level, where you organize your analytical projects into directories and, when working on project A, set Rs working directory to the associated directory. Although I do not recommend it, in case youre curious, you can set Rs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudios Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (Youll see even more options there). Or within the Files pane, choose More and Set As Working Directory. But theres a better way. A way that also puts you on the path to managing your R work like an expert. 4.3 RStudio projects Keeping all the files associated with a project organized together  input data, R scripts, analytical results, figures  is such a wise and common practice that RStudio has built-in support for this via its projects. Lets make one to use for the rest of this workshop/class. Do this: File &gt; New Project. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the home directory for your project is the working directory of our current R process: getwd() I cant print my output here because this document itself does not reside in the RStudio Project we just created. Lets enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.447 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Lets say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click To Source. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line  whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click Run in the upper right corner of editor pane). Source the entire document  equivalent to entering source('toy-line.r') in the Console  by keyboard shortcut (Shift+Command+S) or mouse (click Source in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 4.4 Stuff It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like youve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code youre saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (well soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (Im one of them), never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R and/or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This is rarely necessary when using RStudio projects properly. "],["lab01.html", "5 Lab: Hello R! 5.1 Lab Goals 5.2 Getting started 5.3 Exercises", " 5 Lab: Hello R! The labs for this course have been adapted from a series of Rstudio tutorials. These tutorials were created by Mine Çetinkaya-Rundel. Mine is fantastic; her work is fantastic; and shes just a badass! Plus, I think it useful to see other people working with R. Pragmatically, using Mines lab materials means that I can spend more time on other aspects of the course  like the website, course notes, videos, feedback, learning how to embed tweets That&#39;s so wonderful to hear, thank you!&mdash; Mine Çetinkaya-Rundel (@minebocek) January 22, 2021 Seriously, youd never know it but every hour of finished video takes between 4 and 6 hours to make. (2 hours of writing, 1.5 hours of filming, and 2.5 hours for video editing). 5.1 Lab Goals Recall: R is the name of the programming language itself and RStudio is a convenient interface. The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions. Recall: git is a version control system (like Track Changes features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the internet (like DropBox but much, much better). An additional goal is to introduce you to Git and GitHub, which is the collaboration and version control system that we will be using throughout the course. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands. To make versioning simpler, this lab is a solo lab. I want to make sure everyone gets a significant amount of time at the steering wheel, working directly with R. In the future modules,youll learn about collaborating on GitHub and produce a single lab report for your team. 5.2 Getting started Each of your assignments will begin with the following steps. Theyre outlined in detail here. Going forward, each lab will start with a Getting started section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment. You can find the assignment link for this lab right here. That GitHub repository (which well refer to as repo going forward) is a template for the assignment. You can build on it to complete your assignment. On GitHub, click on the green Clone or download button, select Use HTTPS (this might already be selected by default, and if it is, youll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option. Copy and paste the URL of your assignment repo into the dialog box: Hit OK, and youre good to go! 5.2.1 Warm up Before we introduce the data, lets warm up with some simple exercises. FYI: The top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for YAML Aint Markup Language. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document. 5.2.1.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 5.2.1.2 Committing changes Then go to the Git pane in your RStudio. If you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If youre happy with these changes, write Update author name in the Commit message box and hit Commit. You dont have to commit after every change, doing so would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments, Ill suggest exactly when to commit and in some cases, what commit message to use. As the semester progresses, you make these decisions. 5.2.1.3 Pushing changes Now that you have made an update and committed this change, its time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only). In order to push your changes to GitHub, click on Push. This will prompt a dialog box where you first need to enter your user name, and then your password. This might feel cumbersome. Soon  you will learn how to save your password so you dont have to enter it every time. But for this one assignment youll have to manually enter each time you push in order to gain some experience with it. 5.2.2 Packages In this lab, we will work with two packages: datasauRus and tidyverse. datasauRus contains the dataset well be using; tidyverse is a collection of packages for doing data analysis in a tidy way. Install these packages by running the following commands in the console. install.packages(&quot;tidyverse&quot;) install.packages(&quot;datasauRus&quot;) Now that the necessary packages are installed, you should be able to Knit your document and see the results. If youd like to run your code in the Console as well youll also need to load the packages there. To do so, run the following in the console. library(tidyverse) library(datasauRus) Note that the packages are also loaded with the same commands in your R Markdown document. 5.2.3 Data Fun fact: If its confusing that the data frame is called datasaurus_dozen when it contains 13 datasets, youre not alone! Have you heard of a bakers dozen? The data frame we will be working with today is called datasaurus_dozen and its in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable. To find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be ran in the Console. 5.3 Exercises Based on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. Lets take a look at what these datasets are. To do so we can make a frequency table of the dataset variable: datasaurus_dozen %&gt;% count(dataset) %&gt;% print(13) #&gt; # A tibble: 13 x 2 #&gt; dataset n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 away 142 #&gt; 2 bullseye 142 #&gt; 3 circle 142 #&gt; 4 dino 142 #&gt; 5 dots 142 #&gt; 6 h_lines 142 #&gt; 7 high_lines 142 #&gt; 8 slant_down 142 #&gt; 9 slant_up 142 #&gt; 10 star 142 #&gt; 11 v_lines 142 #&gt; 12 wide_lines 142 #&gt; 13 x_shape 142 Fun fact: Matejka, Justin, and George Fitzmaurice. Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017. The original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets that have the same summary statistics as the Datasaurus but have very different distributions.    Knit, commit, and push your changes to GitHub with the commit message Added answer for Ex 1. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset. Below is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results. Start with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data. dino_data &lt;- datasaurus_dozen %&gt;% filter(dataset == &quot;dino&quot;) Because a lot going on here  lets slow down and unpack it a bit. First, the pipe operator: %&gt;%, takes what comes before it and sends it as the first argument to what comes after it. So here, were saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\". Second, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame. Next, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data youre visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case, we want these to be points, hence geom_point. ggplot(data = dino_data, mapping = aes(x = x, y = y)) + geom_point() If this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in layer in detail next week. For now, follow along with the code that is provided. For the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. Recall: Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesnt make sense since the relationship between x and y is definitely not linear  its dinosaurial! But, for illustrative purposes, lets calculate the correlation coefficient between x and y. Tip: Start with dino_data and calculate a summary statistic that we will call r as the correlation between x and y. ```r dino_data %&gt;% summarize(r = cor(x, y)) #&gt; # A tibble: 1 x 1 #&gt; r #&gt; &lt;dbl&gt; #&gt; 1 -0.0645    Knit, commit, and push your changes to GitHub with the commit message Added answer for Ex 2. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?    This is another good place to pause, knit, commit changes with the commit message Added answer for Ex 3, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?    You should pause again, commit changes with the commit message Added answer for Ex 4, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Hint: Facet by the dataset variable, placing the plots in a 3 column grid, and dont add a legend. Finally, lets plot all datasets at once. In order to do this we will make use of faceting. ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+ geom_point()+ facet_wrap(~ dataset, ncol = 3) + theme(legend.position = &quot;none&quot;) And we can use the group_by function to generate all the summary correlation coefficients. datasaurus_dozen %&gt;% group_by(dataset) %&gt;% summarize(r = cor(x, y)) %&gt;% print(13) Youre done with the data analysis exercises, but wed like you to do two more things: Resize your figures: Click on the gear icon in on top of the R Markdown document, and select Output Options in the dropdown menu. In the pop up dialog box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until youre happy with the figure sizes. Note that these values get saved in the YAML. You can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well. Change the look of your report: Once again, click on the gear icon in on top of the R Markdown document, and select Output Options in the dropdown menu. In the General tab of the pop up dialog box, try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until youre happy with the look. Pro Tip: Not sure how to use emojis on your computer? Maybe a teammate can help?    Yay, youre done! Commit all remaining changes, use the commit message \"Done with Lab 1! \", and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo. "],["data-and-visualization.html", "6 Data and Visualization 6.1 Module Materials", " 6 Data and Visualization This module is designed to introduce you to graphical data analysis. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 6.1 Module Materials Slides Data and visualization Building plots for various data types Activities Star Wars! Lab Plastic waste You can follow along with the slides here if they do not appear below. "],["building-plots-for-various-data-types.html", "7 Building plots for various data types! 7.1 Activity 03: Star Wars!", " 7 Building plots for various data types! You can follow along with the slides here if they do not appear below. 7.1 Activity 03: Star Wars! You can find the materials for the Star Wars activity here. The compiled version should look something like the following "],["visualization-examples.html", "8 Visualization Examples 8.1 Census Reporter Data from North Carolina", " 8 Visualization Examples Here are some fun examples of data visualization. 8.1 Census Reporter Data from North Carolina "],["lab02.html", "9 Lab: Global plastic waste 9.1 Learning goals 9.2 Getting started 9.3 Warm up 9.4 Exercises 9.5 Wrapping up", " 9 Lab: Global plastic waste Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010. Additionally, National Geographic ran a data visualization communication contest on plastic waste as seen here. 9.1 Learning goals Visualizing numerical and categorical data and interpreting visualizations Recreating visualizations Getting more practice using with R, RStudio, Git, and GitHub 9.2 Getting started Go to the course GitHub organization and locate your assignment repo, which should be named lab-02-plastic-waste-YOUR_GITHUB_USERNAME. If youre in the right place, it should look like the following. Grab the URL of the repo, and clone it in RStudio. First, open the R Markdown document lab-02.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 9.2.1 Packages Well use the tidyverse package for this analysis. Run the following code in the Console to load this package. library(tidyverse) 9.2.2 Data The dataset for this assignment can be found as a csv file in the data folder of your repository. You can read it in using the following. plastic_waste &lt;- read_csv(&quot;data/plastic-waste.csv&quot;) The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder 9.3 Warm up Recall that RStudio is divided into four panes. Without looking, can you name them all and briefly describe their purpose? Verify that the dataset has loaded into the Environment. How many observations are in the dataset? Clicking on the dataset in the Environment will allow you to inspect it more carefully. Alternatively, you can type View(plastic_waste) into the Console to do this. Hint: If youre not sure, run the command ?NA which will lead you to the documentation. Have a quick look at the data and notice that there are cells taking the value NA  what does this mean? 9.4 Exercises Lets start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_histogram(binwidth = 0.2) ## Warning: Removed 51 rows containing non-finite values (stat_bin). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. plastic_waste %&gt;% filter(plastic_waste_per_cap &gt; 3.5) ## # A tibble: 1 x 10 ## code entity continent year gdp_per_cap plastic_waste_p~ mismanaged_plas~ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TTO Trini~ North Am~ 2010 31261. 3.6 0.19 ## # ... with 3 more variables: mismanaged_plastic_waste &lt;dbl&gt;, coastal_pop &lt;dbl&gt;, ## # total_pop &lt;dbl&gt; Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? NOTE: From this point onwards, the plots and the output of the code are not displayed in the lab instructions, but you can and should the code and view the results yourself. Another way of visualizing numerical data is using density plots. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_density() And compare distributions across continents by coloring density curves by continent. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent)) + geom_density() The resulting plot may be a little difficult to read, so lets also fill the curves in with colors as well. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density() The overlapping colors make it difficult to tell whats happening with the distributions in continents plotted first, and hence covered by continents plotted over them. We can change the transparency level of the fill color to help with this. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so you just need to try a few. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density(alpha = 0.7) This still doesnt look great Recreate the density plots above using a different (lower) alpha level that works better for displaying the density curves for all continents. Describe why we defined the color and fill of the curves by mapping aesthetics of the plot but we defined the alpha level as a characteristic of the plotting geom.    Now is a good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. And yet another way to visualize this relationship is using side-by-side box plots. ggplot(data = plastic_waste, mapping = aes(x = continent, y = plastic_waste_per_cap)) + geom_boxplot() Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? Remember: We use geom_point() to make scatterplots. Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?    Now is another good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 9.5 Wrapping up If you still have some time left, move on to the remaining exercises below. If not, you should find a time to meet with your team and complete them after the workshop. If you havent had time to finish the exercises above, please ask for help before you leave! Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the data are not only represented with points on the plot but also a smooth curve. The term smooth should help you pick which geom to use. Recreate the following plot, and interpret what you see in context of the data.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure youre happy with the final state of your work. Once youre done, check to make sure your latest changes are on GitHub and that you have a green indicator for the automated check for your R Markdown document knitting. "],["basic-data-care.html", "10 Basic care and feeding of data in R 10.1 Buckle your seatbelt 10.2 Data frames are awesome 10.3 Get the Gapminder data 10.4 Meet the gapminder data frame or tibble 10.5 Look at the variables inside a data frame 10.6 Recap", " 10 Basic care and feeding of data in R 10.1 Buckle your seatbelt Ignore if you dont need this bit of support. Now is the time to make sure you are working in an appropriate directory on your computer, probably through the use of an RStudio project. Enter getwd() in the Console to see current working directory or, in RStudio, this is displayed in the bar at the top of Console. You should clean out your workspace. In RStudio, click on the Clear broom icon from the Environment tab or use Session &gt; Clear Workspace. You can also enter rm(list = ls()) in the Console to accomplish same. Now restart R. This will ensure you dont have any packages loaded from previous calls to library(). In RStudio, use Session &gt; Restart R. Otherwise, quit R with q() and re-launch it. Why do we do this? So that the code you write is complete and re-runnable. If you return to a clean slate often, you will root out hidden dependencies where one snippet of code only works because it relies on objects created by code saved elsewhere or, much worse, never saved at all. Similarly, an aggressive clean slate approach will expose any usage of packages that have not been explicitly loaded. Finally, open a new R script and develop and run your code from there. In RStudio, use File &gt; New File &gt; R Script. Save this script with a name ending in .r or .R, containing no spaces or other funny stuff, and that evokes whatever it is were doing today. Example: cm004_data-care-feeding.r. Another great idea is to do this in an R Markdown document. See Test drive R Markdown for a refresher. 10.2 Data frames are awesome Whenever you have rectangular, spreadsheet-y data, your default data receptacle in R is a data frame. Do not depart from this without good reason. Data frames are awesome because Data frames package related variables neatly together, keeping them in sync vis-a-vis row order applying any filtering of observations uniformly Most functions for inference, modelling, and graphing are happy to be passed a data frame via a data = argument. This has been true in base R for a long time. The set of packages known as the tidyverse takes this one step further and explicitly prioritizes the processing of data frames. This includes popular packages like dplyr and ggplot2. In fact the tidyverse prioritizes a special flavor of data frame, called a tibble. Data frames  unlike general arrays or, specifically, matrices in R  can hold variables of different flavors, such as character data (subject ID or name), quantitative data (white blood cell count), and categorical information (treated vs. untreated). If you use homogeneous structures, like matrices, for data analysis, you are likely to make the terrible mistake of spreading a dataset out over multiple, unlinked objects. Why? Because you cant put character data, such as subject name, into the numeric matrix that holds white blood cell count. This fragmentation is a Bad Idea. 10.3 Get the Gapminder data We will work with some of the data from the Gapminder project. Ive released this as an R package, so we can install it from CRAN like so: install.packages(&quot;gapminder&quot;) Now load the package: library(gapminder) 10.4 Meet the gapminder data frame or tibble By loading the gapminder package, we now have access to a data frame by the same name. Get an overview of this with str(), which displays the structure of an object. str(gapminder) #&gt; tibble [1,704 x 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just str() some of the recently created objects to get some ideas about what to do next. We could print the gapminder object itself to screen. However, if youve used R before, you might be reluctant to do this, because large datasets just fill up your Console and provide very little insight. This is the first big win for tibbles. The tidyverse offers a special case of Rs default data frame: the tibble, which is a nod to the actual class of these objects, tbl_df. If you have not already done so, install the tidyverse meta-package now: install.packages(&quot;tidyverse&quot;) Now load it: library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v ggplot2 3.3.3 v purrr 0.3.4 #&gt; v tibble 3.0.5 v dplyr 1.0.2 #&gt; v tidyr 1.1.2 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.0 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() Now we can boldly print gapminder to screen! It is a tibble (and also a regular data frame) and the tidyverse provides a nice print method that shows the most important stuff and doesnt fill up your Console. ## see? it&#39;s still a regular data frame, but also a tibble class(gapminder) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 1,694 more rows If you are dealing with plain vanilla data frames, you can rein in data frame printing explicitly with head() and tail(). Or turn it into a tibble with as_tibble()! head(gapminder) #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder) #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zimbabwe Africa 1982 60.4 7636524 789. #&gt; 2 Zimbabwe Africa 1987 62.4 9216418 706. #&gt; 3 Zimbabwe Africa 1992 60.4 10704340 693. #&gt; 4 Zimbabwe Africa 1997 46.8 11404948 792. #&gt; 5 Zimbabwe Africa 2002 40.0 11926563 672. #&gt; 6 Zimbabwe Africa 2007 43.5 12311143 470. as_tibble(iris) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # ... with 140 more rows More ways to query basic info on a data frame: names(gapminder) #&gt; [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; ncol(gapminder) #&gt; [1] 6 length(gapminder) #&gt; [1] 6 dim(gapminder) #&gt; [1] 1704 6 nrow(gapminder) #&gt; [1] 1704 A statistical overview can be obtained with summary(): summary(gapminder) #&gt; country continent year lifeExp #&gt; Afghanistan: 12 Africa :624 Min. :1952 Min. :23.6 #&gt; Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.2 #&gt; Algeria : 12 Asia :396 Median :1980 Median :60.7 #&gt; Angola : 12 Europe :360 Mean :1980 Mean :59.5 #&gt; Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.8 #&gt; Australia : 12 Max. :2007 Max. :82.6 #&gt; (Other) :1632 #&gt; pop gdpPercap #&gt; Min. :6.00e+04 Min. : 241 #&gt; 1st Qu.:2.79e+06 1st Qu.: 1202 #&gt; Median :7.02e+06 Median : 3532 #&gt; Mean :2.96e+07 Mean : 7215 #&gt; 3rd Qu.:1.96e+07 3rd Qu.: 9325 #&gt; Max. :1.32e+09 Max. :113523 #&gt; Although we havent begun our formal coverage of visualization yet, its so important for smell-testing dataset that we will make a few figures anyway. Here we use only base R graphics, which are very basic. plot(lifeExp ~ year, gapminder) plot(lifeExp ~ gdpPercap, gapminder) plot(lifeExp ~ log(gdpPercap), gapminder) Lets go back to the result of str() to talk about what a data frame is. str(gapminder) #&gt; tibble [1,704 x 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... A data frame is a special case of a list, which is used in R to hold just about anything. Data frames are a special case where the length of each list component is the same. Data frames are superior to matrices in R because they can hold vectors of different flavors, e.g. numeric, character, and categorical data can be stored together. This comes up a lot! 10.5 Look at the variables inside a data frame To specify a single variable from a data frame, use the dollar sign $. Lets explore the numeric variable for life expectancy. head(gapminder$lifeExp) #&gt; [1] 28.8 30.3 32.0 34.0 36.1 38.4 summary(gapminder$lifeExp) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 23.6 48.2 60.7 59.5 70.8 82.6 hist(gapminder$lifeExp) The year variable is an integer variable, but since there are so few unique values it also functions a bit like a categorical variable. summary(gapminder$year) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1952 1966 1980 1980 1993 2007 table(gapminder$year) #&gt; #&gt; 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 #&gt; 142 142 142 142 142 142 142 142 142 142 142 142 The variables for country and continent hold truly categorical information, which is stored as a factor in R. class(gapminder$continent) #&gt; [1] &quot;factor&quot; summary(gapminder$continent) #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 levels(gapminder$continent) #&gt; [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) #&gt; [1] 5 The levels of the factor continent are Africa, Americas, etc. and this is whats usually presented to your eyeballs by R. In general, the levels are friendly human-readable character strings, like male/female and control/treated. But never ever ever forget that, under the hood, R is really storing integer codes 1, 2, 3, etc. Look at the result from str(gapminder$continent) if you are skeptical. str(gapminder$continent) #&gt; Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... This Janus-like nature of factors means they are rich with booby traps for the unsuspecting but they are a necessary evil. I recommend you resolve to learn how to properly care and feed for factors. The pros far outweigh the cons. Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Here we count how many observations are associated with each continent and, as usual, try to portray that info visually. This makes it much easier to quickly see that African countries are well represented in this dataset. table(gapminder$continent) #&gt; #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 barplot(table(gapminder$continent)) In the figures below, we see how factors can be put to work in figures. The continent factor is easily mapped into facets or colors and a legend by the ggplot2 package. Making figures with ggplot2 is covered in Chapter ?? so feel free to just sit back and enjoy these plots or blindly copy/paste. ## we exploit the fact that ggplot2 was installed and loaded via the tidyverse p &lt;- ggplot(filter(gapminder, continent != &quot;Oceania&quot;), aes(x = gdpPercap, y = lifeExp)) # just initializes p &lt;- p + scale_x_log10() # log the x axis the right way p + geom_point() # scatterplot p + geom_point(aes(color = continent)) # map continent to color p + geom_point(alpha = (1/3), size = 3) + geom_smooth(lwd = 3, se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; p + geom_point(alpha = (1/3), size = 3) + facet_wrap(~ continent) + geom_smooth(lwd = 1.5, se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 10.6 Recap Use data frames!!! Use the tidyverse!!! This will provide a special type of data frame called a tibble that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Always understand the basic extent of your data frames: number of rows and columns. Understand what flavor the variables are. Use factors!!! But with intention and care. Do basic statistical and visual sanity checking of each variable. Refer to variables by name, e.g., gapminder$lifeExp, not by column number. Your code will be more robust and readable. "],["grammar-of-data-wrangling.html", "11 Grammar of data wrangling 11.1 Module Materials 11.2 Tidy data and data wrangling! 11.3 Building plots for various data types!", " 11 Grammar of data wrangling This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-03]. Most of the slides used to make the videos in this module can be found in the slides repo. 11.1 Module Materials Slides Tidy data and data wrangling Joining data from multiple sources Data tidying and reshaping &lt;! Activities Star Wars! &gt; Lab 302-nobel-laureates 11.2 Tidy data and data wrangling! You can follow along with the slides here) if they do not appear below. 11.3 Building plots for various data types! You can follow along with the slides here) if they do not appear below. "],["lab03.html", "12 Lab: Nobel laureates 12.1 Learning goals 12.2 Lab prep 12.3 Merge conflict activity 12.4 Getting started 12.5 Exercises 12.6 But of those US-based Nobel laureates, many were born in other countries 12.7 Interested in how Buzzfeed made their visualizations?", " 12 Lab: Nobel laureates In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions. 12.1 Learning goals Collaborating on GitHub and resolving merge conflicts Replicating published results Data wrangling and visualisation 12.2 Lab prep You have two tasks you should complete before the lab: Task 1: Read the Buzzfeed article titled These Nobel Prize Winners Show Why Immigration Is So Important For American Science. We will replicate this analysis in the workshop so its crucial that youre familiar with it ahead of time. Task 2: Read about merge conflicts below. The merge conflict exercise well start with during the lab will assume that you have this background information. 12.2.1 Merges and merge conflicts This is the second week youre working in teams, so were going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes youll run into merge conflicts. So our first task today is to walk you through a merge conflict! Pushing to a repo replaces the code on GitHub with the code you have on your computer. If a collaborator has made a change to your repo on GitHub that you havent incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborators work! So you need to explicitly merge your collaborators work before you can push. If your and your collaborators changes are in different files or in different parts of the same file, git merges the work for you automatically when you *pull*. If you both changed the same part of a file, git will produce a **merge conflict** because it doesnt know how which change you want to keep and which change you want to overwrite. Git will put conflict markers in your code that look like: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD See also: [dplyr documentation](https://dplyr.tidyverse.org/) ======= See also [ggplot2 documentation](https://ggplot2.tidyverse.org/) &gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4 The ===s separate your changes (top) from their changes (bottom). Note that on top you see the word HEAD, which indicates that these are your changes. And at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f). This is the hash (a unique identifier) of the commit your collaborator made with the conflicting change. Your job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then you can stage and commit the result. 12.3 Merge conflict activity 12.3.1 Setup Clone the repo and open the .Rmd file. Assign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers. 12.3.2 Lets cause a merge conflict! Our goal is to see two different types of merges: first well see a type of merge that git cant figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention. Doing this will require some tight choreography, so pay attention! Take turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you wont be able to resist the urge to touch your computer when its not your turn, we recommend putting your hands in your pockets or sitting on them! Before starting: everyone should have the repo cloned and know which role number(s) they are. Role 1: Change the team name to your actual team name. Knit, commit, push.  Make sure the previous role has finished before moving on to the next step. Role 2: Change the team name to some other word. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by editing the document to choose the correct/preferred change. Knit. Click the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes. Commit and push.  Make sure the previous role has finished before moving on to the next step. Role 3: Change the a label of the first code chunk Knit, commit, push. You should get an error. Pull. No merge conflicts should occur, but you should see a message about merging. Now push.  Make sure the previous role has finished before moving on to the next step. Role 4: Change the label of the first code chunk to something other than previous role did. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.  Make sure the previous role has finished before moving on to the next step. Everyone: Pull, and observe the changes in your document. 12.3.3 Tips for collaborating via GitHub Always pull first before you start working. Resolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict. Knit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve. If you find yourself in a situation that is difficult to resolve, ask questions ASAP. Dont let it linger and get bigger. 12.4 Getting started Go to the course GitHub organization and locate your lab repo, which should be named lab-03-nobel-laureates-YOUR_GITHUB_USERNAME. Grab the URL of the repo, and clone it in RStudio. First, open the R Markdown document lab-03.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 12.4.1 Warm up Before we introduce the data, lets warm up with some simple exercises. Update the YAML, changing the author name to your name, and knit the document. Commit your changes with a meaningful commit message. Push your changes to GitHub. Go to your repo on GitHub and confirm that your changes are visible in your Rmd and md files. If anything is missing, commit and push again. 12.4.2 Packages Well use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console: library(tidyverse) 12.4.3 Data The dataset for this assignment can be found as a CSv (comma separated values) file in the data folder of your repository. You can read it in using the following. nobel &lt;- read_csv(&quot;data/nobel.csv&quot;) The variable descriptions are as follows: id: ID number firstname: First name of laureate surname: Surname year: Year prize won category: Category of prize affiliation: Affiliation of laureate city: City of laureate in prize year country: Country of laureate in prize year born_date: Birth date of laureate died_date: Death date of laureate gender: Gender of laureate born_city: City where laureate was born born_country: Country where laureate was born born_country_code: Code of country where laureate was born died_city: City where laureate died died_country: Country where laureate died died_country_code: Code of country where laureate died overall_motivation: Overall motivation for recognition share: Number of other winners award is shared with motivation: Motivation for recognition In a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`. born_country_original: Original country where laureate was born born_city_original: Original city where laureate was born died_country_original: Original country where laureate died died_city_original: Original city where laureate died city_original: Original city where laureate lived at the time of winning the award country_original: Original country where laureate lived at the time of winning the award 12.5 Exercises Take turns answering the exercises. Make sure each team member gets to commit to the repo by the time you submit your work. And make sure that the person taking the lead for an exercise is sharing their screen. You dont have to switch at each exercise, you can find your a cadence that works for your team and stick to it. 12.5.1 Get to know your data How many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent? There are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results. Create a new data frame called nobel_living that filters for laureates for whom country is available laureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender) laureates who are still alive (their died_date is NA) Confirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 12.5.2 Most living Nobel laureates were based in the US when they won their prizes  says the Buzzfeed article. Lets see if thats true. First, well create a new variable to identify whether the laureate was in the US when they won their prize. Well use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function were using to write this if statement is the condition were testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\". Note: We can achieve the same result using the fct_other() function weve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R. nobel_living &lt;- nobel_living %&gt;% mutate( country_us = if_else(country == &quot;USA&quot;, &quot;USA&quot;, &quot;Other&quot;) ) Next, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics. nobel_living_science &lt;- nobel_living %&gt;% filter(category %in% c(&quot;Physics&quot;, &quot;Medicine&quot;, &quot;Chemistry&quot;, &quot;Economics&quot;)) For the next exercise work with the nobel_living_science data frame you created above. This means youll need to define this data frame in your R Markdown document, even though the next exercise doesnt explicitly ask you to do so. Create a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data. Your visualization should be faceted by category. For each facet you should have two bars, one for winners in the US and one for Other. Flip the coordinates so the bars are horizontal, not vertical.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.d 12.6 But of those US-based Nobel laureates, many were born in other countries Hint: You should be able to cheat borrow from code you used earlier to create the country_us variable. Create a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US? Add a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeeds claim? Explain your reasoning in 1-2 sentences. Your final visualization should contain a facet for each category. Within each facet, there should be a bar for whether the laureate won the award in the US or not. Each bar should have segments for whether the laureate was born in the US or not.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 12.6.1 Heres where those immigrant Nobelists were born Note: Your bar plot wont exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published. In a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure youre happy with the final state of your work. Now go back through your write up to make sure youve answered all questions and all of your R chunks are properly labeled. Once you decide as a team that youre done with this lab, all members of the team should pull the changes and knit the R Markdown document to confirm that they can reproduce the report. 12.7 Interested in how Buzzfeed made their visualizations? The plots in the Buzzfeed article are called waffle plots. You can find the code used for making these plots in Buzzfeeds GitHub repo (yes, they have one!) here. Youre not expected to recreate them as part of your assignment, but youre welcomed to do so for fun! "],["data-types-and-data-transformations.html", "13 Data types and Data Transformations 13.1 Module Materials 13.2 Topic 1! 13.3 Topic 2!", " 13 Data types and Data Transformations This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 13.1 Module Materials Slides Data types and recoding Joining data from multiple sources &gt; Activities Lab Visualizing spatial data 13.2 Topic 1! You can follow along with the slides here if they do not appear below. 13.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab04.html", "14 Lab: Visualizing spatial data 14.1 La Quinta is Spanish for next to Dennys, Pt. 1 14.2 Getting started 14.3 Housekeeping 14.4 The data 14.5 Exercises", " 14 Lab: Visualizing spatial data 14.1 La Quinta is Spanish for next to Dennys, Pt. 1 Have you ever taken a road trip in the US and thought to yourself I wonder what La Quinta means. Well, the late comedian Mitch Hedberg thinks its Spanish for next to Dennys. If youre not familiar with these two establishments, Dennys is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain. These two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data. The inspiration for this lab comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reisers blog post focuses on scraping data from Dennys and La Quinta Inn and Suites websites using Python. In this lab we focus on visualization and analysis of these data. However note that the data scraping was also done in R, and we we will discuss web scraping using R later in the course. But for now we focus on the data that has already been scraped and tidied for you. 14.2 Getting started 14.2.1 Packages In this lab we will use the tidyverse and dsbox packages. library(tidyverse) library(dsbox) 14.3 Housekeeping 14.3.1 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 03 - Visualizing spatial data. 14.3.2 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 14.3.3 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 14.3.4 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 14.3.5 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 14.4 The data The datasets well use are called dennys and laquinta from the dsbox package. Note that these data were scraped from here and here, respectively. To help with our analysis we will also use a dataset on US states: states &lt;- read_csv(&quot;data/states.csv&quot;) Each observation in this dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles). 14.5 Exercises What are the dimensions of the Dennys dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables? What are the dimensions of the La Quintas dataset? What does each row in the dataset represent? What are the variables? We would like to limit our analysis to Dennys and La Quinta locations in the United States. Take a look at the websites that the data come from (linked above). Are there any La Quintas locations outside of the US? If so, which countries? What about Dennys? Now take a look at the data. What would be some ways of determining whether or not either establishment has any locations outside the US using just the data (and not the websites). Dont worry about whether you know how to implement this, just brainstorm some ideas. Write down at least one as your answer, but youre welcomed to write down a few options too. We will determine whether or not the establishment has a location outside the US using the state variable in the dn and lq datasets. We know exactly which states are in the US, and we have this information in the states dataframe we loaded. Find the Dennys locations that are outside the US, if any. To do so, filter the Dennys locations for observations where state is not in states$abbreviation. The code for this is given below. Note that the %in% operator matches the states listed in the state variable to those listed in states$abbreviation. The ! operator means not. Are there any Dennys locations outside the US? &quot;Filter for `state`s that are not in `states$abbreviation`.&quot; dn %&gt;% filter(!(state %in% states$abbreviation)) Add a country variable to the Dennys dataset and set all observations equal to \"United States\". Remember, you can use the mutate function for adding a variable. Make sure to save the result of this as dn again so that the stored data frame contains the new variable going forward. We don&#39;t need to tell R how many times to repeat the character string &quot;United States&quot; to fill in the data for all observations, R takes care of that automatically. dn %&gt;% mutate(country = &quot;United States&quot;) Find the La Quinta locations that are outside the US, and figure out which country they are in. This might require some googling. Take notes, you will need to use this information in the next exercise. Add a country variable to the La Quinta dataset. Use the case_when function to populate this variable. Youll need to refer to your notes from Exercise 7 about which country the non-US locations are in. Here is some starter code to get you going: lq %&gt;% mutate(country = case_when( state %in% state.abb ~ &quot;United States&quot;, state %in% c(&quot;ON&quot;, &quot;BC&quot;) ~ &quot;Canada&quot;, state == &quot;ANT&quot; ~ &quot;Colombia&quot;, ... # fill in the rest )) Going forward we will work with the data from the United States only. All Dennys locations are in the United States, so we dont need to worry about them. However we do need to filter the La Quinta dataset for locations in United States. lq &lt;- lq %&gt;% filter(country == &quot;United States&quot;) Which states have the most and fewest Dennys locations? What about La Quinta? Is this surprising? Why or why not? Next, lets calculate which states have the most Dennys locations per thousand square miles. This requires joinining information from the frequency tables you created in Exercise 8 with information from the states data frame. First, we count how many observations are in each state, which will give us a data frame with two variables: state and n. Then, we join this data frame with the states data frame. However note that the variables in the states data frame that has the two-letter abbreviations is called abbreviation. So when were joining the two data frames we specify that the state variable from the Dennys data should be matched by the abbreviation variable from the states data: dn %&gt;% count(state) %&gt;% inner_join(states, by = c(&quot;state&quot; = &quot;abbreviation&quot;)) Before you move on the the next question, run the code above and take a look at the output. In the next exercise you will need to build on this pipe. Which states have the most Dennys locations per thousand square miles? What about La Quinta? Next, we put the two datasets together into a single data frame. However before we do so, we need to add an identifier variable. Well call this establishment and set the value to \"Denny's\" and \"La Quinta\" for the dn and lq data frames, respectively. dn &lt;- dn %&gt;% mutate(establishment = &quot;Denny&#39;s&quot;) lq &lt;- lq %&gt;% mutate(establishment = &quot;La Quinta&quot;) Since the two data frames have the same columns, we can easily bind them with the bind_rows function: dn_lq &lt;- bind_rows(dn, lq) We can plot the locations of the two establishments using a scatter plot, and color the points by the establishment type. Note that the latitude is plotted on the x-axis and the longitude on the y-axis. ggplot(dn_lq, mapping = aes(x = longitude, y = latitude, color = establishment)) + geom_point() The following two questions ask you to create visualizations. These should follow best practices you learned in class, such as informative titles, axis labels, etc. See http://ggplot2.tidyverse.org/reference/labs.html for help with the syntax. You can also choose different themes to change the overall look of your plots, see http://ggplot2.tidyverse.org/reference/ggtheme.html for help with these. Filter the data for observations in North Carolina only, and recreate the plot. You should also adjust the transparency of the points, by setting the alpha level, so that its easier to see the overplotted ones. Visually, does Mitch Hedbergs joke appear to hold here? Now filter the data for observations in Texas only, and recreate the plot, with an appropriate alpha level. Visually, does Mitch Hedbergs joke appear to hold here? Thats it for now! In the next lab we will take a more quantitative approach to answering these questions. "],["tips-for-effective-data-visualization.html", "15 Tips for effective data visualization 15.1 Module Materials 15.2 Topic 1! 15.3 Topic 2!", " 15 Tips for effective data visualization This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 15.1 Module Materials Slides Activities Lab Lab 15.2 Topic 1! You can follow along with the slides here if they do not appear below. 15.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab05.html", "16 Lab: Wrangling spatial data 16.1 La Quinta is Spanish for next to Dennys, Pt. 2\" 16.2 Getting started 16.3 Warm up 16.4 The data 16.5 Exercises", " 16 Lab: Wrangling spatial data 16.1 La Quinta is Spanish for next to Dennys, Pt. 2\" In this lab, we revisit the Dennys and La Quinta Inn and Suites data we visualized in the previous lab. 16.2 Getting started Go to the course organization on GitHub. Find your lab repo. In the repo, click on the green Clone or download button, select Use HTTPS (this might already be selected by default, and if it is, youll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio Cloud and into the course workspace. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option. Copy and paste the URL of your assignment repo into the dialog box: Hit OK, and youre good to go! 16.2.1 Packages In this lab we will use the tidyverse and dsbox packages. library(tidyverse) library(dsbox) 16.2.2 Housekeeping 16.2.2.1 Password caching If you would like your git password cached for a week for this project, type the following in the Terminal: git config --global credential.helper &#39;cache --timeout 604800&#39; 16.2.2.2 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 04 - Wrangling spatial data. 16.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 16.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 16.3.2 Commiting and pushing changes: Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 16.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 16.4 The data The datasets well use are called dennys and laquinta from the dsbox package. 16.5 Exercises Filter the Dennys dataframe for Alaska (AK) and save the result as dn_ak. How many Dennys locations are there in Alaska? dn_ak &lt;- dn %&gt;% filter(state == &quot;AK&quot;) nrow(dn_ak) Filter the La Quinta dataframe for Alaska (AK) and save the result as lq_ak. How many La Quinta locations are there in Alaska? lq_ak &lt;- lq %&gt;% filter(state == &quot;AK&quot;) nrow(lq_ak) Next well calculate the distance between all Dennys and all La Quinta locations in Alaska. Lets take this step by step: Step 1: There are 3 Dennys and 2 La Quinta locations in Alaska. (If you answered differently above, you might want to recheck your answers.) Step 2: Lets focus on the first Dennys location. Well need to calculate two distances for it: (1) distance between Dennys 1 and La Quinta 1 and (2) distance between Dennys 1 and La Quinta (2). Step 3: Now lets consider all Dennys locations. How many pairings are there between all Dennys and all La Quinta locations in Alaska, i.e. how many distances do we need to calculate between the locations of these establishments in Alaska? In order to calculate these distances we need to first restructure our data to pair the Dennys and La Quinta locations. To do so, we will join the two data frames. We have six join options in R. Each of these join functions take at least three arguments: x, y, and by. x and y are data frames to join by is the variable(s) to join by Four of these join functions combine variables from the two data frames: These are called **mutating joins**. inner_join(): return all rows from x where there are matching values in y, and all columns from x and y. left_join(): return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. right_join(): return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. full_join(): return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. And the other two join functions only keep cases from the left-hand data frame, and are called filtering joins. Well learn about these another time but you can find out more about the join functions in the help files for any one of them, e.g. ?full_join. In practice we mostly use mutating joins. In this case we want to keep all rows and columns from both dn_ak and lq_ak data frames. So we will use a full_join. Full join of Dennys and La Quinta locations in AK Lets join the data on Dennys and La Quinta locations in Alaska, and take a look at what it looks like: dn_lq_ak &lt;- full_join(dn_ak, lq_ak, by = &quot;state&quot;) dn_lq_ak How many observations are in the joined dn_lq_ak data frame? What are the names of the variables in this data frame. .x in the variable names means the variable comes from the x data frame (the first argument in the full_join call, i.e. dn_ak), and .y means the variable comes from the y data frame. These varibles are renamed to include .x and .y because the two data frames have the same variables and its not possible to have two variables in a data frame with the exact same name. Now that we have the data in the format we wanted, all that is left is to calculate the distances between the pairs. What function from the tidyverse do we use the add a new variable to a data frame while keeping the existing variables? One way of calculating the distance between any two points on the earth is to use the Haversine distance formula. This formula takes into account the fact that the earth is not flat, but instead spherical. This function is not available in R, but we have it saved in a file called haversine.R that we can load and then use: haversine &lt;- function(long1, lat1, long2, lat2, round = 3) { # convert to radians long1 = long1 * pi / 180 lat1 = lat1 * pi / 180 long2 = long2 * pi / 180 lat2 = lat2 * pi / 180 R = 6371 # Earth mean radius in km a = sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((long2 - long1)/2)^2 d = R * 2 * asin(sqrt(a)) return( round(d,round) ) # distance in km } This function takes five arguments: Longitude and latitude of the first location Longitude and latitude of the second location A parameter by which to round the responses Calculate the distances between all pairs of Dennys and La Quinta locations and save this variable as distance. Make sure to save this variable in THE dn_lq_ak data frame so that you can use it later. Calculate the minimum distance between a Dennys and La Quinta for each Dennys location. To do so we group by Dennys locations and calculate a new variable that stores the information for the minimum distance. dn_lq_ak_mindist &lt;- dn_lq_ak %&gt;% group_by(address.x) %&gt;% summarise(closest = min(distance)) Describe the distribution of the distances Dennys and the nearest La Quinta locations in Alaska. Also include an appripriate visualization and relevant summary statistics. Repeat the same analysis for North Carolina: (i) filter Dennys and La Quinta Data Frames for NC, (ii) join these data frames to get a completelist of all possible pairings, (iii) calculate the distances between all possible pairings of Dennys and La Quinta in NC, (iv) find the minimum distance between each Dennys and La Quinta location, (v) visualize and describe the distribution of these shortest distances using appropriate summary statistics. Repeat the same analysis for Texas. Repeat the same analysis for a state of your choosing, different than the ones we covered so far. Among the states you examined, where is Mitch Hedbergs joke most likely to hold true? Explain your reasoning. "],["scientific-studies-and-confounding.html", "17 Scientific studies and confounding 17.1 Module Materials 17.2 Topic 1! 17.3 Topic 2!", " 17 Scientific studies and confounding This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-06]. Most of the slides used to make the videos in this module can be found in the slides repo. 17.1 Module Materials Slides Activities Lab Lab 17.2 Topic 1! You can follow along with the slides here if they do not appear below. 17.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab06.html", "18 Lab: Ugly charts 18.1 Getting started 18.2 Merges and merge conflicts 18.3 Packages 18.4 Take a sad plot and make it better 18.5 Wrapping up 18.6 More ugly charts", " 18 Lab: Ugly charts Given below are two data visualizations that violate many data visualization best practices. Improve these visualizations using R and the tips for effective visualizations that we introduced in class. You should produce one visualization per dataset. Your visualization should be accompanied by a brief paragraph describing the choices you made in your improvement, specifically discussing what you didnt like in the original plots and why, and how you addressed them in the visualization you created. On the due date you will give a brief presentation describing one of your improved visualizations and the reasoning for the choices you made. The learning goals for this lab are: Telling a story with data Data visualization best practices Reshaping data 18.1 Getting started Go to the course GitHub organization and locate your lab repo. Grab the URL of the repo, and clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 18.1.1 Housekeeping Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to do some configuration so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. Run the following (but update it for your name and email!) in the Console to configure git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 18.1.2 Workflow This is the second week youre working in teams, so were going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes youll run into merge conflicts. So our first task today is to walk you through a merge conflict! 18.2 Merges and merge conflicts Pushing to a repo replaces the code on GitHub with the code you have on your computer. If a collaborator has made a change to your repo on GitHub that you havent incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborators work! So you need to explicitly merge your collaborators work before you can push. If your and your collaborators changes are in different files or in different parts of the same file, git merges the work for you automatically when you pull. If you both changed the same part of a file, git will produce a merge conflict because it doesnt know how which change you want to keep and which change you want to overwrite. Git will put conflict markers in your code that look like: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD See also: [dplyr documentation](https://dplyr.tidyverse.org/) ======= See also [ggplot2 documentation](https://ggplot2.tidyverse.org/) &gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4 The ===s separate your changes (top) from their changes (bottom). Note that on top you see the word HEAD, which indicates that these are your changes. And at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f). This is the hash (a unique identifier) of the commit your collaborator made with the conflicting change. Your job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then Stage and Commit the result. 18.2.1 Setup Clone the repo and open the .Rmd file. Assign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers. 18.2.2 Lets cause a merge conflict! Our goal is to see two different types of merges: first well see a type of merge that git cant figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention. Doing this will require some tight choreography, so pay attention! Take turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you wont be able to resist the urge to touch your computer when its not your turn, we recommend putting your hands in your pockets or sitting on them! Before starting: everyone should have the repo cloned and know which role number(s) they are. Role 1: Change the team name to your actual team name. Knit, commit, push.  Wait for instructions before moving on to the next step. Role 2: Change the team name to some other word. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by editing the document to choose the correct/preferred change. Knit. Click the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes. Commit and push.  Wait for instructions before moving on to the next step. Role 3: Add a label to the first code chunk Knit, commit, push. You should get an error. Pull. No merge conflicts should occur, but you should see a message about merging. Now push.  Wait for instructions before moving on to the next step. Role 4: Add a different label to the first code chunk. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.  Wait for instructions before moving on to the next step. Everyone: Pull, and observe the changes in your document. 18.2.3 Tips for collaborating via GitHub Always pull first before you start working. Resolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict. Knit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve. If you find yourself in a situation that is difficult to resolve, ask questions ASAP. Dont let it linger and get bigger. 18.3 Packages Run the following code in the Console to load this package. library(tidyverse) 18.4 Take a sad plot and make it better 18.4.1 Instructional staff employment trends The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report compiled by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below. Lets start by loading the data used to create this plot. staff &lt;- read_csv(&quot;data/instructional-staff.csv&quot;) Each row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year. ## # A tibble: 5 x 12 ## faculty_type `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full-Time T~ 29 27.6 25 24.8 21.8 20.3 19.3 17.8 17.2 ## 2 Full-Time T~ 16.1 11.4 10.2 9.6 8.9 9.2 8.8 8.2 8 ## 3 Full-Time N~ 10.3 14.1 13.6 13.6 15.2 15.5 15 14.8 14.9 ## 4 Part-Time F~ 24 30.4 33.1 33.2 35.5 36 37 39.3 40.5 ## 5 Graduate St~ 20.5 16.5 18.1 18.8 18.7 19 20 19.9 19.5 ## # ... with 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt; In order to recreate this visualization we need to first reshape the data to have one variable for faculty type and one variable for year. In other words, we will convert the data from wide format to long format. But before we do so, a thought exercise: How many rows will the long-format data have? It will have a row for each combination of year and faculty type. If there are 5 faculty types and 11 years of data, how many rows will we have? We do the wide to long conversion using a new function: pivot_longer(). The animation below show how this function works, as well as its counterpart pivot_wider(). The function has the following arguments: pivot_longer(data, cols, names_to = &quot;name&quot;) The first argument is data as usual. The second argument, cols, is where you specify which columns to pivot into longer format  in this case all columns except for the faculty_type The third argument, names_to, is a string specifying the name of the column to create from the data stored in the column names of data  in this case year staff_long &lt;- staff %&gt;% pivot_longer(cols = -faculty_type, names_to = &quot;year&quot;) %&gt;% mutate(value = as.numeric(value)) Lets take a look at what the new longer data frame looks like. staff_long ## # A tibble: 55 x 3 ## faculty_type year value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Full-Time Tenured Faculty 1975 29 ## 2 Full-Time Tenured Faculty 1989 27.6 ## 3 Full-Time Tenured Faculty 1993 25 ## 4 Full-Time Tenured Faculty 1995 24.8 ## 5 Full-Time Tenured Faculty 1999 21.8 ## 6 Full-Time Tenured Faculty 2001 20.3 ## 7 Full-Time Tenured Faculty 2003 19.3 ## 8 Full-Time Tenured Faculty 2005 17.8 ## 9 Full-Time Tenured Faculty 2007 17.2 ## 10 Full-Time Tenured Faculty 2009 16.8 ## # ... with 45 more rows And now lets plot is as a line plot. A possible approach for creating a line plot where we color the lines by faculty type is the following: staff_long %&gt;% ggplot(aes(x = year, y = value, color = faculty_type)) + geom_line() ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? But note that this results in a message as well as an unexpected plot. The message is saying that there is only one observation for each faculty type year combination. We can fix this using the group aesthetic following. staff_long %&gt;% ggplot(aes(x = year, y = value, group = faculty_type, color = faculty_type)) + geom_line() Include the line plot you made above in your report and make sure the figure width is large enough to make it legible. Also fix the title, axis labels, and legend label. Suppose the objective of this plot was to show that the proportion of part-time faculty have gone up over time compared to other instructional staff types. What changes would you propose making to this plot to tell this story. (You dont need to implement these changes now, you will get to do that as part of this weeks homework. But work as a team to come up with ideas and list them as bullet points. The more precise you are, the easier your homework will be.)   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 18.4.2 Fisheries Fisheries and Aquaculture Department of the Food and Agriculture Organization of the United Nations collects data on fisheries production of countries. This Wikipedia page lists fishery production of countries for 2016. For each country tonnage from capture and aquaculture are listed. Note that countries whose total harvest was less than 100,000 tons are not included in the visualization. A researcher shared with you the following visualization they created based on these data . Can you help them make improve it? First, brainstorm how you would improve it. Then create the improved visualization and write up the changes/decisions you made as bullet points. Its ok if some of your improvements are aspirational, i.e. you dont know how to implement it, but you think its a good idea. Ask a tutor for help, but also keep an eye on the time. Implement what you can and leave note identifying the aspirational improvements. fisheries &lt;- read_csv(&quot;data/fisheries.csv&quot;)   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 18.5 Wrapping up Go back through your write up to make sure youre following coding style guidelines we discussed in class. Make any edits as needed. Also, make sure all of your R chunks are properly labeled, and your figures are reasonably sized. Once the team leader for the week pushes their final changes, others should pull the changes and knit the R Markdown document to confirm that they can reproduce the report. 18.6 More ugly charts Want to see more ugly charts? Flowing Data - Ugly Charts Reddit - Data is ugly Missed Opportunities and Graphical Failures (Mostly Bad) Graphics and Tables "],["secrets.html", "19 Optional: Secrets of a happy graphing life 19.1 Load gapminder and the tidyverse 19.2 Hidden data wrangling problems 19.3 Keep stuff in data frames 19.4 Tidying and reshaping 19.5 Factor management 19.6 Worked example", " 19 Optional: Secrets of a happy graphing life 19.1 Load gapminder and the tidyverse library(gapminder) library(tidyverse) 19.2 Hidden data wrangling problems If you are struggling to make a figure, dont assume its a problem between you and ggplot2. Stop and ask yourself which of these rules you are breaking: Keep stuff in data frames Keep your data frames tidy; be willing to reshape your data often Use factors and be the boss of them In my experience, the vast majority of graphing agony is due to insufficient data wrangling. Tackle your latent data storage and manipulation problems and your graphing problem often melts away. 19.3 Keep stuff in data frames I see a fair amount of early-career code where variables are copied out of a data frame, to exist as stand-alone objects in the workspace. life_exp &lt;- gapminder$lifeExp year &lt;- gapminder$year Historically, ggplot2 has had an incredibly strong preference for variables in data frames. It used to be a requirement for the main data frame underpinning a plot. Although this requirement is no longer the case, it is still a good idea to keep your variables associated with their data frames. ggplot(mapping = aes(x = year, y = life_exp)) + geom_jitter() Just leave the variables in place and pass the associated data frame! This advice applies to base and lattice graphics as well. It is not specific to ggplot2. ggplot(data = gapminder, aes(x = year, y = life_exp)) + geom_jitter() What if we wanted to filter the data by country, continent, or year? This is much easier to do safely if all affected variables live together in a data frame, not as individual objects that can get out of sync. Dont write-off ggplot2 as a highly opinionated outlier! In fact, keeping data in data frames and computing and visualizing it in situ are widely regarded as best practices. The option to pass a data frame via data = is a common feature of many high-use R functions, e.g. lm(), aggregate(), plot(), and t.test(), so make this your default modus operandi. 19.3.1 Explicit data frame creation via tibble::tibble() and tibble::tribble() If your data is already lying around and its not in a data frame, ask yourself why not? Did you create those variables? Maybe you should have created them in a data frame in the first place! The tibble() function is an improved version of the built-in data.frame(), which makes it possible to define one variable in terms of another and which wont turn character data into factor. If constructing tiny tibbles by hand, tribble() can be an even handier function, in which your code will be laid out like the table you are creating. These functions should remove the most common excuses for data frame procrastination and avoidance. my_dat &lt;- tibble(x = 1:5, y = x ^ 2, text = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;delta&quot;, &quot;epsilon&quot;)) ## if you&#39;re truly &quot;hand coding&quot;, tribble() is an alternative my_dat &lt;- tribble( ~ x, ~ y, ~ text, 1, 1, &quot;alpha&quot;, 2, 4, &quot;beta&quot;, 3, 9, &quot;gamma&quot;, 4, 16, &quot;delta&quot;, 5, 25, &quot;epsilon&quot; ) str(my_dat) #&gt; tibble [5 x 3] (S3: tbl_df/tbl/data.frame) #&gt; $ x : num [1:5] 1 2 3 4 5 #&gt; $ y : num [1:5] 1 4 9 16 25 #&gt; $ text: chr [1:5] &quot;alpha&quot; &quot;beta&quot; &quot;gamma&quot; &quot;delta&quot; ... ggplot(my_dat, aes(x, y)) + geom_line() + geom_text(aes(label = text)) Together with dplyr::mutate(), which adds new variables to a data frame, this gives you the tools to work within data frames whenever youre handling related variables of the same length. 19.3.2 Sidebar: with() Sadly, not all functions offer a data = argument. Take cor(), for example, which computes correlation. This does not work: cor(year, lifeExp, data = gapminder) #&gt; Error in cor(year, lifeExp, data = gapminder): unused argument (data = gapminder) Sure, you can always just repeat the data frame name like so: cor(gapminder$year, gapminder$lifeExp) #&gt; [1] 0.436 but people hate typing. I suspect subconscious dread of repeatedly typing gapminder is what motivates those who copy variables into stand-alone objects in the workspace. The with() function is a better workaround. Provide the data frame as the first argument. The second argument is an expression that will be evaluated in a special environment. It could be a single command or a multi-line snippet of code. Whats special is that you can refer to variables in the data frame by name. with(gapminder, cor(year, lifeExp)) #&gt; [1] 0.436 If you use the magrittr package, another option is to use the %$% operator to expose the variables inside a data frame for further computation: library(magrittr) gapminder %$% cor(year, lifeExp) #&gt; [1] 0.436 19.4 Tidying and reshaping This is an entire topic covered elsewhere: Chapter ?? - Tidy data using Lord of the Rings 19.5 Factor management This is an entire topic covered elsewhere: Chapter ?? - Be the boss of your factors 19.6 Worked example Inspired by this question from a student when we first started using ggplot2: How can I focus in on country, Japan for example, and plot all the quantitative variables against year? Your first instinct might be to filter the Gapminder data for Japan and then loop over the variables, creating separate plots which need to be glued together. And, indeed, this can be done. But in my opinion, the data reshaping route is more R native given our current ecosystem, than the loop way. 19.6.1 Reshape your data We filter the Gapminder data and keep only Japan. Then we use tidyr::gather() to gather up the variables pop, lifeExp, and gdpPercap into a single value variable, with a companion variable key. japan_dat &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) japan_tidy &lt;- japan_dat %&gt;% gather(key = var, value = value, pop, lifeExp, gdpPercap) dim(japan_dat) #&gt; [1] 12 6 dim(japan_tidy) #&gt; [1] 36 5 The filtered japan_dat has 12 rows. Since we are gathering or stacking three variables in japan_tidy, it makes sense to see three times as many rows, namely 36 in the reshaped result. 19.6.2 Iterate over the variables via faceting Now that we have the data we need in a tidy data frame, with a proper factor representing the variables we want to iterate over, we just have to facet. p &lt;- ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~ var, scales=&quot;free_y&quot;) p + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) 19.6.3 Recap Heres the minimal code to produce our Japan example. japan_tidy &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) %&gt;% gather(key = var, value = value, pop, lifeExp, gdpPercap) ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~ var, scales=&quot;free_y&quot;) + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) This snippet demonstrates the payoffs from the rules we laid out at the start: We isolate the Japan data into its own data frame. We reshape the data. We gather three columns into one, because we want to depict them via position along the y-axis in the plot. We use a factor to distinguish the observations that belong in each mini-plot, which then becomes a simple application of faceting. This is an example of expedient data reshaping. I dont actually believe that gdpPercap, lifeExp, and pop naturally belong together in one variable. But gathering them was by far the easiest way to get this plot. "],["web-scraping.html", "20 Web scraping 20.1 Module Materials 20.2 Topic 1! 20.3 Topic 2!", " 20 Web scraping This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 20.1 Module Materials Slides Activities Lab 20.2 Topic 1! You can follow along with the slides here if they do not appear below. 20.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab07.html", "21 Lab: Smokers in Whickham 21.1 Simpsons paradox 21.2 Getting started 21.3 The data 21.4 Exercises", " 21 Lab: Smokers in Whickham 21.1 Simpsons paradox 21.2 Getting started A study of conducted in Whickham, England recorded participants age, smoking status at baseline, and then 20 years later recorded their health outcome. 21.2.1 Packages In this lab we will work with the tidyverse and mosaicData packages. This is the first time were using the mosaicData package, you need to make sure to install it first by running install.packages(\"mosaicData\") in the console. library(tidyverse) library(mosaicData) Note that these packages are also loaded in your R Markdown document. 21.3 The data The data is in the mosaicData package. You can load it with data(Whickham) Take a peek at the codebook with ?Whickham 21.4 Exercises What type of study do you think these data comne from: observational or experiment? Why? How many observations are in this dataset? What does each observation represent? How many variables are in this dataset? What type of variable is each? Display each variable using an appropriate visualization. What would you expect the relationship between smoking status and health outcome to be? Create a visualization depicting the relationship between smoking status and health outcome. Briefly describe the relationship, and evaluate whether this meets your expectations. Additionally, calculate the relevant conditional probabilities to help your narrative. Here is some code to get you started: Whickham %&gt;% count(smoker, outcome) Create a new variable called age_cat using the following scheme: age &lt;= 44 ~ \"18-44\" age &gt; 44 &amp; age &lt;= 64 ~ \"45-64\" age &gt; 64 ~ \"65+\" Re-create the visualization depicting the relationship between smoking status and health outcome, faceted by age_cat. What changed? What might explain this change? Extend the contingency table from earlier by breaking it down by age category and use it to help your narrative. Whickham %&gt;% count(smoker, age_cat, outcome) "],["functions.html", "22 Functions 22.1 Module Materials 22.2 Topic 1! 22.3 Topic 2!", " 22 Functions This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 22.1 Module Materials Slides Activities Lab 22.2 Topic 1! You can follow along with the slides here if they do not appear below. 22.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab08.html", "23 Lab: University of Edinburgh Art Collection 23.1 Getting started 23.2 R scripts vs. R Markdown documents 23.3 SelectorGadget 23.4 Functions 23.5 Iteration 23.6 Analysis", " 23 Lab: University of Edinburgh Art Collection The University of Edinburgh Art Collection supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.1. See the sidebar [here](https://collections.ed.ac.uk/art) and note that there are 2909 pieces in the art collection we&#39;re collecting data on. In this lab well scrape data on all art pieces in the Edinburgh College of Art collection. The earning goals of this lab are: Web scraping from a single page Writing functions Iteration Writing data Before getting started, lets check that a bot has permissions to access pages on this domain. paths_allowed(&quot;https://collections.ed.ac.uk/art)&quot;) ## collections.ed.ac.uk ## [1] TRUE 23.1 Getting started Go to the course GitHub organization and locate your lab repo, which should be named lab-08-uoe-art-YOUR_TEAMNAME. Grab the URL of the repo, and clone it in RStudio Cloud. Your email address is the address tied to your GitHub account and your name should be first and last name. Run the following (but update it for your name and email!) in the Console to configure Git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 23.2 R scripts vs. R Markdown documents Today we will be using both R scripts and R Markdown documents: .R: R scripts are plain text files containing only code and brief comments, Well use R scripts in the web scraping stage and ultimately save the scraped data as a csv. .Rmd: R Markdown documents are plain text files containing. Well use an R Markdown document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage. Here is the organization of your repo, and the corresponding section in the lab that each file will be used for: |-data | |- README.md |-lab-06-uoe-art.Rmd # analysis |-lab-06-uoe-art.Rproj |-README.md |-scripts # webscraping | |- 01-scrape-page-one.R # scraping a single page | |- 02-scrape-page-function.R # functions | |- 03-scrape-page-many.R # iteration 23.3 SelectorGadget For this lab, please use Google Chrome as your web browser. If you are using the one of the computers in the computer lab, you can access Google Chrome by searching for it in the search bar at the bottom left of your home screen. Then, go to the SelectorGadget extension page on the Chrome Web Store and click on Add to Chrome (big blue button). A pop up window will ask Add SelectorGadget?, click Add extension. Another pop up window will asl whether you want to get your extensions on all your computer. If you want this, you can turn on sync, but you dont need to for the purpose of this lab. You should now be able to access SelectorGadget by clicking on the icon next to the search bar in the Chrome browser. 23.3.1 Scraping a single page **Tip:** To run the code you can highlight or put your cursor next to the lines of code you want to run and hit Command+Enter. Work in scripts/01-scrape-page-one.R. We will start off by scraping data on the first 10 pieces in the collection from here. First, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R. # set url first_url &lt;- &quot;https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0&quot; # read html page page &lt;- read_html(first_url) For the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame. 23.3.2 Titles Lets start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes: page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) ## {xml_nodeset (10)} ## [1] &lt;a href=&quot;./record/99260?highlight=*:*&quot;&gt;Untitled - Street View of Rooftop ... ## [2] &lt;a href=&quot;./record/99261?highlight=*:*&quot;&gt;Untitled - Abstracted Plant Forms ... ## [3] &lt;a href=&quot;./record/21438?highlight=*:*&quot;&gt;Portrait in Green ... ## [4] &lt;a href=&quot;./record/21441?highlight=*:*&quot;&gt;Portrait of a Man ... ## [5] &lt;a href=&quot;./record/21439?highlight=*:*&quot;&gt;Portrait of a Man ... ## [6] &lt;a href=&quot;./record/21442?highlight=*:*&quot;&gt;Portrait of a Woman ... ## [7] &lt;a href=&quot;./record/21443?highlight=*:*&quot;&gt;Seated Female Nude ... ## [8] &lt;a href=&quot;./record/21440?highlight=*:*&quot;&gt;Poirtrait of a Man ... ## [9] &lt;a href=&quot;./record/21435?highlight=*:*&quot;&gt;House and Garden ... ## [10] &lt;a href=&quot;./record/21486?highlight=*:*&quot;&gt;Harbour Scene ... Then we extract the text with html_text(): page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() ## [1] &quot;Untitled - Street View of Rooftops and Chimneys (1967-1968)&quot; ## [2] &quot;Untitled - Abstracted Plant Forms in Black and White (1968)&quot; ## [3] &quot;Portrait in Green (2018)&quot; ## [4] &quot;Portrait of a Man (2018)&quot; ## [5] &quot;Portrait of a Man (2018)&quot; ## [6] &quot;Portrait of a Woman (2018)&quot; ## [7] &quot;Seated Female Nude (2018)&quot; ## [8] &quot;Poirtrait of a Man (2018)&quot; ## [9] &quot;House and Garden (2018)&quot; ## [10] &quot;Harbour Scene (1987-1988)&quot; And get rid of all the spurious whitespace in the text with str_squish(): Take a look at the help docs for `str_squish()` (with `?str_squish`) to page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() ## [1] &quot;Untitled - Street View of Rooftops and Chimneys (1967-1968)&quot; ## [2] &quot;Untitled - Abstracted Plant Forms in Black and White (1968)&quot; ## [3] &quot;Portrait in Green (2018)&quot; ## [4] &quot;Portrait of a Man (2018)&quot; ## [5] &quot;Portrait of a Man (2018)&quot; ## [6] &quot;Portrait of a Woman (2018)&quot; ## [7] &quot;Seated Female Nude (2018)&quot; ## [8] &quot;Poirtrait of a Man (2018)&quot; ## [9] &quot;House and Garden (2018)&quot; ## [10] &quot;Harbour Scene (1987-1988)&quot; And finally save the resulting data as a vector of length 10: titles &lt;- page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() 23.3.3 Links The same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes. A mini HTML lesson! The following is how we define hyperlinked text in HTML: &lt;a href=&quot;https://www.google.com&quot;&gt;Seach on Google&lt;/a&gt; And this is how the text would look like on a webpage: Seach on Google. Here the text is Seach on Google and the href attribute contains the url of the website youd go to if you click on the hyperlinked text: https://www.google.com. The moral of the story is: the link is stored in the href attribute. page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% # same nodes html_node(&quot;h3 a&quot;) %&gt;% # as before html_attr(&quot;href&quot;) # but get href attribute instead of text ## [1] &quot;./record/99260?highlight=*:*&quot; &quot;./record/99261?highlight=*:*&quot; ## [3] &quot;./record/21438?highlight=*:*&quot; &quot;./record/21441?highlight=*:*&quot; ## [5] &quot;./record/21439?highlight=*:*&quot; &quot;./record/21442?highlight=*:*&quot; ## [7] &quot;./record/21443?highlight=*:*&quot; &quot;./record/21440?highlight=*:*&quot; ## [9] &quot;./record/21435?highlight=*:*&quot; &quot;./record/21486?highlight=*:*&quot; These dont really look like urls as we know then though. Theyre relative links. See the help for `str_replace()` to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the `pattern` and `replacement` arguments. Click on one of art piece titles in your browser and take note of the url of the webpage it takes you to. How does that url compare to what we scraped above? How is it different? Using str_replace(), fix the URLs. 23.3.4 Artists Fill in the blanks to scrape artist names. 23.3.5 Put it altogether Fill in the blanks to organize everything in a tibble. 23.3.6 Scrape the next page Click on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten. 23.4 Functions Work in scripts/02-scrape-page-function.R. Youve been using R functions, now its time to write your own! Lets start simple. Here is a function that takes in an argument x, and adds 2 to it. add_two &lt;- function(x){ x + 2 } Lets test it: add_two(3) ## [1] 5 add_two(10) ## [1] 12 The skeleton for defining functions in R is as follows: function_name &lt;- function(input){ # do something with the input(s) # return something } Then, a function for scraping a page should look something like: **Reminder:** Function names should be short but evocative verbs. function_name &lt;- function(url){ # read page at url # extract title, link, artist info for n pieces on page # return a n x 3 tibble } Fill in the blanks using code you already developed in the previous exercises. Name the function scrape_page. Test out your new function by running the following in the console. Does the output look right? Discuss with teammaates whether youre getting the same results as before. scrape_page(first_url) scrape_page(second_url) 23.5 Iteration Work in scripts/03-scrape-page-many.R. We went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection. **Reminder:** The collection has 2909 pieces in total. That means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 2909 rows and 3 columns. 23.5.1 List of URLs Click through the first few of the pages in the art collection and observe their URLs to confirm the following pattern: [sometext]offset=0 # Pieces 1-10 [sometext]offset=10 # Pieces 11-20 [sometext]offset=20 # Pieces 21-30 [sometext]offset=30 # Pieces 31-40 ... [sometext]offset=2900 # Pieces 2900-2909 We can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 2900. Two new functions are helpful for accomplishing this: paste0() for pasting two pieces of text and seq() for generating a sequence of numbers. Fill in the blanks to construct the list of URLs. 23.5.2 Mapping Finally, were ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R (which well learn about in more detail tomorrow), and they each take the following form: map([x], [function to apply to each element of x]) In our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function: map_dfr(urls, scrape_page) Fill in the blanks to scrape all pages, and to create a new data frame called uoe_art. 23.5.3 Write out data Finally write out the data frame you constructed into the data folder so that you can use it in the analysis section. 23.6 Analysis Work in lab-06-uoe-art.Rmd for the rest of the lab. Now that we have a tidy dataset that we can analyze, lets do that! Well start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up! First thing well try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to separate the title column at the first occurence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date Luckily, theres a function that does just this: separate()! And once we have completed separating the single title column into title and date, we need to do further cleanup in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable. **Hint:** Remember escaping special characters from yesterday&#39;s lecture? You&#39;ll need to use that trick again. Fill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and thats OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where its convenient to do so. Print out a summary of the dataframe using the skim() function. How many pieces have artist info missing? How many have year info missing? Make a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary? **Hint:** You&#39;ll want to use `mutate()` and `if_else()` or `case_when()` to implement the correction. Find which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didnt capture the correct year information? Correct the error in the data frame and visualize the data again. Who is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them? **Hint:** You&#39;ll want to use a combination of `filter()` and `str_detect()`. You will want to read the help for `str_detect()` at a minimum, and consider how you might capture titles where the word appears as &quot;child&quot; and &quot;Child&quot;. Final question! How many art pieces have the word child in their title? See if you can figure it out, and ask for help if not. Source: https://collections.ed.ac.uk/art/about "],["data-and-ethics.html", "24 Data and Ethics 24.1 Module Materials 24.2 Data Science Ethics in 6 Minutes 24.3 Big Data! 24.4 Topic 2!", " 24 Data and Ethics This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-09]. Most of the slides used to make the videos in this module can be found in the slides repo. 24.1 Module Materials Slides Activities Lab Lab 24.2 Data Science Ethics in 6 Minutes 24.3 Big Data! You can follow along with the slides here if they do not appear below. 24.4 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab09.html", "25 Lab: Professor attractiveness and course evaluations 25.1 Modeling with a single predictor 25.2 Getting started 25.3 Warm up 25.4 The data 25.5 Exercises", " 25 Lab: Professor attractiveness and course evaluations 25.1 Modeling with a single predictor 25.2 Getting started Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings. (Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. http://www.sciencedirect.com/science/article/pii/S0272775704001165.) For this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. 25.2.1 Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 25.2.2 Housekeeping 25.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 25.2.2.2 Project name Update the name of your project to match the labs title. 25.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 25.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 25.3.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 25.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 25.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 25.5 Exercises 25.5.1 Part 1: Exploratory Data Analysis Visualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response. Visualize and describe the relationship between score and the new variable you created, bty_avg. **Hint:** See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html. Replot the scatterplot from Exercise 3, but this time use geom_jitter()? What does jitter mean? What was misleading about the initial scatterplot? 25.5.2 Part 2: Linear regression with a numerical predictor Linear model is in the form $\\hat{y} = b_0 + b_1 x$. Lets see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model. Replot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line. Interpret the slope of the linear model in context of the data. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context. Determine the \\(R^2\\) of the model and interpret it in context of the data. 25.5.3 Part 3: Linear regression with a categorical predictor Fit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data. What is the equation of the line corresponding to male professors? What is it for female professors? Fit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Create a new variable called rank_relevel where \"tenure track\" is the baseline level. Fit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. Create another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\". Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. "],["fitting-and-interpreting-models.html", "26 Fitting and interpreting models 26.1 Module Materials 26.2 Topic 1! 26.3 Topic 2!", " 26 Fitting and interpreting models This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 26.1 Module Materials Slides Activities Lab 26.2 Topic 1! You can follow along with the slides here if they do not appear below. 26.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab10.html", "27 Lab: Modeling with multiple predictors 27.1 Professor attractiveness and course evaluations, Pt. 2 27.2 Getting started 27.3 Warm up 27.4 The data 27.5 Exercises", " 27 Lab: Modeling with multiple predictors 27.1 Professor attractiveness and course evaluations, Pt. 2 In this lab we revisit the professor evaluations data we modeled in the previous lab. In the last lab we modeled evaluation scores using a single predictor at a time. However this time we use multiple predictors to model evaluation scores. If you dont remember the data, review the previous labs introduction before continuing to the exercises. 27.2 Getting started 27.2.1 Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 27.2.2 Housekeeping 27.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 27.2.2.2 Project name Update the name of your project to match the labs title. 27.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 27.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 27.3.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 27.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 27.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 27.5 Exercises Load the data by including the appropriate code in your R Markdown file. 27.5.1 Part 1: Simple linear regression Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). 27.5.2 Part 2: Multiple linear regression Fit a linear model (one you have fit before): m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). Interpret the slope and intercept of m_bty_gen in context of the data. What percent of the variability in score is explained by the model m_bty_gen. What is the equation of the line corresponding to just male professors? For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score? How does the relationship between beauty and evaluation score vary between male and female professors? How do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beaty score of the professor. Compare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg? Create a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data. 27.5.3 Part 3: The search for the best model Going forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg. Which variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professors score. Check your suspicions from the previous exercise. Include the model output for that variable in your response. Suppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why? Fit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question. Using backward-selection with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on. Interpret the slopes of one numerical and one categorical predictor based on your final model. Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score. Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not? "],["prediction-and-overfitting.html", "28 Prediction and overfitting 28.1 Module Materials 28.2 Topic 1! 28.3 Topic 2!", " 28 Prediction and overfitting This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 28.1 Module Materials Slides Activities Lab 28.2 Topic 1! You can follow along with the slides here if they do not appear below. 28.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab11.html", "29 Lab: So what if you smoke when pregnant? 29.1 Simulation based inference 29.2 Getting started 29.3 Housekeeping 29.4 Warm up 29.5 Set a seed! 29.6 The data 29.7 Exercises", " 29 Lab: So what if you smoke when pregnant? 29.1 Simulation based inference In 2004, the state of North Carolina released a large data set containing information on births recorded in this state. This data set is useful to researchers studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of observations from this data set. 29.2 Getting started 29.2.1 Packages In this lab we will work with the tidyverse, infer, and openintro packages. We can install and load them with the following: library(tidyverse) library(infer) library(openintro) 29.3 Housekeeping 29.3.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 29.3.2 Project name Update the name of your project to match the labs title. 29.4 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 29.4.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 29.4.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 29.4.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 29.5 Set a seed! In this lab well be generating random samples. The last thing you want is those samples to change every time you knit your document. So, you should set a seed. Theres an R chunk in your R Markdown file set aside for this. Locate it and add a seed. Make sure all members in a team are using the same seed so that you dont get merge conflicts and your results match up for the narratives. 29.6 The data Load the ncbirths data from the openintro package: data(ncbirths) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is as follows. variable description fage fathers age in years. mage mothers age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. 29.7 Exercises What are the cases in this data set? How many cases are there in our sample? The first step in the analysis of a new dataset is getting acquanted with the data. Make summaries of the variables in your dataset, determine which variables are categorical and which are numerical. For numerical variables, are there outliers? If you arent sure or want to take a closer look at the data, make a graph. 29.7.1 Baby weights Wen, Shi Wu, Michael S. Kramer, and Robert H. Usher. &quot;Comparison of birth weight distributions between Chinese and Caucasian infants.&quot; American Journal of Epidemiology 141.12 (1995): 1177-1187. A 1995 study suggestes that average weight of Caucasian babies born in the US is 3,369 grams (7.43 pounds). In this dataset we only have information on mothers race, so we will make the simplifying assumption that babies of Caucasian mothers are also Caucasian, i.e. whitemom = \"white\". We want to evaluate whether the average weight of Caucasian babies has changed since 1995. Our null hypothesis should state there is nothing going on, i.e. no change since 1995: \\(H_0: \\mu = 7.43~pounds\\). Our alternative hypothesis should reflect the research question, i.e. some change since 1995. Since the research question doesnt state a direction for the change, we use a two sided alternative hypothesis: \\(H_A: \\mu \\ne 7.43~pounds\\). Create a filtered data frame called ncbirths_white that contain data only from white mothers. Then, calculate the mean of the weights of their babies. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Lets discuss how this test would work. Our goal is to simulate a null distribution of sample means that is centered at the null value of 7.43 pounds. In order to do so, we take a bootstrap sample of from the original sample, calculate this bootstrap samples mean, repeat these two steps a large number of times to create a bootstrap distribution of means centered at the observed sample mean, shift this distribution to be centered at the null value by substracting / adding X to all boostrap mean (X = difference between mean of bootstrap distribution and null value), and calculate the p-value as the proportion of bootstrap samples that yielded a sample mean at least as extreme as the observed sample mean. Run the appropriate hypothesis test, visualize the null distribution, calculate the p-value, and interpret the results in context of the data and the hypothesis test. 29.7.2 Baby weight vs. smoking Consider the possible relationship between a mothers smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make side-by-side boxplots displaying the relationship between habit and weight. What does the plot highlight about the relationship between these two variables? Before moving forward, save a version of the dataset omitting observations where there are NAs for habit. You can call this version ncbirths_habitgiven. The box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the habit variable, and then calculate the mean weight in these groups using. ncbirths_habitgiven %&gt;% group_by(habit) %&gt;% summarise(mean_weight = mean(weight)) There is an observed difference, but is this difference statistically significant? In order to answer this question we will conduct a hypothesis test . Write the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Run the appropriate hypothesis test, calculate the p-value, and interpret the results in context of the data and the hypothesis test. Construct a 95% confidence interval for the difference between the average weights of babies born to smoking and non-smoking mothers. 29.7.3 Baby weight vs. mothers age In this portion of the analysis we focus on two variables. The first one is maturemom. First, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. The other variable of interest is lowbirthweight. Conduct a hypothesis test evaluating whether the proportion of low birth weight babies is higher for mature mothers. State the hypotheses, verify the conditions, run the test and calculate the p-value, and state your conclusion in context of the research question. Use \\(\\alpha = 0.05\\). If you find a significant difference, costruct a confidence interval, at the equivalent level to the hypothesis test, for the difference between the proportions of low birth weight babies between mature and younger moms, and interpret this interval in context of the data. "],["cross-validation.html", "30 Cross validation 30.1 Module Materials 30.2 Topic 1! 30.3 Topic 2!", " 30 Cross validation This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-12]. Most of the slides used to make the videos in this module can be found in the slides repo. 30.1 Module Materials Slides Activities Lab 30.2 Topic 1! You can follow along with the slides here if they do not appear below. 30.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab-work-on-portfolios.html", "31 Lab: Work on portfolios", " 31 Lab: Work on portfolios This week youll be working on your projects. Here are a few to do items to get you started. Once you complete these, use the rest of the time to, well, work on your project! Remind yourself of the project assignment Go to the course organization on GitHub and clone your project repo titled project-TEAM_NAME Add your project title and team name to the README.Rmd file in the repo and commit and push your changes. Observe that these are updated in the README of the repo. Open the presentation.Rmd file, knit the document, and review the presentation format. This is where your presentation will go. Update the YAML with your project title, team name, etc. and commit and push your changes. Go to your project repo on GitHub, click on Settings on the top right corner, and scroll down to the section titled GiHub Pages. Under Source, select master branch. This will give you a URL where the website for your project will be automatically built from the content in your README. This might take a few minutes. Click on the link to confirm that the website has been built. (Optional) Once the website it build, you can change its theme using the Theme Chooser. Also, once the website is built, youll need to pull changes to your project in RStudio. Take a look at your rendered project website. Click on the link in the presentation section and you should be able to view the rendered slides. This is the link we will use to project your slides during the presentations. On your repo you should see a text on top No description, website, or topics provided.. Next to it theres an Edit button. Add a short description as well as the URL of your project website here. Note: This website is public, but your repository will remain private,unless you as a team decide you would like to feature your repos in your personal GitHub profiles. If so, I will help you convert your repo to a public repo at the end of the semester. I will not add any marks to your repos so that your public work wont contain your score for the project. Add your dataset to the data folder and add your codebook to the README inthat folder. If in your proposal you were advised to update your codebook, make sure to make those updates. If you had R scripts you used to scrape your data, add them to this folder as well. Add the content from your proposal to the proposal.Rmd file in the proposal folder. Knit the document to make sure everything works and commit and push your proposal to your project repo. Important: Your data now lives in a folder called data that is not inside your proposal folder. So you need to specify the path to your data with \"../data/name_of_datafile\" in your read_csv() (or similar) function. You dont need to make further updates to your proposal at this point, even if your plans for the project change slightly. Load your data in your presentation.Rmd, knit, and make sure everything works. Commit and push your updated proposal to your project repo. Important: Same note as above! Your data now lives in a folder called data that is not inside your presentation folder. So you need to specify the path to your data with \"../data/name_of_datafile\" in your read_csv() (or similar) function. Now that all the logistical details are done, start working on your project. Open issues for things you want to accomplish. Assign them to specific team member(s) if you like. And as you complete the tasks, close the issues. You can also use the issues for discussion on the specific tasks. Strongly recommended: Get a hold of a tutor and run your ideas by them. "],["quantifying-uncertainty.html", "32 Quantifying Uncertainty 32.1 Module Materials 32.2 Topic 1! 32.3 Topic 2!", " 32 Quantifying Uncertainty This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 32.1 Module Materials Slides Activities Lab 32.2 Topic 1! You can follow along with the slides here if they do not appear below. 32.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["lab13.html", "33 Collaborating on GitHub 33.1 GitHub issues 33.2 Project progress", " 33 Collaborating on GitHub This week youll continue working on your projects. The first half of the workshop is structured, and you can use the second half to make progress on your projects. 33.1 GitHub issues Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. Theyre kind of like emailexcept they can be shared and discussed with the rest of your team. You can use issues as to-do lists as well as a place for brainstorming / discussing ideas. 33.1.1 Opening an issue Go to your project repo and open a new issue titled Practice issue. Add the following text to the issue: This is not a real issue. This is just some placeholder text. And the following is a bulleted to-do list: - [ ] Do this - [ ] Then that - [ ] And finally this Hit preview to make sure the issue looks like the following: Submit the issue. Then, assign the issue to one or few members of the team. 33.1.2 Working on the issue As you work on the issue you can check the boxes. Note that this will also show progress on the issue on the issue dashboard. Check some of the boxes on your practice issue and confirm that you can see the progress result on the issue dashboard. 33.1.3 Closing the issue Once youre done with an issue, you should close it. You can do this in one of two ways: on GitHub by clicking on Close issue or via a commit that directly addresses the issue. Well practice the second one. If you preface your commits with Fixes, Fixed, Fix, Closes, Closed, or Close, the issue will be closed when you push the changes to your repo. Take a note of the issue number, which will show up next to the issue title. Go to your project on RStudio and make a change. This can be something silly like adding a new line to the issue README. Then commit this change. In your commit message, use one of the special words listed above and reference the issue. For example, if the change I made was to add a new line to the README I would say something like the following: Add a new line to the README, closes #2 Push your changes and observe that the issue is now closed on GitHub. Click on the referenced commit to confirm that it was your last commit that closed the issue. 33.2 Project progress Now back to your project Crafting your to-do list: Discuss your plan for your project as a team, and open at least n issues, where n is the number of students in your team. Not every issue needs to have a checklist, but you might want to include checklists in some of them to remind yourselves the exact steps you discussed to tackle the issue. Then assign at least one issue to each team member. Customizing your website theme: We attempted this last week, and failed due to permission issues. Lets try it one more time! Browse the possible GitHub themes demo pages at the following links. architect cayman dinky hacker leap-day merlot midnight minimal modernist slate tactile time machine Once you decide which theme you prefer (and its perfectly fine if its the default theme you had to begin with), go to the _config.yml file in your repo on RStudio and edit the theme name in the _config.yml file. For example, if you were going from cayman to hacker, your diff would look like the following. Once you commit and push this change, give it a couple minutes for the website to rebuild, and confirm that the theme was changed. **Note:** This is an extremely important step as this is the link I will use on the day of your presentation. There will not be time to make push updates once your presentation session starts. Updating your project description: If you have not yet done so, add a brief description, link to your project website, and topics to your project repo. Citing your data: Now is the time to fix up those citations! In your project README there is a link to a resource for properly citing data. Develop a citation for your dataset and add it under the data section using this guidance. If you have questions, ask a tutor for help! Confirming presentation format: Go to the website for your repo and click on the link that should take you to your presentation. Confirm that your latest changes to the presentation are reflected at this link (which means you must have pushed the resulting HTML file along with the Rmd file where you wrote your presentation). Confirming schedules: Go to the schedule for presentations and confirm that all team members can make it at the beginning of the hour youre assigned to present in. Also note that we will not be meeting in the computer lab. Some of you who have an ILA workshop before this class have been assigned to the first hour due to the sheer number of such students with conflicts. I have checked in with the ILA course organizer and have been told that next weeks workshop is revision, and the presentations in IDS should take priority. So please make sure to leave your workshop by 11:30 at the latest to get to Kings Buildings in time for the presentations. Order within each hour will be announced on the day of the presentations, so you should all be ready to go at the beginning of the hour. Tidying up your coding style: Go to the pull requests tab and take a look at the code styling suggestions. Implement them in the relevant files. Styling suggestions are generated daily at 13:30, so if you do more work today, there may be more suggestions tomorrow. Make sure to check these before you finalize work on your repo. Strongly recommended: Get a hold of a tutor and run your ideas by them. "],["rshiny.html", "34 Interactive web apps 34.1 Module Materials 34.2 Topic 1! 34.3 Topic 2!", " 34 Interactive web apps This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-14] . Most of the slides used to make the videos in this module can be found in the slides repo. 34.1 Module Materials Slides Activities Lab lab 14 34.2 Topic 1! You can follow along with the slides here if they do not appear below. 34.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["special-topics.html", "35 Special Topics 35.1 Module Materials 35.2 Topic 1! 35.3 Topic 2!", " 35 Special Topics This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 35.1 Module Materials Slides Activities Lab 35.2 Topic 1! You can follow along with the slides here if they do not appear below. 35.3 Topic 2! You can follow along with the slides here if they do not appear below. "],["good-resources.html", "36 Good Resources", " 36 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ "],["media-without-a-home-yet.html", "37 Media without a home yet 37.1 How computer memory works! 37.2 Quantum Computers Explained! 37.3 The Rise of the Machines  Why Automation is Different this Time 37.4 Who Would Be King of America if George Washington had been made a monarch? 37.5 Emergence  How Stupid Things Become Smart Together 37.6 The Birthday Paradox 37.7 Why cant you divide by zero? 37.8 Yea hes chewing up my stats homework but that face though 37.9 Coding Kitty", " 37 Media without a home yet 37.1 How computer memory works! 37.2 Quantum Computers Explained! 37.3 The Rise of the Machines  Why Automation is Different this Time 37.4 Who Would Be King of America if George Washington had been made a monarch? 37.5 Emergence  How Stupid Things Become Smart Together 37.6 The Birthday Paradox 37.7 Why cant you divide by zero? 37.8 Yea hes chewing up my stats homework but that face though Yea hes chewing up my stats homework but that face though from r/CatsBeingCats 37.9 Coding Kitty https://hostrider.com/ "],["references.html", "References", " References "]]

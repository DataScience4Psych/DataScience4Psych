[["index.html", "Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R Welcome to PSY 703 Mason Notes", " Data Science for Psychologists A Refreshed Exploratory &amp; Graphical Data Analysis in R S. Mason Garrison 2021-05-03 Welcome to PSY 703 Welcome to class! This website is designed to accompany Mason Garrisons Data Science for Psychologists (DS4P). DS4P is a graduate-level quantitative methods course at Wake Forest University. This class assumes zero knowledge of programming, computer science, linear algebra, probability, or really anything fancy. I encourage anyone who is quant-curious to work their way through these course notes. The course notes include lectures, worked examples, readings, activities, and labs. You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. All the embedded lecture videos can be found on a youtube playlist. Mason Notes This website is constantly changing. This new course is in active development, and approximately 100% done. I have made this process explicitly transparent because I want you to see how you can use R to produce some pretty neat things. Indeed, Ive included the source code for this website in the class github. I encourage you to contribute to the course code. If you catch typos, errors, please issue a pull request with the fixes. If you find cool / useful resources, please add them. By the end of the semester, I would love for everyone to have contributed to the course materials. It can be as simple as adding a course request to the wishlist. I believe that it is useful skill to be able to do. How to use these notes This document is broken down into multiple chapters. Use the table of contents on the left side of the screen to navigate, and use the hamburger icon (horizontal bars) at the top of the document to open or close the table of contents. At the top of the document, youll see additional icons which you can click to search the document, change the size, font or color scheme of the page. The document will be updated (unpredictably) throughout the semester. Every module corresponds to a weeks worth of material. Most modules are dedicated to improving a specific skill or at the very least dedicated to a specific theme. Within each module, there are embedded videos, slides, activities, labs, and tutorials. The skills developed in each module build upon skills youve developed in previous modules. Eventually, this class will have more modules available than weeks in a semester, so that you  the reader can choose your own adventure (err module) youd like to start with. Although these notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give you all a set of common materials on which to draw during the course. In class, we will sometimes do things outside the notes. The idea here is that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. Status of course In terms of timing, I will have each module completed by the start of the week. Given that the class meets either on Monday or Friday, the start of the week will be Wednesday at 12 p.m. EST. It is possible that I will get ahead of this deadline. You can see the current status of the course below. Although you are welcome to work ahead, be aware that I will be making changes to modules that havent officially started yet. In addition, I may add optional materials to previous modules that might be helpful. This table provides the current status of the course. It lists proportions of specific components by module. Overall it is 100% complete. Course Wishlist Although there will be hiccups and snafus along the way, one major advantage of this process is that you(!) can have a major input on what we cover. Some of these inputs have already been incorporated (such as github, Rshiny). So take advantage! Add your requests to the markdown list below! As I incorporate those requests, Ill move them into the Wish Granted Section. Wishes Github Rshiny Learning R functions fun, unusual ways of presenting data grammar of R Wish Not Yet Granted Data Science and the Law Computational neuroscience Multilevel Choosing the best analysis SEM, longitudinal data what to do with existing data basic data and programming skills (applied across platforms) troubleshooting (e.g., figuring out how to answer questions) Major Mission!!! Being comfortable with R really understand data draw meaningful conclusions about it understand why certain code is written certain way and applying that logic "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major Attributions Jenny Bryans (jennybryan.org) STAT 545 and Happy Git with R; Joe Rodgerss PSY 8751 Exploratory and Graphical Data Analysis Course Mine Çetinkaya-Rundels Data Science in a Box. Additional Attributions Academic.ios AWESOME DATA SCIENCE Julia Fukuyamas EXPLORATORY DATA ANALYSIS Benjamin Soltoffs Computing for the Social Sciences Grant McDermotts course materials on environmental economics and data science Thomas E. Love Karl Broman EMILY SUZANNE CLARKs Rubric for Unessays "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This information is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["sitemap.html", "Sitemap", " Sitemap html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #iccbvwaxdf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #iccbvwaxdf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #iccbvwaxdf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #iccbvwaxdf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #iccbvwaxdf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #iccbvwaxdf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #iccbvwaxdf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #iccbvwaxdf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #iccbvwaxdf .gt_column_spanner_outer:first-child { padding-left: 0; } #iccbvwaxdf .gt_column_spanner_outer:last-child { padding-right: 0; } #iccbvwaxdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #iccbvwaxdf .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #iccbvwaxdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #iccbvwaxdf .gt_from_md > :first-child { margin-top: 0; } #iccbvwaxdf .gt_from_md > :last-child { margin-bottom: 0; } #iccbvwaxdf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #iccbvwaxdf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #iccbvwaxdf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #iccbvwaxdf .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #iccbvwaxdf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #iccbvwaxdf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #iccbvwaxdf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #iccbvwaxdf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #iccbvwaxdf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #iccbvwaxdf .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #iccbvwaxdf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #iccbvwaxdf .gt_sourcenote { font-size: 90%; padding: 4px; } #iccbvwaxdf .gt_left { text-align: left; } #iccbvwaxdf .gt_center { text-align: center; } #iccbvwaxdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #iccbvwaxdf .gt_font_normal { font-weight: normal; } #iccbvwaxdf .gt_font_bold { font-weight: bold; } #iccbvwaxdf .gt_font_italic { font-style: italic; } #iccbvwaxdf .gt_super { font-size: 65%; } #iccbvwaxdf .gt_footnote_marks { font-style: italic; font-size: 65%; } title link api wrappers website attribution website automation website basic data care website bechdal website bootstrapping website choice your own activity website classic things in r website colophon website communicating data science results effectively website cross validation website data as rhetoric website data science and ethics website data types and recoding website data usually finds me website deeper diving into ggplot2 website designing effective visualizations website diy web data website dont miss module 00 website dont miss the last module website dplyr intro website exploratory data analysis website fitting and interpreting models website functions 1 website functions part1 website functions practicum website good resources website grammar of data wrangling website handson website import export website important topic a website importing data website index website lab01 website lab02 website lab03 website lab04 website lab05 website lab06a website lab06b website lab07 website lab08 website lab08b website lab09 website lab10 website lab11 website language of models website license website machines learn website media without a home yet website meet our toolbox website merges website mod06 website modeling non linear relationships website modeling with multiple predictors website natural language processing website neural networks website notes on feature engineering website notes on hypothesis testing website notes on logistic regression website odd design choices in data visualization website odd legacy data types website odd notes on cross validation website odd transformations data website overfitting website plots behaving badly website practical advice from the data professor website public health dashboards website quantifying uncertainty website r basics website reading error codes website references website rshiny overview website rshiny website scientific studies and confounding website scraping the web website secrets website shiny overview website shiny resources website shorthappygit website sitemap website special topics machine learn website star wars activity website syllabus website thoughtful workflow website thoughts from hadley wickham on tidyverse website tidy data website visualization examples website visualizing categorical data website visualizing data with ggplot2 website visualizing numerical data website welcome to base r website welcome to cross validation website welcome to data and ethics website welcome to data and visualization website welcome to data diving with types website welcome to data science website welcome to functions and automation website welcome to modeling the tidy way website welcome to prediction and overfitting website welcome to quantifying uncertainty website welcome to the template module website welcome to the tidyverse website welcome to tips for effective data visualization website welcome to web scraping website what is data science website working with multiple data frames website "],["colophon.html", "Colophon", " Colophon These notes was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from github. The book style was designed by Desirée De Leon. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.5 (2021-03-31) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-05-03 Along with these packages: "],["public-health-dashboards.html", "Public Health Dashboards Crowns and Tears: A Covid-19 visualization with a Music Box and punch cards Wake Forest Forsyth County North Carolina", " Public Health Dashboards Ok, so I know that this class is about data science, R, and data visualization However, I figured that it might be helpful for us all to have some public health dashboards in one easy place. If you want, we can pretend that this section is course content related because it shows data and whatnot. Or that it can give you ideas for portfolio pieces Crowns and Tears: A Covid-19 visualization with a Music Box and punch cards Source Code: https://github.com/simonhuwiler/crowns-and-tears Wake Forest The embedded dashboards are maintained by Wake Forest University and are made using Microsoft Power BI. More info about the dashboard can be found here here Forsyth County The embedded maps are maintained by Forsyth Countys department of public health. More info here Vaccinations Case Counts North Carolina "],["dont-miss-module-00.html", "Dont Miss Module 00 0.1 Big Ideas 0.2 Course Modality 0.3 Knowledge is Power 0.4 Meet Mason 0.5 Website Tour", " Dont Miss Module 00 This overview is designed to orient you to the class. Please watch the videos from this playlist and work your way through the notes. Although the module-level playlists are embedded in the course, you can find the full-course video playlist here. In addition, you can find the slides for this module here. Currently, there are 7 videos in this playlist. The average video length is 12 minutes, 27 seconds. The total length of the playlist is 1 hour, 27 minutes, 10 seconds. Data Science for Psychologists (DS4P) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. In this class, we link those principles to psychological methods and open science practices by emphasizing exploratory analyses and description, rather than confirmatory analyses and prediction. Through the semester we will work our way thru Wickham and Grolemunds R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. DS4P is a practical skilled-based class and should be useful to students aiming for academia as well as those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g, eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., valence analysis). 0.1 Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. 0.2 Course Modality This class is a blended class. The online portions are asynchronous, and often contain pre-recorded videos. Ive created a video highlighting how to be a successful asynchronous learner. Much of this information comes from Northeastern Universitys Tips for Taking Online Classes 0.2.1 Productivity During Lockdown 0.3 Knowledge is Power This brief video is covers the icebreaker I do in all of my classes. I encourage you to watch it. In it, I discuss stereotype threats and statistics anxiety. 0.4 Meet Mason 0.5 Website Tour "],["syllabus.html", "Syllabus 0.6 Materials 0.7 Assignment Instructions", " Syllabus You can find the full syllabus on my syllabus git repo. 0.6 Materials 0.6.1 Hardware This class requires that you have a laptop that can run R. 0.6.2 Required Texts The text is intended to supplement the videos, lecture notes, tutorials, activities, and labs. You probably need to consume all of them in order to be successful in this class. R for Data Science text 0.6.3 Software 0.6.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface for R. 0.6.3.2 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files  called a repository  in a structured way. Think of it like the Track Changes features from Microsoft Word. Github is a free IDE and hosting service for Git. As a Wake Forest student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account 0.7 Assignment Instructions 0.7.1 Portfolio EDA is like basketball. We can watch either being done, and appreciate the art and skill involved in high-level performance. In the hands of Lebron James or Michael Jordan, a basketball is a highly-tuned artistic instrument; in the hands of John Tukey, a graph sings the praises of data in melodies both harmonious and discordant, reflecting model, data, and mood. Part of this course will be devoted to Watching and Studying the master at his work. But basketball is played by thousands of bodies with less than NBA training and ability. Some novice basketball players are just learning their craft, and others will evolve into future LJs and MJs; others have lower aspirations, yet still enjoy participating. So also should EDA be played. A second part of this course will involve learning to do EDA by Doing It. Each of you will be expected to do several EDA projects. These projects will be done during EDA Labs in class, as well as during out-of-class effort. The nature of most of the particular projects will be entirely up to you. You will report to your instructor during EDA Labs on what you have been doing and what you plan to do. You will give a 15-minute individual presentation to the class at the end of the course on what you did in one or two of your major projects. Each project will require some data, to which EDA techniques will be applied. You are welcomed (in fact, strongly encouraged) to use data with which you are currently involved; dissertation or thesis data, a research project, the almanac, data from an article, data from EDA or VDat, data you collect from your family or friends, or data provided to you by your instructor are possible sources. Examples of appropriate portfolio pieces are listed below. I hope some or all of these will be worked on by members of the class. You should develop and work on your projects individually (except for the group project), but discussion with the instructor and others class members is encouraged and in fact expected. 0.7.1.1 Possible Projects Draw plots by hand of some data that are of interest to you, and transform the variables in several different ways. Interpret your results. Choose some data from EDA or VDat; table or plot them in a way that Tukey/Cleveland didnt. Find some population data of interest to you (e.g., North Carolina, Forsyth County,your cat herd, etc.) and do several hand plots like those in Chapter 5 of EDA. Interpret results. Find some data in the World Almanac and plot and/or table them. Use some two-way data, and repeatedly extract the medians like Tukey does in Ch. 10 &amp; 11. Find some time series data, and smooth them in several different ways (see EDA, ch. 7). Data with seasonal patterns are especially interesting (see VDat, pp. 152-172). Write an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA, or other program to portray influence-enhanced scatterplots. Produce scatterplots of several relationships. Write a BASIC, FORTRAN, C, SAS, SPSS, JAVA, or other program to portray scatter plots on a computer. Give the user the option to plot X and/or Y as either raw data, logs, squares, cubes, reciprocals, roots, etc. Write an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA or other program to produce some exotic version of stem-and-leaf diagrams. Write a an R, SAS-Graph, SPSS, BASIC, FORTRAN, C, JAVA or other program to plot in three-dimensions with time as one of the dimensions (i.e., a kinostatistical plot). Use R or SAS-Graph or some other dedicated graphical package to plot some interesting data (preferably in color, possibly in 3D, maybe even in higher than 3D). Write an R/SAS routine to do median smoothing by three, and use it on some data. Write an R program, or SAS MACRO or SAS PROC or SAS program to produce some EDA output (but dont duplicate what PROC UNIVARIATE already does). Find an R program in the R library that does interesting EDA; apply it to some interesting data. Produce a correlation matrix between many variables, and develop a scatterplot matrix from it. Read the literature on graphical data analysis and develop some new graphical techniques. Program your techniques. Apply them to real data. Invent a new EDA graphical application, and apply it to real data. Additional ideas that arent thoroughly thought out: Data Cleaning Project using Lab Data Web scraping project Tidy Tuesday Project Data Innovation Recreate a classic visualization Interactive Project (Rshiny) Infographic masters thesis / first year project Misleading Graph Impossible to Read Colorblind Friendly Visualization that only uses X colors Animated/video Tutorial Webscraper data Digital Humanities Project reproduce findings from an article in your content area machine learn! live dashboard maps/ geospatial lie to me graphic 0.7.1.2 Documenting Your Project You should keep a log describing all EDA projects you undertake. At the end of the course, two things will happen. First, you will give a 12- to 15-minute asynchronous presentation in which you choose one of your EDA projects to describe to the class. Your description should include the goal of the project, the data you used, and a demonstration (PowerPoint, handout, holdup, computer demo, etc.) of the product. 2nd, you will turn in a Portfolio, which consists of two components: A report describing all your projects. The total number depends on the scope and difficulty of each project (as specified in your contract). There may be projects that you dont finish. Thats fine; EDA projects are hardly ever completely finished; write them up anyway. The projects should be numbered consecutively (i.e., in the order in which you began them), and should include for each project a description of the goal, the product (computer program, hand graph, computer graph, etc.), the data, and some interpretation. Reports must be Word Processed and of high quality in terms of writing, grammar, presentation, etc. A prototypical example of the product of each project (e.g., a graph, computer code, etc.). You may wish to put computer output into binders or appendices, graphs into report folders, etc. Portfolios will not be returned; if you wish to have a copy, make one before you turn it in. Portfolios are due the last week of class. Project reports will not be accepted late. Please, no exceptions!!! 0.7.1.3 What is an unessay? An unessay is exactly what the name sounds like. It is not an essay (although it can be one if youd like). What it is is a means to highlight what you have learned. In this class, effectively what that means is that you get to select either a specific portfolio piece or perhaps several of them to highlight what you have learned in this class. 0.7.1.3.1 What makes a good unessay? A good unessay constitutes a critical and active engagement with the course material that shows insight and creativity and demonstrates time and effort devoted to creating something thoughtful. The chosen medium works persuasively with the design and polish of the unessay. The projects structural and formal elements productively serve the core concept of the unessay. The unessay includes a clear and insightful connection between your three choices and reflects a convincing and nuanced thesis. An A unessay come with a clearly stated explanation. This will include your thesis and an explanation of how your unessay responds to the prompt. 0.7.1.4 More Information about contract grading https://marckissel.netlify.app/post/on-the-unessay/ https://esclark.hcommons.org/the-unessay/ https://ryancordell.org/teaching/how-not-to-teach-digital-humanities/ "],["welcome-to-data-science.html", "1 Welcome to Data Science 1.1 Module Materials", " 1 Welcome to Data Science This module is designed to introduce you to data science. Please watch the videos and work your way through the notes. You can find the module playlist here. Most of the slides used to make the videos in this module can be found in the slides repo. 1.1 Module Materials Videos Located in the subchapters of this module Slidedecks Welcome Slides Meet the toolkit Suggested Readings All subchapters of this module, including R basics and workflow R4DS Book Introduction Data exploration Introduction Happy Git with R If Happy Git is too much, start here If Short Happy Git is too much, start with Oh My Git Activities UN Voting Covid Data Bechdal Test Oh My Git Lab Hello R 1.1.1 Estimated Video Length No of videos : 8 Average length of video: 12 minutes, 6 seconds Total length of playlist: 1 hour, 36 minutes, 48 seconds "],["what-is-data-science.html", "2 What is Data Science? 2.1 See for yourselves! 2.2 Course structure and some other useful things", " 2 What is Data Science? You can follow along with the slides here if they do not appear below. 2.1 See for yourselves! Ive embedded a few examples below. 2.1.1 Shiny App 2.1.2 Hans Rosling The video below is the shorter version. Hans Roslings 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. 2.1.3 Social Media Social media contains a ton of great (and terrible examples of data science in action. These examples range from entire subreddits, such as /r/DataisBeautiful (be sure to check out the highest voted posts) to celebrity tweets about data scientists. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones  (@Lesdoggg) January 5, 2021 Good reasons to not be a Data Scientist:- It is a lot of work- Literally nobody will know what you&#39;re talking about- In the end, your computer will be your only real friend&mdash;  Kareem Carr  (@kareem_carr) January 22, 2021 2.1.4 Read for yourselves! Link Preview What is Data Science @ Oreilly Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdiscplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: heres a lot of data, what can you make from it? What is Data Science @ Quora Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyze it, and find innovative solutions to difficult problems. Basically Data Science is all about Analyzing data and driving for business growth by finding creative ways. The sexiest job of 21st century Data scientists today are akin to Wall Street quants of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed masters programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs. Wikipedia Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. How to Become a Data Scientist Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientists role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations. a very short history of #datascience The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young onecomputer science. The term Data Science has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term Data Science and its use, attempts to define it, and related terms. 2.2 Course structure and some other useful things You can follow along with the slides here if they do not appear below. "],["choice-your-own-activity.html", "3 Choice your own activity 3.1 UN Votes 3.2 Covid Data", " 3 Choice your own activity You can do either activity. The choice is yours. 3.1 UN Votes You can find the materials for the UN activity here. The compiled version should look something like the following 3.2 Covid Data You can find the materials for the Covid version of this activity here. The compiled version should look something like the following "],["meet-our-toolbox.html", "4 Meet our toolbox! 4.1 R and RStudio", " 4 Meet our toolbox! You can follow along with the slides here if they do not appear below. I recommend installing R, Rstudio, git, and github before starting activity 02 4.1 R and RStudio 4.1.1 Install R and RStudio library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 4.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. 4.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage 4.1.4 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["bechdal.html", "5 Bechdel Activity", " 5 Bechdel Activity You can find the materials for the Bechdel activity here. The compiled version should look something like the following "],["thoughtful-workflow.html", "6 Thoughtful Workflow 6.1 R Markdown 6.2 Git and Github 6.3 Getting Help with R", " 6 Thoughtful Workflow At this point, I recommend you pause and think about your workflow. I give you permission to spend some time and energy sorting this out! It can be as or more important than learning a new R function or package. The experts dont talk about this much, because theyve already got a workflow; its something they do almost without thinking. Working through subsequent material in R Markdown documents, possibly using Git and GitHub to track and share your progress, is a great idea and will leave you more prepared for your future data analysis projects. Typing individual lines of R code is but a small part of data analysis and it pays off to think holistically about your workflow. If you want a lot more detail on workflows, you can wander over to the optional bit on r basics and workflow. 6.1 R Markdown If you are in the mood to be entertained, start the video from the beginning. But if youd rather just get on with it, start watching at 6:52. You can follow along with the slides here if they do not appear below. R Markdown is a very accessible way to create computational documents that combine prose and tables and figures produced by R code. An introductory R Markdown workflow, including how it intersects with Git, GitHub, and RStudio, is now maintained within the Happy Git site: Test drive R Markdown 6.2 Git and Github XKCD on Git First, its important to realize that Git and GitHub are distinct things. GitHub is an online hosting platform that provides an array of services built on top of the Git system. (Similar platforms include Bitbucket and GitLab.) Just like we dont need Rstudio to run R code, we dont need GitHub to use Git But, it will make our lives so much easier. I recommend checking out Jenny Bryans instructions around installation, setup, and early Git usage with her book Happy Git with R. I have provided a optional deep dive in a later chapter](#happygit). You can follow along with the slides here if they do not appear below. 6.2.1 What is Github? 6.2.2 Git Git is a distributed version control system. (Wait, what?) Okay, try this: Imagine if Dropbox and the Track changes feature in MS Word had a baby. Git would be that baby. In fact, its even better than that because Git is optimized for the things that social scientists and data scientists spend a lot of time working on (e.g. code). The learning curve is worth it  I promise you. Git and GitHubs role in global software development is not in question. - Theres a high probability that your favorite app, program or package is built using Git-based tools. (RStudio is a case in point.) Scientists and academic researchers are cottoning on too. Benefits of version control and collaboration tools aside, Git(Hub) helps to operationalize the ideals of open science and reproducibility. Journals have increasingly strict requirements regarding reproducibility and data access. GH makes this easy (DOI integration, off-the-shelf licenses, etc.). I run my entire lab on GH; this entire course is running on github; these lecture notes are hosted on github 6.3 Getting Help with R You can follow along with the slides here if they do not appear below. "],["r-basics.html", "7 Optional Deep Dive: R basics and workflows 7.1 Basics of working with R at the command line and RStudio goodies 7.2 Workspace and working directory 7.3 RStudio projects 7.4 Stuff", " 7 Optional Deep Dive: R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 This chapter is a recommended, but optional deep dive (ODD) that might be useful to you. 7.1 Basics of working with R at the command line and RStudio goodies Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects  assignments  have this form: objectName &lt;- value and in my head I hear, e.g., x gets 12. You will make lots of assignments and the operator &lt;- is a pain to type. Dont be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudios keyboard shortcut: Alt + - (the minus sign). Notice that RStudio auto-magically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this, try out RStudios completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2 ^ 3 Lets try to inspect: masonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;masonrocks&#39; not found masn_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;masn_rocks&#39; not found Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Get better at typing. R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Lets try using seq() which makes regular sequences of numbers and, while were at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a functions arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Since we didnt specify step size, the default value of by in the function definition is used, which ends up being 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you dont get to see the value, so then youre tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and print to screen to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Mon May 03 16:13:52 2021&quot; Now look at your workspace  in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudios Environment pane. 7.2 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is real, i.e. will you save it as your lasting record of what happened? Where does your analysis live? 7.2.1 Workspace, .RData As a beginning R user, its OK to consider your workspace real. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as real. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as real (saving and reloading all the time), if you need to redo analysis  youre going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your good R code in a script for future reuse. Because it can be useful sometimes, note the commands youve recently run appear in the History pane. But you dont have to choose right now and the two strategies are not incompatible. Lets demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. Youll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click Save. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and youll see the same objects as before. In the History tab of the same pane, you should also see your command history. Youre back in business. This way of starting and stopping analytical work will not serve you well for long but its a start. 7.2.2 Working directory Any process running on your computer has a notion of its working directory. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, its OK let your home directory or any other weird directory on your computer be Rs working directory. Very soon, I urge you to evolve to the next level, where you organize your projects into directories and, when working on project A, set Rs working directory to the associated directory. Although I do not recommend it, in case youre curious, you can set Rs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudios Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (Youll see even more options there). Or within the Files pane, choose More and Set As Working Directory. But theres a better way. A way that also puts you on the path to managing your R work like an expert. 7.3 RStudio projects Keeping all the files associated with a project organized together  input data, R scripts, results, figures  is such a wise and common practice that RStudio has built-in support for this via its projects. Lets make one to use for the rest of this class. Do this: File &gt; New Project. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the home directory for your project is the working directory of our current R process: getwd() I cant print my output here because this document itself does not reside in the RStudio Project we just created. Lets enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.501 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Lets say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click To Source. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line  whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click Run in the upper right corner of editor pane). Source the entire document  equivalent to entering source('toy-line.r') in the Console  by keyboard shortcut (Shift+Command+S) or mouse (click Source in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 7.4 Stuff It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like youve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code youre saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (well soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (Im one of them), never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R and/or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This method is rarely necessary when using RStudio projects. "],["shorthappygit.html", "8 ODD: Getting Started with Github 8.1 Half the battle 8.2 Register a GitHub account 8.3 Install Git 8.4 Windows 8.5 Introduce yourself to Git 8.6 Install a Git client", " 8 ODD: Getting Started with Github This optional deep dive (ODD) is partially adapted from Jenny Bryans happygitwithr. I encourage you to check out the unabridged version as it has so much more detail. 8.1 Half the battle Getting all the necessary software installed, configured, and playing nicely together is honestly half the battle when first adopting Git. Brace yourself for some pain. The upside is that you can give yourself a pat on the back once you get through this. And you WILL get through this. You will find far more resources for how to use Git than for installation and configuration. Why? The experts  Have been doing this for years. Its simply not hard for them anymore. Probably use some flavor of Unix. They may secretly (or not so secretly) take pride in neither using nor knowing Windows. Get more satisfaction and reward for thinking and writing about Git concepts and workflows than Git installation. In their defense, its hard to write installation instructions. Failures can be specific to an individual OS or even individual computer. 8.2 Register a GitHub account Register an account with GitHub. Its free! https://github.com 8.2.1 Username advice You will be able to upgrade to a paid level of service, apply discounts, join organizations, etc. in the future, so dont fret about any of that now. Except your username. You might want to give that some thought. A few tips, which sadly tend to contradict each other: Incorporate your actual name! People like to know who theyre dealing with. Also makes your username easier for people to guess or remember. Reuse your username from other contexts, e.g., Twitter or Slack. But, of course, someone with no GitHub activity will probably be squatting on that. Pick a username you will be comfortable revealing to your future boss. Shorter is better than longer. Be as unique as possible in as few characters as possible. In some settings GitHub auto-completes or suggests usernames. Make it timeless. Dont highlight your current university, employer, or place of residence, e.g. JennyFromTheBlock. Avoid words laden with special meaning in programming. In Jennys first inept efforts to script around the GitHub API, she assigned lots of issues to the guy with username NA because my vector of GitHub usernames contained missing values. A variant of Little Bobby Tables. Avoid the use of upper vs. lower case to separate words. We highly recommend all lowercase. GitHub treats usernames in a case insensitive way, but using all lowercase is kinder to people doing downstream regular expression work with usernames, in various languages. A better strategy for word separation is to use a hyphen - or underscore _. You can change your username later, but better to get this right the first time. https://help.github.com/articles/changing-your-github-username/ https://help.github.com/articles/what-happens-when-i-change-my-username/ 8.2.2 Free private repos GitHub offers free unlimited private repositories for all users. These free private repositories support up to three external collaborators, making them a perfect place for your personal projects, for job applications, and testing things out before making your project open source. Go ahead and register your free account NOW and then pursue any special offer that applies to you: Students, faculty, and educational/research staff: GitHub Education. GitHub Organizations can be extremely useful for courses or research/lab groups, where you need some coordination across a set of repos and users. Official nonprofit organizations and charities: GitHub for Good 8.3 Install Git You need Git, so you can use it at the command line and so RStudio can call it. If theres any chance its installed already, verify that, rejoice, and skip this step. Otherwise, find installation instructions below for your operating system. 8.3.1 Git already installed? Go to the shell (More info on shell from Jenny Bryan). Enter which git to request the path to your Git executable: which git and git --version to see its version: git --version If you are successful, thats great! You have Git already. No need to install! Move on. If, instead, you see something more like git: command not found, keep reading. macOS users might get an immediate offer to install command line developer tools. Yes, you should accept! Click Install and read more below. 8.4 Windows Option 1 (highly recommended): Install Git for Windows, also known as msysgit or Git Bash, to get Git in addition to some other useful tools, such as the Bash shell. Yes, all those names are totally confusing, but you might encounter them elsewhere and I want you to be well-informed. We like this because Git for Windows leaves the Git executable in a conventional location, which will help you and other programs, e.g. RStudio, find it and use it. This also supports a transition to more expert use, because the Git Bash shell will be useful as you venture outside of R/RStudio. NOTE: When asked about Adjusting your PATH environment, make sure to select Git from the command line and also from 3rd-party software. Otherwise, we believe it is good to accept the defaults. Note that RStudio for Windows prefers for Git to be installed below C:/Program Files and this appears to be the default. This implies, for example, that the Git executable on my Windows system is found at C:/Program Files/Git/bin/git.exe. Unless you have specific reasons to otherwise, follow this convention. This also leaves you with a Git client, though not a very good one. So check out Git clients we recommend (chapter 8.6). FYI, this appears to be equivalent to what you would download from here: https://git-scm.com/download/. Additional approaches for Windows can be found here 8.4.1 macOS Although I (Mason) have limited knowledge about the inner workings of mac, I do know of quantitative psychologists who use macs with R, including Bill Revelle  author of the psych package and Full Professor at Northwestern. Option 1 (highly recommended): Install the Xcode command line tools (not all of Xcode), which includes Git. Go to the shell and enter one of these commands to elicit an offer to install developer command line tools: git --version git config Accept the offer! Click on Install. Heres another way to request this installation, more directly: xcode-select --install We just happen to find this Git-based trigger apropos. Note also that, after upgrading macOS, you might need to re-do the above and/or re-agree to the Xcode license agreement. We have seen this cause the RStudio Git pane to disappear on a system where it was previously working. Use commands like those above to tickle Xcode into prompting you for what it needs, then restart RStudio. Option 2 (recommended): Install Git from here: http://git-scm.com/downloads. This arguably sets you up the best for the future. It will certainly get you the latest version of Git of all approaches described here. The GitHub home for the macOS installer is here: https://github.com/timcharper/git_osx_installer. At that link, you can find more info if something goes wrong or you are working on an old version of macOS. Additional approaches for macOS can be found here 8.5 Introduce yourself to Git In the shell (More info on shell from Jenny Bryan): git config --global user.name &#39;Jane Doe&#39; git config --global user.email &#39;jane@example.com&#39; git config --global --list substituting your name and the email associated with your GitHub account. The usethis package offers an alternative approach. You can set your Git user name and email from within R: ## install if needed (do this exactly once): ## install.packages(&quot;usethis&quot;) library(usethis) use_git_config(user.name = &quot;Jane Doe&quot;, user.email = &quot;jane@example.org&quot;) 8.5.1 More about git config An easy way to get into a shell from RStudio is Tools &gt; Terminal or Tools &gt; Shell. (More info on shell from Jenny Bryan). Special Windows gotchas: If you are struggling on Windows, consider there are different types of shell and you might be in the wrong one. You want to be in a Git Bash shell, as opposed to Power Shell or the legacy cmd.exe command prompt. This might also be a reason to do this configuration via the usethis package in R. What user name should you give to Git? This does not have to be your GitHub user name, although it can be. Another good option is your actual first name and last name. If you commit from different machines, sometimes people work that info into the user name. Your commits will be labeled with this user name, so make it informative to potential collaborators and future you. What email should you give to Git? This must be the email associated with your GitHub account. These commands return nothing. You can check that Git understood what you typed by looking at the output of git config --global --list. 8.5.2 Configure the Git editor Another Git option that many people eventually configure is the editor. At some point, you will fail to give Git what it wants in terms of a commit message and it will kick you into an editor. This can be distressing, if its not your editor of choice and you dont even know how to save and quit. You can enforce your will with something along these lines: git config --global core.editor &quot;emacs&quot; Substitute your preferred editor for \"emacs\" here. Software Carpentrys Git lesson has a comprehensive listing of the exact git config command needed for many combinations of OS and editor. 8.6 Install a Git client Although having a git client is, I highly recommend it for the same reasons as I recommend having Rstudio. Learning to use version control can be rough at first. I found the use of a GUI  as opposed to the command line  extremely helpful when I was getting started. I call this sort of helper application a Git client. Its really a Git(Hub) client because it also helps you interact with GitHub or other remotes. 8.6.1 What is a Git client? Why would you want one? Git is really just a collection of individual commands you execute in the shell (Appendix ??). This interface is not appealing for everyone. Some may prefer to do Git operations via a client with a graphical interface. Git and your Git client are not the same thing, just like R and RStudio are not the same thing. A Git client and an integrated development environment, such as RStudio, are not necessary to use Git or R, respectively. But they make the experience more pleasant because they reduce the amount of command line bullshittery and provide a richer visual representation of the current state. RStudio offers a very basic Git client via its Git pane. I use this often for simple operations, but you probably want another, more powerful one as well. Fair warning: for some tasks, you must use the command line. But the more powerful your Git client is, the less often this happens. The visual overview given by your Git client can also be invaluable for understanding the current state of things, even when preparing calls to command line Git. Fantastic news: because all of the clients are just forming and executing Git commands on your behalf, you dont have to pick one. You can literally do one operation from the command line, do another from RStudio, and another from SourceTree, one after the other, and it just works. Very rarely, both clients will scan the repo at the same time and youll get an error message about .git/index.lock. Try the operation again at least once before doing any further troubleshooting. 8.6.2 A picture is worth a thousand words Heres a screenshot of SourceTree (see below) open to the repository for this site. You get a nice graphical overview of the recent commit history, branches, and diffs, as well as a GUI that facilitates the most common Git operations. SourceTree screenshot In contrast, heres a shell session where Ive used command line Git to access some of the same information. Command line Git Which do you prefer? 8.6.3 No one is giving out Git Nerd merit badges Work with Git in whatever way makes you most effective. Feel free to revisit your approach over time or to use different approaches for different tasks. No one can tell whether you use the command line or a GUI when they look at your Git history or your GitHub repo. If your Git life happens on your own computer, there is no reason to deny yourself a GUI if thats what you like. If you prefer working in the shell or if you frequently log into a remote server, then it makes sense to prioritize building Git skills at the command line. Do whatever works for you, but dont do anything for the sake of purity or heroism. 8.6.4 Recommended Git clients GitKraken is a free, powerful Git(Hub) client that is Jenny Bryans current favorite. Its especially exciting because it works on Windows, macOS, and Linux. This is great news, especially for long-suffering Linux users who have previously had very few options. SourceTree is another free client that Jenny highly recommends, at least on Windows.1 It was hery first and most beloved Git client, but she eventually had to give it up on macOS, due to a long-standing bug re: leaking file handles that they will clearly never fix. I still use SourceTree on Windows. GitHub offers a free Git(Hub) client, GitHub Desktop, for Windows and macOS. GitHub Desktop is aimed at beginners who want the most useful features of Git front and center. The flipside is that it may not support some of the more advanced workflows exposed by the clients above. At present, this client is what I mostly use. Others that I have heard positive reviews for: magit, for Emacs nerds GitUp SmartGit git-cola Browse even more Git(Hub) clients. During installation and registration, youll need to create a free Atlassian Bitbucket account and link that to a free Atlassian Bitbucket Cloud account. Also, feel free to uncheck the checkbox about installing Mercurial (another version control system), unless you feel you need it. "],["lab01.html", "9 Lab: Hello R! 9.1 Lab Goals 9.2 Getting started 9.3 Exercises", " 9 Lab: Hello R! The labs for this course have been adapted from a series of Rstudio tutorials. These tutorials were initially created by Mine Çetinkaya-Rundel. Mine is fantastic; her work is fantastic; and shes just a badass! I have adapted these tutorials for two reasons. 1) I think it useful to see other people working with R; and 2) Pragmatically, using Mines lab materials means that I can spend more time on other aspects of the course  like the website, course notes, videos, feedback, learning how to embed tweets That&#39;s so wonderful to hear, thank you!&mdash; Mine Çetinkaya-Rundel (@minebocek) January 22, 2021 Seriously, youd never know it but every hour of finished video takes between 4 and 6 hours to make. (2 hours of writing, 1.5 hours of filming, and 2.5 hours for video editing). 9.1 Lab Goals Recall: R is the name of the programming language itself and RStudio is a convenient interface. The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions. Recall: git is a version control system (like Track Changes features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the Internet (like DropBox but much, much better). An additional goal is to introduce you to Git and GitHub, which is the collaboration and version control system that we will be using throughout the course. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands. To make versioning simpler, this lab is a solo lab. I want to make sure everyone gets a significant amount of time at the steering wheel, working directly with R. In the future modules,youll learn about collaborating on GitHub and produce a single lab report for your team. 9.2 Getting started Each of your assignments will begin with the following steps. Theyre outlined in detail here. Going forward, each lab will start with a Getting started section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment. You can find the assignment link for this lab right here. That GitHub repository (which well refer to as repo going forward) is a template for the assignment. You can build on it to complete your assignment. On GitHub, click on the green Clone or download button, select Use HTTPS (this might already be selected by default, and if it is, youll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option. Copy and paste the URL of your assignment repo into the dialog box: Hit OK, and youre good to go! 9.2.1 Warm up Before we introduce the data, lets warm up with some simple exercises. FYI: The top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for YAML Aint Markup Language. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document. 9.2.1.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 9.2.1.2 Committing changes Then go to the Git pane in your RStudio. If you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If youre happy with these changes, write Update author name in the Commit message box and hit Commit. You dont have to commit after every change, doing so would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments, Ill suggest exactly when to commit and in some cases, what commit message to use. As the semester progresses, you make these decisions. 9.2.1.3 Pushing changes Now that you have made an update and committed this change, its time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only). In order to push your changes to GitHub, click on Push. This will prompt a dialog box where you first need to enter your user name, and then your password. This might feel cumbersome. Soon  you will learn how to save your password so you dont have to enter it every time. But for this one assignment youll have to manually enter each time you push in order to gain some experience with it. 9.2.2 Packages In this lab, we will work with two packages: datasauRus and tidyverse. datasauRus contains the dataset well be using; tidyverse is a collection of packages for doing data analysis in a tidy way. Install these packages by running the following commands in the console. install.packages(&quot;tidyverse&quot;) install.packages(&quot;datasauRus&quot;) Now that the necessary packages are installed, you should be able to Knit your document and see the results. If youd like to run your code in the Console as well youll also need to load the packages there. To do so, run the following in the console. library(tidyverse) library(datasauRus) Note that the packages are also loaded with the same commands in your R Markdown document. 9.2.3 Data Fun fact: If its confusing that the data frame is called datasaurus_dozen when it contains 13 datasets, youre not alone! Have you heard of a bakers dozen? The data frame we will be working with today is called datasaurus_dozen and its in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable. To find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be run in the Console. 9.3 Exercises Based on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. Lets take a look at what these datasets are. To do so we can make a frequency table of the dataset variable: datasaurus_dozen %&gt;% count(dataset) %&gt;% print(13) #&gt; # A tibble: #&gt; # 13 x 2 #&gt; dataset #&gt; &lt;chr&gt; #&gt; 1 away #&gt; 2 bullseye #&gt; 3 circle #&gt; 4 dino #&gt; 5 dots #&gt; 6 h_lines #&gt; 7 high_lines #&gt; 8 slant_down #&gt; 9 slant_up #&gt; 10 star #&gt; 11 v_lines #&gt; 12 wide_lines #&gt; 13 x_shape #&gt; # ... with 1 #&gt; # more #&gt; # variable: #&gt; # n &lt;int&gt; Fun fact: Matejka, Justin, and George Fitzmaurice. Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017. The original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets that have the same summary statistics as the Datasaurus but have very different distributions.    Knit, commit, and push your changes to GitHub with the commit message Added answer for Ex 1. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset. Below is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results. Start with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data. dino_data &lt;- datasaurus_dozen %&gt;% filter(dataset == &quot;dino&quot;) Because a lot going on here  lets slow down and unpack it a bit. First, the pipe operator: %&gt;%, takes what comes before it and sends it as the first argument to what comes after it. So here, were saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\". Second, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame. Next, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data youre visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case, we want these to be points, hence geom_point. ggplot(data = dino_data, mapping = aes(x = x, y = y)) + geom_point() If this seems like a lot, it is. And you will learn about the philosophy of layering data visualizations in detail next week. For now, follow along with the code that is provided. For the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. Recall: Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This nonlinear relationships are exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesnt make sense since the relationship between x and y is definitely not linear  its dinosaurial! But, for illustrative purposes, lets calculate the correlation coefficient between x and y. Tip: Start with dino_data and calculate a summary statistic that we will call r as the correlation between x and y. dino_data %&gt;% summarize(r = cor(x, y)) #&gt; # A tibble: 1 x 1 #&gt; r #&gt; &lt;dbl&gt; #&gt; 1 -0.0645    Knit, commit, and push your changes to GitHub with the commit message Added answer for Ex 2. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?    This is another good place to pause, knit, commit changes with the commit message Added answer for Ex 3, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Plot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?    You should pause again, commit changes with the commit message Added answer for Ex 4, and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Hint: Facet by the dataset variable, placing the plots in a 3 column grid, and dont add a legend. Finally, lets plot all datasets at once. In order to do this we will make use of faceting. ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+ geom_point()+ facet_wrap(~ dataset, ncol = 3) + theme(legend.position = &quot;none&quot;) And we can use the group_by function to generate all the summary correlation coefficients. datasaurus_dozen %&gt;% group_by(dataset) %&gt;% summarize(r = cor(x, y)) %&gt;% print(13) Youre done with the data analysis exercises, but wed like you to do two more things: Resize your figures: Click on the gear icon near the top of the R Markdown document, and select Output Options in the dropdown menu. In the pop up dialog box, go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until youre happy with the figure sizes. Note that these values get saved in the YAML. You can also use different figure sizes for different figures. To do so, click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well. Change the look of your report: Once again, click on the gear icon in on top of the R Markdown document, and select Output Options in the dropdown menu. In the General tab of the pop up dialog box, try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until youre happy with the look. Pro Tip: Not sure how to use emojis on your computer? Maybe a teammate can help?    Yay, youre done! Commit all remaining changes, use the commit message \"Done with Lab 1! \", and push. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo. "],["welcome-to-data-and-visualization.html", "10 Welcome to Data and Visualization 10.1 Module Materials", " 10 Welcome to Data and Visualization This module is designed to introduce you to exploratory and graphical data analysis. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. The slides used to make the videos in this module can be found in the slides repo. 10.1 Module Materials Slides from Lectures Data and visualization Visualizing data with ggplot2 Visualizing numerical data Visualizing categorical data Suggested Readings All subchapters of this module, including Basic care and feeding of data in R R4DS Data Exploratation Section, including Data visualization Exploratory Data Analysis Activities Star Wars! Lab Plastic waste 10.1.1 Estimated Video Length No of videos : 9 Average length of video : 13 minutes, 34 seconds Total length of playlist : 2 hours, 2 minutes, 11 seconds "],["exploratory-data-analysis.html", "11 Exploratory Data Analysis 11.1 What is in a dataset?", " 11 Exploratory Data Analysis You can follow along with the slides here if they do not appear below. 11.1 What is in a dataset? 11.1.1 Why do we visualize? "],["visualizing-data-with-ggplot2.html", "12 Visualizing data with ggplot2 12.1 ggplot2 and aesthetics", " 12 Visualizing data with ggplot2 Does anyone else feel that their affinity for an artistic skill helps them with statistics? Just me?&mdash; Valerie Polad (@valeriepolad) May 2, 2021 You can follow along with the slides here if they do not appear below. 12.1 ggplot2 and aesthetics "],["visualizing-numerical-data.html", "13 Visualizing numerical data 13.1 Looking at Data 13.2 More on visualizing numerical data", " 13 Visualizing numerical data You can follow along with the slides here if they do not appear below. 13.1 Looking at Data 13.2 More on visualizing numerical data Fun fact, when you screen capture with f.lux running in the background, f.lux is captured too. "],["visualizing-categorical-data.html", "14 Visualizing categorical data", " 14 Visualizing categorical data You can follow along with the slides here if they do not appear below. "],["star-wars-activity.html", "15 Star Wars Activity!", " 15 Star Wars Activity! You can find the materials for the Star Wars activity here. The compiled version should look something like the following "],["visualization-examples.html", "16 Visualization Examples 16.1 Census Reporter Data from North Carolina", " 16 Visualization Examples Here are some fun examples of data visualization. Eventually, this section will include examples from previous classes. :) For now, its going to just include random stuff. 16.1 Census Reporter Data from North Carolina "],["basic-data-care.html", "17 ODD: Basic care and feeding of data in R 17.1 Buckle your seatbelt 17.2 Data frames are awesome 17.3 Get the Gapminder data 17.4 Meet the gapminder data frame or tibble 17.5 Look at the variables inside a data frame 17.6 Recap", " 17 ODD: Basic care and feeding of data in R This section is an optional deep dive (ODD) that might be useful to you. Feel free to ignore it if you dont need this bit of support. 17.1 Buckle your seatbelt Now is the time to make sure you are working in an appropriate directory on your computer, probably through the use of an RStudio project. Enter getwd() in the Console to see current working directory or, in RStudio, this is displayed in the bar at the top of Console. You should clean out your workspace. In RStudio, click on the Clear broom icon from the Environment tab or use Session &gt; Clear Workspace. You can also enter rm(list = ls()) in the Console to accomplish same. Now restart R. This will ensure you dont have any packages loaded from previous calls to library(). In RStudio, use Session &gt; Restart R. Otherwise, quit R with q() and re-launch it. Why do we do this? So that the code you write is complete and re-runnable. If you return to a clean slate often, you will root out hidden dependencies where one snippet of code only works because it relies on objects created by code saved elsewhere or, much worse, never saved at all. Similarly, an aggressive clean slate approach will expose any usage of packages that have not been explicitly loaded. Finally, open a new R script and develop and run your code from there. In RStudio, use File &gt; New File &gt; R Script. Save this script with a name ending in .r or .R, containing no spaces or other funny stuff, and that evokes whatever it is were doing today. Example: cm004_data-care-feeding.r. Another great idea is to do this in an R Markdown document. See Test drive R Markdown for a refresher. 17.2 Data frames are awesome Whenever you have rectangular, spreadsheet-y data, your default data receptacle in R is a data frame. Do not depart from this without good reason. Data frames are awesome because Data frames package related variables neatly together, keeping them in sync vis-a-vis row order applying any filtering of observations uniformly Most functions for inference, modeling, and graphing are happy to be passed a data frame via a data = argument. This has been true in base R for a long time. The set of packages known as the tidyverse takes this one step further and explicitly prioritizes the processing of data frames. This includes popular packages like dplyr and ggplot2. In fact the tidyverse prioritizes a special flavor of data frame, called a tibble. Data frames  unlike general arrays or, specifically, matrices in R  can hold variables of different flavors, such as character data (subject ID or name), quantitative data (white blood cell count), and categorical information (treated vs. untreated). If you use homogeneous structures, like matrices, for data analysis, you are likely to make the terrible mistake of spreading a dataset out over multiple, unlinked objects. Why? Because you cant put character data, such as subject name, into the numeric matrix that holds white blood cell count. This fragmentation is a Bad Idea. 17.3 Get the Gapminder data We will work with some of the data from the Gapminder project. Jenny Bryan has released this data as an R package, so we can install it from CRAN like so: install.packages(&quot;gapminder&quot;) Now load the package: library(gapminder) 17.4 Meet the gapminder data frame or tibble By loading the gapminder package, we now have access to a data frame by the same name. Get an overview of this with str(), which displays the structure of an object. str(gapminder) #&gt; tibble[,6] [1,704 x 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just str() some of the recently created objects to get some ideas about what to do next. We could print the gapminder object itself to screen. However, if youve used R before, you might be reluctant to do this, because large datasets just fill up your Console and provide very little insight. This is the first big win for tibbles. The tidyverse offers a special case of Rs default data frame: the tibble, which is a nod to the actual class of these objects, tbl_df. If you have not already done so, install the tidyverse meta-package now: install.packages(&quot;tidyverse&quot;) Now load it: library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v ggplot2 3.3.3 v purrr 0.3.4 #&gt; v tibble 3.1.0 v dplyr 1.0.5 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() Now we can boldly print gapminder to screen! It is a tibble (and also a regular data frame) and the tidyverse provides a nice print method that shows the most important stuff and doesnt fill up your Console. ## see? it&#39;s still a regular data frame, but also a tibble class(gapminder) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 1,694 more rows If you are dealing with plain vanilla data frames, you can rein in data frame printing explicitly with head() and tail(). Or turn it into a tibble with as_tibble()! head(gapminder) #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder) #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zimbabwe Africa 1982 60.4 7636524 789. #&gt; 2 Zimbabwe Africa 1987 62.4 9216418 706. #&gt; 3 Zimbabwe Africa 1992 60.4 10704340 693. #&gt; 4 Zimbabwe Africa 1997 46.8 11404948 792. #&gt; 5 Zimbabwe Africa 2002 40.0 11926563 672. #&gt; 6 Zimbabwe Africa 2007 43.5 12311143 470. as_tibble(iris) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # ... with 140 more rows More ways to query basic info on a data frame: names(gapminder) #&gt; [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; ncol(gapminder) #&gt; [1] 6 length(gapminder) #&gt; [1] 6 dim(gapminder) #&gt; [1] 1704 6 nrow(gapminder) #&gt; [1] 1704 A statistical overview can be obtained with summary(): summary(gapminder) #&gt; country continent year lifeExp #&gt; Afghanistan: 12 Africa :624 Min. :1952 Min. :23.6 #&gt; Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.2 #&gt; Algeria : 12 Asia :396 Median :1980 Median :60.7 #&gt; Angola : 12 Europe :360 Mean :1980 Mean :59.5 #&gt; Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.8 #&gt; Australia : 12 Max. :2007 Max. :82.6 #&gt; (Other) :1632 #&gt; pop gdpPercap #&gt; Min. :6.00e+04 Min. : 241 #&gt; 1st Qu.:2.79e+06 1st Qu.: 1202 #&gt; Median :7.02e+06 Median : 3532 #&gt; Mean :2.96e+07 Mean : 7215 #&gt; 3rd Qu.:1.96e+07 3rd Qu.: 9325 #&gt; Max. :1.32e+09 Max. :113523 #&gt; Although we havent begun our formal coverage of visualization yet, its so important for smell-testing dataset that we will make a few figures anyway. Here we use only base R graphics, which are very basic. plot(lifeExp ~ year, gapminder) plot(lifeExp ~ gdpPercap, gapminder) plot(lifeExp ~ log(gdpPercap), gapminder) Lets go back to the result of str() to talk about what a data frame is. str(gapminder) #&gt; tibble[,6] [1,704 x 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... A data frame is a special case of a list, which is used in R to hold just about anything. Data frames are a special case where the length of each list component is the same. Data frames are superior to matrices in R because they can hold vectors of different flavors, e.g. numeric, character, and categorical data can be stored together. This comes up a lot! 17.5 Look at the variables inside a data frame To specify a single variable from a data frame, use the dollar sign $. Lets explore the numeric variable for life expectancy. head(gapminder$lifeExp) #&gt; [1] 28.8 30.3 32.0 34.0 36.1 38.4 summary(gapminder$lifeExp) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 23.6 48.2 60.7 59.5 70.8 82.6 hist(gapminder$lifeExp) The year variable is an integer variable, but since there are so few unique values it also functions a bit like a categorical variable. summary(gapminder$year) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1952 1966 1980 1980 1993 2007 table(gapminder$year) #&gt; #&gt; 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 #&gt; 142 142 142 142 142 142 142 142 142 142 142 142 The variables for country and continent hold truly categorical information, which is stored as a factor in R. class(gapminder$continent) #&gt; [1] &quot;factor&quot; summary(gapminder$continent) #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 levels(gapminder$continent) #&gt; [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) #&gt; [1] 5 The levels of the factor continent are Africa, Americas, etc. and this is whats usually presented to your eyeballs by R. In general, the levels are friendly human-readable character strings, like male/female and control/treated. But never ever ever forget that, under the hood, R is really storing integer codes 1, 2, 3, etc. Look at the result from str(gapminder$continent) if you are skeptical. str(gapminder$continent) #&gt; Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... This Janus-like nature of factors means they are rich with booby traps for the unsuspecting but they are a necessary evil. I recommend you resolve to learn how to properly care and feed for factors. The pros far outweigh the cons. Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Here we count how many observations are associated with each continent and, as usual, try to portray that info visually. This makes it much easier to quickly see that African countries are well represented in this dataset. table(gapminder$continent) #&gt; #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 barplot(table(gapminder$continent)) In the figures below, we see how factors can be put to work in figures. The continent factor is easily mapped into facets or colors and a legend by the ggplot2 package. Making figures with ggplot2 is covered in Chapter ?? so feel free to just sit back and enjoy these plots or blindly copy/paste. ## we exploit the fact that ggplot2 was installed and loaded via the tidyverse p &lt;- ggplot(filter(gapminder, continent != &quot;Oceania&quot;), aes(x = gdpPercap, y = lifeExp)) # just initializes p &lt;- p + scale_x_log10() # log the x axis the right way p + geom_point() # scatterplot p + geom_point(aes(color = continent)) # map continent to color p + geom_point(alpha = (1/3), size = 3) + geom_smooth(lwd = 3, se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; p + geom_point(alpha = (1/3), size = 3) + facet_wrap(~ continent) + geom_smooth(lwd = 1.5, se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 17.6 Recap Use data frames!!! Use the tidyverse!!! This will provide a special type of data frame called a tibble that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Always understand the basic extent of your data frames: number of rows and columns. Understand what flavor the variables are. Use factors!!! But with intention and care. Do basic statistical and visual sanity checking of each variable. Refer to variables by name, e.g., gapminder$lifeExp, not by column number. Your code will be more robust and readable. "],["lab02.html", "18 Lab: Global plastic waste 18.1 Learning goals 18.2 Getting started 18.3 Warm up 18.4 Exercises 18.5 Wrapping up", " 18 Lab: Global plastic waste Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010. Additionally, National Geographic ran a data visualization communication contest on plastic waste as seen here. 18.1 Learning goals Visualizing numerical and categorical data and interpreting visualizations Recreating visualizations Getting more practice using with R, RStudio, Git, and GitHub 18.2 Getting started Go to the course GitHub organization and locate the assignment repo template, which should be named lab-02-plastic-waste. If youre in the right place, it should look like the following. Fork or use the template to make your own repo, and then clone it in RStudio. First, open the R Markdown document lab-02.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 18.2.1 Packages Well use the tidyverse package for this analysis. Run the following code in the Console to load this package. library(tidyverse) 18.2.2 Data The dataset for this assignment can be found as a csv file in the data folder of your repository. You can read it in using the following. plastic_waste &lt;- read_csv(&quot;data/plastic-waste.csv&quot;) The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder 18.3 Warm up Recall that RStudio is divided into four panes. Without looking, can you name them all and briefly describe their purpose? Verify that the dataset has loaded into the Environment. How many observations are in the dataset? Clicking on the dataset in the Environment will allow you to inspect it more carefully. Alternatively, you can type View(plastic_waste) into the Console to do this. Hint: If youre not sure, run the command ?NA which will lead you to the documentation. Have a quick look at the data and notice that there are cells taking the value NA  what does this mean? 18.4 Exercises Lets start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_histogram(binwidth = 0.2) ## Warning: Removed 51 rows containing non-finite values (stat_bin). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. plastic_waste %&gt;% filter(plastic_waste_per_cap &gt; 3.5) ## # A tibble: 1 x 10 ## code entity continent year gdp_per_cap plastic_waste_p~ mismanaged_plast~ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TTO Trinida~ North Ame~ 2010 31261. 3.6 0.19 ## # ... with 3 more variables: mismanaged_plastic_waste &lt;dbl&gt;, coastal_pop &lt;dbl&gt;, ## # total_pop &lt;dbl&gt; Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. 1.1. Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? NOTE: From this point onwards, the plots and the output of the code are not displayed in the lab instructions, but you can and should the code and view the results yourself. Another way of visualizing numerical data is using density plots. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_density() And compare distributions across continents by coloring density curves by continent. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent)) + geom_density() The resulting plot may be a little difficult to read, so lets also fill the curves in with colors as well. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density() The overlapping colors make it difficult to tell whats happening with the distributions. The first plotted in continents get covered by continents plotted over them. We can change the transparency level of the fill color to help with this problem. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so you just need to try a few. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density(alpha = 0.7) This plot still doesnt look great 2.1. Recreate the density plots above using a different (lower) alpha level that works better for displaying the density curves for all continents. 2.2. Describe why we defined the color and fill of the curves by mapping aesthetics of the plot but we defined the alpha level as a characteristic of the plotting geom.    Now is a good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. And yet another way to visualize this relationship is using side-by-side box plots. ggplot(data = plastic_waste, mapping = aes(x = continent, y = plastic_waste_per_cap)) + geom_boxplot() 3.1. Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? Remember: We use geom_point() to make scatterplots. 4.1. Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. 4.2. color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? 4.3. Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?    Now is another good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 18.5 Wrapping up If you still have some time left, move on to the remaining exercises below. Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the data are not only represented with points on the plot but also a smooth curve. The term smooth should help you pick which geom to use. 5.1. Recreate the following plot, and interpret what you see in context of the data.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure youre happy with the final state of your work. Once youre done, check to make sure your latest changes are on GitHub. (If previous versions of this lab, there was an automated check to see if for your R Markdown document knitted properly. It required using github actions, which made this lab a bit too complicated.) "],["welcome-to-the-tidyverse.html", "19 Welcome to the tidyverse! 19.1 Module Materials 19.2 Estimated Video Length", " 19 Welcome to the tidyverse! This module is designed to introduce you to the key ideas of data wrangling and the grammar of tidyverse. In essence, data wrangling is the process of transforming and mapping data from one raw data form into a better-suited format. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. The slides used to make the videos in this module can be found in the slides repo. 19.1 Module Materials Slides from Lectures Tidy data Grammar of data wrangling Hands on Data Wrangling Working with Multiple Data Frames Suggested Readings All subchapters of this module, including Introduction to dplyr Merges on Github R4DS Data Wrangling, including Tidy Data Pipes Activities Hotels! Lab Nobel Laureates 19.2 Estimated Video Length No of videos : 8 Average length of video : 14 minutes, 11 seconds Total length of playlist : 1 hour, 53 minutes, 34 seconds "],["tidy-data.html", "20 Tidy data 20.1 Data structures in R", " 20 Tidy data You can follow along with the slides here if they do not appear below. 20.1 Data structures in R "],["grammar-of-data-wrangling.html", "21 Grammar of data wrangling 21.1 Piping", " 21 Grammar of data wrangling You can follow along with the slides here if they do not appear below. 21.1 Piping "],["dplyr-intro.html", "22 Introduction to dplyr 22.1 Think before you create excerpts of your data  22.2 Use filter() to subset data row-wise 22.3 Meet the new pipe operator 22.4 Use select() to subset the data on variables or columns. 22.5 Revel in the convenience 22.6 Pure, predictable, pipeable", " 22 Introduction to dplyr dplyr is a package for data manipulation, developed by Hadley Wickham and Romain Francois. It is built to be fast, highly expressive, and open-minded about how your data is stored. It is installed as part of the tidyverse meta-package and, as a core package, it is among those loaded via library(tidyverse). dplyrs roots are in an earlier package called plyr, which implements the split-apply-combine strategy for data analysis (Hadley Wickham 2011b). Where plyr covers a diverse set of inputs and outputs (e.g., arrays, data frames, lists), dplyr has a laser-like focus on data frames or, in the tidyverse, tibbles. dplyr is a package-level treatment of the ddply() function from plyr, because data frame in, data frame out proved to be so incredibly important. Have no idea what Im talking about? Not sure if you care? If you use these base R functions: subset(), apply(), [sl]apply(), tapply(), aggregate(), split(), do.call(), with(), within(), then you should keep reading. Also, if you use for() loops a lot, you might enjoy learning other ways to iterate over rows or groups of rows or variables in a data frame. 22.0.1 Load dplyr and gapminder I choose to load the tidyverse, which will load dplyr, among other packages we use incidentally below. library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v ggplot2 3.3.3 v purrr 0.3.4 #&gt; v tibble 3.1.0 v dplyr 1.0.5 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() Also load gapminder. library(gapminder) 22.0.2 Say hello to the gapminder tibble The gapminder data frame is a special kind of data frame: a tibble. gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 1,694 more rows Its tibble-ness is why we get nice compact printing. For a reminder of the problems with base data frame printing, go type iris in the R Console or, better yet, print a data frame to screen that has lots of columns. Note how gapminders class() includes tbl_df; the tibble terminology is a nod to this. class(gapminder) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; There will be some functions, like print(), that know about tibbles and do something special. There will others that do not, like summary(). In which case the regular data frame treatment will happen, because every tibble is also a regular data frame. To turn any data frame into a tibble use as_tibble(): as_tibble(iris) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # ... with 140 more rows 22.1 Think before you create excerpts of your data  If you feel the urge to store a little snippet of your data: (canada &lt;- gapminder[241:252, ]) #&gt; # A tibble: 12 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Canada Americas 1952 68.8 14785584 11367. #&gt; 2 Canada Americas 1957 70.0 17010154 12490. #&gt; 3 Canada Americas 1962 71.3 18985849 13462. #&gt; 4 Canada Americas 1967 72.1 20819767 16077. #&gt; 5 Canada Americas 1972 72.9 22284500 18971. #&gt; 6 Canada Americas 1977 74.2 23796400 22091. #&gt; 7 Canada Americas 1982 75.8 25201900 22899. #&gt; 8 Canada Americas 1987 76.9 26549700 26627. #&gt; 9 Canada Americas 1992 78.0 28523502 26343. #&gt; 10 Canada Americas 1997 78.6 30305843 28955. #&gt; 11 Canada Americas 2002 79.8 31902268 33329. #&gt; 12 Canada Americas 2007 80.7 33390141 36319. Stop and ask yourself  Do I want to create mini datasets for each level of some factor (or unique combination of several factors)  in order to compute or graph something? If YES, use proper data aggregation techniques or faceting in ggplot2  dont subset the data. Or, more realistic, only subset the data as a temporary measure while you develop your elegant code for computing on or visualizing these data subsets. If NO, then maybe you really do need to store a copy of a subset of the data. But seriously consider whether you can achieve your goals by simply using the subset = argument of, e.g., the lm() function, to limit computation to your excerpt of choice. Lots of functions offer a subset = argument! Copies and excerpts of your data clutter your workspace, invite mistakes, and sow general confusion. Avoid whenever possible. Reality can also lie somewhere in between. You will find the workflows presented below can help you accomplish your goals with minimal creation of temporary, intermediate objects. 22.2 Use filter() to subset data row-wise filter() takes logical expressions and returns the rows for which all are TRUE. filter(gapminder, lifeExp &lt; 29) #&gt; # A tibble: 2 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Rwanda Africa 1992 23.6 7290203 737. filter(gapminder, country == &quot;Rwanda&quot;, year &gt; 1979) #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Rwanda Africa 1982 46.2 5507565 882. #&gt; 2 Rwanda Africa 1987 44.0 6349365 848. #&gt; 3 Rwanda Africa 1992 23.6 7290203 737. #&gt; 4 Rwanda Africa 1997 36.1 7212583 590. #&gt; 5 Rwanda Africa 2002 43.4 7852401 786. #&gt; 6 Rwanda Africa 2007 46.2 8860588 863. filter(gapminder, country %in% c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) #&gt; # A tibble: 24 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 14 more rows Compare with some base R code to accomplish the same things: gapminder[gapminder$lifeExp &lt; 29, ] ## repeat `gapminder`, [i, j] indexing is distracting subset(gapminder, country == &quot;Rwanda&quot;) ## almost same as filter; quite nice actually Under no circumstances should you subset your data the way I did at first: excerpt &lt;- gapminder[241:252, ] Why is this a terrible idea? It is not self-documenting. What is so special about rows 241 through 252? It is fragile. This line of code will produce different results if someone changes the row order of gapminder, e.g. sorts the data earlier in the script. filter(gapminder, country == &quot;Canada&quot;) This call explains itself and is fairly robust. 22.3 Meet the new pipe operator Before we go any further, we should exploit the new pipe operator that the tidyverse imports from the magrittr package by Stefan Bache. This is going to change your data analytical life. You no longer need to enact multi-operation commands by nesting them inside each other, like so many Russian nesting dolls. This new syntax leads to code that is much easier to write and to read. Heres what it looks like: %&gt;%. The RStudio keyboard shortcut: Ctrl+Shift+M (Windows), Cmd+Shift+M (Mac). Lets demo, then Ill explain. gapminder %&gt;% head() #&gt; # A tibble: 6 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. This code is equivalent to head(gapminder). The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side  literally, drops it in as the first argument. Never fear, you can still specify other arguments to this function! To see the first 3 rows of gapminder, we could say head(gapminder, 3) or this: gapminder %&gt;% head(3) #&gt; # A tibble: 3 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. Ive advised you to think gets whenever you see the assignment operator, &lt;-. Similarly, you should think then whenever you see the pipe operator, %&gt;%. You are probably not impressed yet, but the magic will soon happen. 22.4 Use select() to subset the data on variables or columns. Back to dplyr. Use select() to subset the data on variables or columns. Heres a conventional call: select(gapminder, year, lifeExp) #&gt; # A tibble: 1,704 x 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 #&gt; 2 1957 30.3 #&gt; 3 1962 32.0 #&gt; 4 1967 34.0 #&gt; 5 1972 36.1 #&gt; 6 1977 38.4 #&gt; 7 1982 39.9 #&gt; 8 1987 40.8 #&gt; 9 1992 41.7 #&gt; 10 1997 41.8 #&gt; # ... with 1,694 more rows And heres the same operation, but written with the pipe operator and piped through head(): gapminder %&gt;% select(year, lifeExp) %&gt;% head(4) #&gt; # A tibble: 4 x 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 #&gt; 2 1957 30.3 #&gt; 3 1962 32.0 #&gt; 4 1967 34.0 Think: Take gapminder, then select the variables year and lifeExp, then show the first 4 rows. 22.5 Revel in the convenience Heres the data for Cambodia, but only certain variables: gapminder %&gt;% filter(country == &quot;Cambodia&quot;) %&gt;% select(year, lifeExp) #&gt; # A tibble: 12 x 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 39.4 #&gt; 2 1957 41.4 #&gt; 3 1962 43.4 #&gt; 4 1967 45.4 #&gt; 5 1972 40.3 #&gt; 6 1977 31.2 #&gt; 7 1982 51.0 #&gt; 8 1987 53.9 #&gt; 9 1992 55.8 #&gt; 10 1997 56.5 #&gt; 11 2002 56.8 #&gt; 12 2007 59.7 and what a typical base R call would look like: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] #&gt; # A tibble: 12 x 2 #&gt; year lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 39.4 #&gt; 2 1957 41.4 #&gt; 3 1962 43.4 #&gt; 4 1967 45.4 #&gt; 5 1972 40.3 #&gt; 6 1977 31.2 #&gt; 7 1982 51.0 #&gt; 8 1987 53.9 #&gt; 9 1992 55.8 #&gt; 10 1997 56.5 #&gt; 11 2002 56.8 #&gt; 12 2007 59.7 22.6 Pure, predictable, pipeable Weve barely scratched the surface of dplyr but I want to point out key principles you may start to appreciate. If youre new to R or programming with data, feel free skip this section and move on. dplyrs verbs, such as filter() and select(), are whats called pure functions. To quote from Wickhams Advanced R Programming book (2015): The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they dont affect the state of the world in any way apart from the value they return. In fact, these verbs are a special case of pure functions: they take the same flavor of object as input and output. Namely, a data frame or one of the other data receptacles dplyr supports. And finally, the data is always the very first argument of the verb functions. This set of deliberate design choices, together with the new pipe operator, produces a highly effective, low friction domain-specific language for data analysis. Go to the next section, for more dplyr! "],["handson.html", "23 Hands on Data Wrangling 23.1 Working with a single data frame 23.2 Activity 04: Hotels! 23.3 ODD: Single table dplyr functions", " 23 Hands on Data Wrangling 23.1 Working with a single data frame You can follow along with the slides here) if they do not appear below. 23.2 Activity 04: Hotels! You can find the materials for the Hotels activity here. The compiled version should look something like the following 23.3 ODD: Single table dplyr functions This optional deep dive covers more detail on dplyr. Previously, on Introduction to dplyr, we used two very important verbs and an operator: filter() for subsetting data with row logic select() for subsetting data variable- or column-wise the pipe operator %&gt;%, which feeds the LHS as the first argument to the expression on the RHS We also discussed dplyrs role inside the tidyverse and tibbles: dplyr is a core package in the tidyverse meta-package. Because we often make incidental usage of the others, we will load dplyr and the others via library(tidyverse). The tidyverse embraces a special flavor of data frame, called a tibble. The gapminder dataset is stored as a tibble. This time, were going to dive a bit deeper into dplyr. 23.3.1 Load dplyr and gapminder I choose to load the tidyverse, which will load dplyr, among other packages we use incidentally below. library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v ggplot2 3.3.3 v purrr 0.3.4 #&gt; v tibble 3.1.0 v dplyr 1.0.5 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() Also load gapminder. library(gapminder) 23.3.2 Create a copy of gapminder Were going to make changes to the gapminder tibble. To eliminate any fear that youre damaging the data that comes with the package, we create an explicit copy of gapminder for our experiments. (my_gap &lt;- gapminder) #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 1,694 more rows Pay close attention to when we evaluate statements but let the output just print to screen: ## let output print to screen, but do not store my_gap %&gt;% filter(country == &quot;Canada&quot;)  versus when we assign the output to an object, possibly overwriting an existing object. ## store the output as an R object my_precious &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) 23.3.3 Use mutate() to add new variables Imagine we wanted to recover each countrys GDP. After all, the Gapminder data has a variable for population and GDP per capita. Lets multiply them together. mutate() is a function that defines and inserts new variables into a tibble. You can refer to existing variables by name. my_gap %&gt;% mutate(gdp = pop * gdpPercap) #&gt; # A tibble: 1,704 x 7 #&gt; country continent year lifeExp pop gdpPercap gdp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. #&gt; # ... with 1,694 more rows Hmmmm  those GDP numbers are almost uselessly large and abstract. Consider the advice of Randall Munroe of xkcd: One thing that bothers me is large numbers presented without context If I added a zero to this number, would the sentence containing it mean something different to me? If the answer is no, maybe the number has no business being in the sentence in the first place.\" Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. But what if I reported GDP per capita, relative to some benchmark country. Since Canada is my adopted home, Ill go with that. I need to create a new variable that is gdpPercap divided by Canadian gdpPercap, taking care that I always divide two numbers that pertain to the same year. How I achieve this: Filter down to the rows for Canada. Create a new temporary variable in my_gap: Extract the gdpPercap variable from the Canadian data. Replicate it once per country in the dataset, so it has the right length. Divide raw gdpPercap by this Canadian figure. Discard the temporary variable of replicated Canadian gdpPercap. ctib &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) ## this is a semi-dangerous way to add this variable ## I&#39;d prefer to join on year, but we haven&#39;t covered joins yet my_gap &lt;- my_gap %&gt;% mutate(tmp = rep(ctib$gdpPercap, nlevels(country)), gdpPercapRel = gdpPercap / tmp, tmp = NULL) Note that, mutate() builds new variables sequentially so you can reference earlier ones (like tmp) when defining later ones (like gdpPercapRel). Also, you can get rid of a variable by setting it to NULL. How could we sanity check that this worked? The Canadian values for gdpPercapRel better all be 1! my_gap %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(country, year, gdpPercapRel) #&gt; # A tibble: 12 x 3 #&gt; country year gdpPercapRel #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Canada 1952 1 #&gt; 2 Canada 1957 1 #&gt; 3 Canada 1962 1 #&gt; 4 Canada 1967 1 #&gt; 5 Canada 1972 1 #&gt; 6 Canada 1977 1 #&gt; 7 Canada 1982 1 #&gt; 8 Canada 1987 1 #&gt; 9 Canada 1992 1 #&gt; 10 Canada 1997 1 #&gt; 11 Canada 2002 1 #&gt; 12 Canada 2007 1 I perceive Canada to be a high GDP country, so I predict that the distribution of gdpPercapRel is located below 1, possibly even well below. Check your intuition! summary(my_gap$gdpPercapRel) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.01 0.06 0.17 0.33 0.45 9.53 The relative GDP per capita numbers are, in general, well below 1. We see that most of the countries covered by this dataset have substantially lower GDP per capita, relative to Canada, across the entire time period. Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that youve done what meant to. Prepare to be horrified. 23.3.4 Use arrange() to row-order data in a principled way arrange() reorders the rows in a data frame. Imagine you wanted this data ordered by year then country, as opposed to by country then year. my_gap %&gt;% arrange(year, country) #&gt; # A tibble: 1,704 x 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 #&gt; 2 Albania Europe 1952 55.2 1282697 1601. 0.141 #&gt; 3 Algeria Africa 1952 43.1 9279525 2449. 0.215 #&gt; 4 Angola Africa 1952 30.0 4232095 3521. 0.310 #&gt; 5 Argentina Americas 1952 62.5 17876956 5911. 0.520 #&gt; 6 Australia Oceania 1952 69.1 8691212 10040. 0.883 #&gt; 7 Austria Europe 1952 66.8 6927772 6137. 0.540 #&gt; 8 Bahrain Asia 1952 50.9 120447 9867. 0.868 #&gt; 9 Bangladesh Asia 1952 37.5 46886859 684. 0.0602 #&gt; 10 Belgium Europe 1952 68 8730405 8343. 0.734 #&gt; # ... with 1,694 more rows Or maybe you want just the data from 2007, sorted on life expectancy? my_gap %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) #&gt; # A tibble: 142 x 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Swaziland Africa 2007 39.6 1.13e6 4513. 0.124 #&gt; 2 Mozambique Africa 2007 42.1 2.00e7 824. 0.0227 #&gt; 3 Zambia Africa 2007 42.4 1.17e7 1271. 0.0350 #&gt; 4 Sierra Leone Africa 2007 42.6 6.14e6 863. 0.0237 #&gt; 5 Lesotho Africa 2007 42.6 2.01e6 1569. 0.0432 #&gt; 6 Angola Africa 2007 42.7 1.24e7 4797. 0.132 #&gt; 7 Zimbabwe Africa 2007 43.5 1.23e7 470. 0.0129 #&gt; 8 Afghanistan Asia 2007 43.8 3.19e7 975. 0.0268 #&gt; 9 Central African Repub~ Africa 2007 44.7 4.37e6 706. 0.0194 #&gt; 10 Liberia Africa 2007 45.7 3.19e6 415. 0.0114 #&gt; # ... with 132 more rows Oh, youd like to sort on life expectancy in descending order? Then use desc(). my_gap %&gt;% filter(year == 2007) %&gt;% arrange(desc(lifeExp)) #&gt; # A tibble: 142 x 7 #&gt; country continent year lifeExp pop gdpPercap gdpPercapRel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Japan Asia 2007 82.6 127467972 31656. 0.872 #&gt; 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 1.09 #&gt; 3 Iceland Europe 2007 81.8 301931 36181. 0.996 #&gt; 4 Switzerland Europe 2007 81.7 7554661 37506. 1.03 #&gt; 5 Australia Oceania 2007 81.2 20434176 34435. 0.948 #&gt; 6 Spain Europe 2007 80.9 40448191 28821. 0.794 #&gt; 7 Sweden Europe 2007 80.9 9031088 33860. 0.932 #&gt; 8 Israel Asia 2007 80.7 6426679 25523. 0.703 #&gt; 9 France Europe 2007 80.7 61083916 30470. 0.839 #&gt; 10 Canada Americas 2007 80.7 33390141 36319. 1 #&gt; # ... with 132 more rows I advise that your analyses NEVER rely on rows or variables being in a specific order. But its still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order. 23.3.5 Use rename() to rename variables When I first cleaned this Gapminder excerpt, I was a camelCase person, but now Im all about snake_case. So I am vexed by the variable names I chose when I cleaned this data years ago. Lets rename some variables! my_gap %&gt;% rename(life_exp = lifeExp, gdp_percap = gdpPercap, gdp_percap_rel = gdpPercapRel) #&gt; # A tibble: 1,704 x 7 #&gt; country continent year life_exp pop gdp_percap gdp_percap_rel #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. 0.0657 #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. 0.0634 #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. 0.0520 #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. 0.0390 #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. 0.0356 #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. 0.0427 #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. 0.0320 #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. 0.0246 #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. 0.0219 #&gt; # ... with 1,694 more rows I did NOT assign the post-rename object back to my_gap because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to my_gap, in a data preparation script, and proceed with the new variable names. 23.3.6 select() can rename and reposition variables Youve seen simple use of select(). There are two tricks you might enjoy: select() can rename the variables you request to keep. select() can be used with everything() to hoist a variable up to the front of the tibble. my_gap %&gt;% filter(country == &quot;Burundi&quot;, year &gt; 1996) %&gt;% select(yr = year, lifeExp, gdpPercap) %&gt;% select(gdpPercap, everything()) #&gt; # A tibble: 3 x 3 #&gt; gdpPercap yr lifeExp #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 463. 1997 45.3 #&gt; 2 446. 2002 47.4 #&gt; 3 430. 2007 49.6 everything() is one of several helpers for variable selection. Read its help to see the rest. 23.3.7 group_by() is a mighty weapon I have found friends and family collaborators love to ask seemingly innocuous questions like, which country experienced the sharpest 5-year drop in life expectancy? In fact, that is a totally natural question to ask. But if you are using a language that doesnt know about data, its an incredibly annoying question to answer. dplyr offers powerful tools to solve this class of problem: group_by() adds extra structure to your dataset  grouping information  which lays the groundwork for computations within the groups. summarize() takes a dataset with \\(n\\) observations, computes requested summaries, and returns a dataset with 1 observation. Window functions take a dataset with \\(n\\) observations and return a dataset with \\(n\\) observations. mutate() and summarize() will honor groups. You can also do very general computations on your groups with do(), though elsewhere in this course, I advocate for other approaches that I find more intuitive, using the purrr package. Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease. 23.3.7.1 Counting things up Lets start with simple counting. How many observations do we have per continent? my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n()) #&gt; # A tibble: 5 x 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 Let us pause here to think about the tidyverse. You could get these same frequencies using table() from base R. table(gapminder$continent) #&gt; #&gt; Africa Americas Asia Europe Oceania #&gt; 624 300 396 360 24 str(table(gapminder$continent)) #&gt; &#39;table&#39; int [1:5(1d)] 624 300 396 360 24 #&gt; - attr(*, &quot;dimnames&quot;)=List of 1 #&gt; ..$ : chr [1:5] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; ... But the object of class table that is returned makes downstream computation a bit fiddlier than youd like. For example, its too bad the continent levels come back only as names and not as a proper factor, with the original set of levels. This is an example of how the tidyverse smooths transitions where you want the output of step i to become the input of step i + 1. The tally() function is a convenience function that knows to count rows. It honors groups. my_gap %&gt;% group_by(continent) %&gt;% tally() #&gt; # A tibble: 5 x 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 The count() function is an even more convenient function that does both grouping and counting. my_gap %&gt;% count(continent) #&gt; # A tibble: 5 x 2 #&gt; continent n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Africa 624 #&gt; 2 Americas 300 #&gt; 3 Asia 396 #&gt; 4 Europe 360 #&gt; 5 Oceania 24 What if we wanted to add the number of unique countries for each continent? You can compute multiple summaries inside summarize(). Use the n_distinct() function to count the number of distinct countries within each continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n(), n_countries = n_distinct(country)) #&gt; # A tibble: 5 x 3 #&gt; continent n n_countries #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Africa 624 52 #&gt; 2 Americas 300 25 #&gt; 3 Asia 396 33 #&gt; 4 Europe 360 30 #&gt; 5 Oceania 24 2 23.3.7.2 General summarization The functions youll apply within summarize() include classical statistical summaries, like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). Remember they are functions that take \\(n\\) inputs and distill them down into 1 output. Although this may be statistically ill-advised, lets compute the average life expectancy by continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(avg_lifeExp = mean(lifeExp)) #&gt; # A tibble: 5 x 2 #&gt; continent avg_lifeExp #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Africa 48.9 #&gt; 2 Americas 64.7 #&gt; 3 Asia 60.1 #&gt; 4 Europe 71.9 #&gt; 5 Oceania 74.3 summarize_at() applies the same summary function(s) to multiple variables. Lets compute average and median life expectancy and GDP per capita by continent by yearbut only for 1952 and 2007. my_gap %&gt;% filter(year %in% c(1952, 2007)) %&gt;% group_by(continent, year) %&gt;% summarize_at(vars(lifeExp, gdpPercap), list(~mean(.), ~median(.))) #&gt; # A tibble: 10 x 6 #&gt; # Groups: continent [5] #&gt; continent year lifeExp_mean gdpPercap_mean lifeExp_median gdpPercap_median #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa 1952 39.1 1253. 38.8 987. #&gt; 2 Africa 2007 54.8 3089. 52.9 1452. #&gt; 3 Americas 1952 53.3 4079. 54.7 3048. #&gt; 4 Americas 2007 73.6 11003. 72.9 8948. #&gt; 5 Asia 1952 46.3 5195. 44.9 1207. #&gt; 6 Asia 2007 70.7 12473. 72.4 4471. #&gt; 7 Europe 1952 64.4 5661. 65.9 5142. #&gt; 8 Europe 2007 77.6 25054. 78.6 28054. #&gt; 9 Oceania 1952 69.3 10298. 69.3 10298. #&gt; 10 Oceania 2007 80.7 29810. 80.7 29810. Lets focus just on Asia. What are the minimum and maximum life expectancies seen by year? my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp)) #&gt; # A tibble: 12 x 3 #&gt; year min_lifeExp max_lifeExp #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 65.4 #&gt; 2 1957 30.3 67.8 #&gt; 3 1962 32.0 69.4 #&gt; 4 1967 34.0 71.4 #&gt; 5 1972 36.1 73.4 #&gt; 6 1977 31.2 75.4 #&gt; 7 1982 39.9 77.1 #&gt; 8 1987 40.8 78.7 #&gt; 9 1992 41.7 79.4 #&gt; 10 1997 41.8 80.7 #&gt; 11 2002 42.1 82 #&gt; 12 2007 43.8 82.6 Of course it would be much more interesting to see which country contributed these extreme observations. Is the minimum (maximum) always coming from the same country? We tackle that with window functions shortly. 23.3.8 Grouped mutate Sometimes you dont want to collapse the \\(n\\) rows for each group into one row. You want to keep your groups, but compute within them. 23.3.8.1 Computing with group-wise summaries Lets make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use mutate() to make a new variable. The first() function extracts the first value from a vector. Notice that first() is operating on the vector of life expectancies within each country group. my_gap %&gt;% group_by(country) %&gt;% select(country, year, lifeExp) %&gt;% mutate(lifeExp_gain = lifeExp - first(lifeExp)) %&gt;% filter(year &lt; 1963) #&gt; # A tibble: 426 x 4 #&gt; # Groups: country [142] #&gt; country year lifeExp lifeExp_gain #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 1952 28.8 0 #&gt; 2 Afghanistan 1957 30.3 1.53 #&gt; 3 Afghanistan 1962 32.0 3.20 #&gt; 4 Albania 1952 55.2 0 #&gt; 5 Albania 1957 59.3 4.05 #&gt; 6 Albania 1962 64.8 9.59 #&gt; 7 Algeria 1952 43.1 0 #&gt; 8 Algeria 1957 45.7 2.61 #&gt; 9 Algeria 1962 48.3 5.23 #&gt; 10 Angola 1952 30.0 0 #&gt; # ... with 416 more rows Within country, we take the difference between life expectancy in year \\(i\\) and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers. 23.3.8.2 Window functions Window functions take \\(n\\) inputs and give back \\(n\\) outputs. Furthermore, the output depends on all the values. So rank() is a window function but log() is not. Here we use window functions based on ranks and offsets. Lets revisit the worst and best life expectancies in Asia over time, but retaining info about which country contributes these extreme values. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) %&gt;% filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) %&gt;% arrange(year) %&gt;% print(n = Inf) #&gt; # A tibble: 24 x 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1952 Israel 65.4 #&gt; 3 1957 Afghanistan 30.3 #&gt; 4 1957 Israel 67.8 #&gt; 5 1962 Afghanistan 32.0 #&gt; 6 1962 Israel 69.4 #&gt; 7 1967 Afghanistan 34.0 #&gt; 8 1967 Japan 71.4 #&gt; 9 1972 Afghanistan 36.1 #&gt; 10 1972 Japan 73.4 #&gt; 11 1977 Cambodia 31.2 #&gt; 12 1977 Japan 75.4 #&gt; 13 1982 Afghanistan 39.9 #&gt; 14 1982 Japan 77.1 #&gt; 15 1987 Afghanistan 40.8 #&gt; 16 1987 Japan 78.7 #&gt; 17 1992 Afghanistan 41.7 #&gt; 18 1992 Japan 79.4 #&gt; 19 1997 Afghanistan 41.8 #&gt; 20 1997 Japan 80.7 #&gt; 21 2002 Afghanistan 42.1 #&gt; 22 2002 Japan 82 #&gt; 23 2007 Afghanistan 43.8 #&gt; 24 2007 Japan 82.6 We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldnt it be nice to have one row per year? How did that actually work? First, I store and view a partial that leaves off the filter() statement. All of these operations should be familiar. asia &lt;- my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) asia #&gt; # A tibble: 396 x 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1957 Afghanistan 30.3 #&gt; 3 1962 Afghanistan 32.0 #&gt; 4 1967 Afghanistan 34.0 #&gt; 5 1972 Afghanistan 36.1 #&gt; 6 1977 Afghanistan 38.4 #&gt; 7 1982 Afghanistan 39.9 #&gt; 8 1987 Afghanistan 40.8 #&gt; 9 1992 Afghanistan 41.7 #&gt; 10 1997 Afghanistan 41.8 #&gt; # ... with 386 more rows Now we apply a window function  min_rank(). Since asia is grouped by year, min_rank() operates within mini-datasets, each for a specific year. Applied to the variable lifeExp, min_rank() returns the rank of each countrys observed life expectancy. FYI, the min part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order. For concreteness, I use mutate() to actually create these variables, even though I dropped this in the solution above. Lets look at a bit of that. asia %&gt;% mutate(le_rank = min_rank(lifeExp), le_desc_rank = min_rank(desc(lifeExp))) %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Japan&quot;, &quot;Thailand&quot;), year &gt; 1995) #&gt; # A tibble: 9 x 5 #&gt; # Groups: year [3] #&gt; year country lifeExp le_rank le_desc_rank #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1997 Afghanistan 41.8 1 33 #&gt; 2 2002 Afghanistan 42.1 1 33 #&gt; 3 2007 Afghanistan 43.8 1 33 #&gt; 4 1997 Japan 80.7 33 1 #&gt; 5 2002 Japan 82 33 1 #&gt; 6 2007 Japan 82.6 33 1 #&gt; 7 1997 Thailand 67.5 12 22 #&gt; 8 2002 Thailand 68.6 12 22 #&gt; 9 2007 Thailand 70.6 12 22 Afghanistan tends to present 1s in the le_rank variable, Japan tends to present 1s in the le_desc_rank variable and other countries, like Thailand, present less extreme ranks. You can understand the original filter() statement now: filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) These two sets of ranks are formed on-the-fly, within year group, and filter() retains rows with rank less than 2, which means  the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max. If we had wanted just the min OR the max, an alternative approach using top_n() would have worked. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% arrange(year) %&gt;% group_by(year) %&gt;% #top_n(1, wt = lifeExp) ## gets the min top_n(1, wt = desc(lifeExp)) ## gets the max #&gt; # A tibble: 12 x 3 #&gt; # Groups: year [12] #&gt; year country lifeExp #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1952 Afghanistan 28.8 #&gt; 2 1957 Afghanistan 30.3 #&gt; 3 1962 Afghanistan 32.0 #&gt; 4 1967 Afghanistan 34.0 #&gt; 5 1972 Afghanistan 36.1 #&gt; 6 1977 Cambodia 31.2 #&gt; 7 1982 Afghanistan 39.9 #&gt; 8 1987 Afghanistan 40.8 #&gt; 9 1992 Afghanistan 41.7 #&gt; 10 1997 Afghanistan 41.8 #&gt; 11 2002 Afghanistan 42.1 #&gt; 12 2007 Afghanistan 43.8 23.3.9 Grand Finale So lets answer that simple question: which country experienced the sharpest 5-year drop in life expectancy? Recall that this excerpt of the Gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints. At this point, thats just too easy, so lets do it by continent while were at it. my_gap %&gt;% select(country, year, continent, lifeExp) %&gt;% group_by(continent, country) %&gt;% ## within country, take (lifeExp in year i) - (lifeExp in year i - 1) ## positive means lifeExp went up, negative means it went down mutate(le_delta = lifeExp - lag(lifeExp)) %&gt;% ## within country, retain the worst lifeExp change = smallest or most negative summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %&gt;% ## within continent, retain the row with the lowest worst_le_delta top_n(-1, wt = worst_le_delta) %&gt;% arrange(worst_le_delta) #&gt; `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. #&gt; # A tibble: 5 x 3 #&gt; # Groups: continent [5] #&gt; continent country worst_le_delta #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Africa Rwanda -20.4 #&gt; 2 Asia Cambodia -9.10 #&gt; 3 Americas El Salvador -1.51 #&gt; 4 Europe Montenegro -1.46 #&gt; 5 Oceania Australia 0.170 Ponder that for a while. The subject matter and the code. Mostly youre seeing what genocide looks like in dry statistics on average life expectancy. Break the code into pieces, starting at the top, and inspect the intermediate results. Thats certainly how I was able to write such a thing. These commands do not leap fully formed out of anyones forehead  they are built up gradually, with lots of errors and refinements along the way. Im not even sure its a great idea to do so much manipulation in one fell swoop. Is the statement above really hard for you to read? If yes, then by all means break it into pieces and make some intermediate objects. Your code should be easy to write and read when youre done. In later tutorials, well explore more of dplyr, such as operations based on two datasets. 23.3.10 Resources dplyr official stuff: Package home on CRAN. Note there are several vignettes, with the introduction being the most relevant right now. The one on window functions will also be interesting to you now. Development home on GitHub. RStudio Data Transformation Cheat Sheet, covering dplyr. Remember you can get to these via Help &gt; Cheatsheets. Data transformation chapter of R for Data Science (Hadley Wickham and Grolemund 2016). Excellent slides on pipelines and dplyr by TJ Mahr, talk given to the Madison R Users Group. Blog post Hands-on dplyr tutorial for faster data manipulation in R by Data School, that includes a link to an R Markdown document and links to videos. "],["working-with-multiple-data-frames.html", "24 Working with multiple data frames 24.1 Case Studies in Joining", " 24 Working with multiple data frames You can follow along with the slides here if they do not appear below. 24.1 Case Studies in Joining You can follow along with the slides here if they do not appear below. "],["merges.html", "25 ODD: Merges and Collaboration 25.1 Learning goal 25.2 Merges and merge conflicts 25.3 Merge conflict activity", " 25 ODD: Merges and Collaboration This optional deep dive is about merge conflicts. You are welcome to try it with your classmates. 25.1 Learning goal Collaborating on GitHub and resolving merge conflicts 25.2 Merges and merge conflicts Were going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes youll run into merge conflicts. So our first task today is to walk you through a merge conflict! Pushing to a repo replaces the code on GitHub with the code you have on your computer. If a collaborator has made a change to your repo on GitHub that you havent incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborators work! So you need to explicitly merge your collaborators work before you can push. If your and your collaborators changes are in different files or in different parts of the same file, git merges the work for you automatically when you *pull*. If you both changed the same part of a file, git will produce a **merge conflict** because it doesnt know how which change you want to keep and which change you want to overwrite. Git will put conflict markers in your code that look like: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD See also: [dplyr documentation](https://dplyr.tidyverse.org/) ======= See also [ggplot2 documentation](https://ggplot2.tidyverse.org/) &gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4 The ===s separate your changes (top) from their changes (bottom). Note that on top you see the word HEAD, which indicates that these are your changes. And at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f). This is the hash (a unique identifier) of the commit your collaborator made with the conflicting change. Your job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then you can stage and commit the result. 25.3 Merge conflict activity 25.3.1 Setup Clone the repo and open the .Rmd file. Assign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers. 25.3.2 Lets cause a merge conflict! Our goal is to see two different types of merges: first well see a type of merge that git cant figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention. Doing this will require some tight choreography, so pay attention! Take turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you wont be able to resist the urge to touch your computer when its not your turn, we recommend putting your hands in your pockets or sitting on them! Before starting: everyone should have the repo cloned and know which role number(s) they are. Role 1: Change the team name to your actual team name. Knit, commit, push.  Make sure the previous role has finished before moving on to the next step. Role 2: Change the team name to some other word. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by editing the document to choose the correct/preferred change. Knit. Click the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes. Commit and push.  Make sure the previous role has finished before moving on to the next step. Role 3: Change the a label of the first code chunk Knit, commit, push. You should get an error. Pull. No merge conflicts should occur, but you should see a message about merging. Now push.  Make sure the previous role has finished before moving on to the next step. Role 4: Change the label of the first code chunk to something other than previous role did. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.  Make sure the previous role has finished before moving on to the next step. Everyone: Pull, and observe the changes in your document. 25.3.3 Tips for collaborating via GitHub Always pull first before you start working. Resolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict. Knit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve. If you find yourself in a situation that is difficult to resolve, ask questions as soon as possible. Dont let it linger and get bigger. "],["lab03.html", "26 Lab: Nobel laureates 26.1 Learning goals 26.2 Lab prep 26.3 Getting started 26.4 Exercises 26.5 But of those US-based Nobel laureates, many were born in other countries 26.6 Interested in how Buzzfeed made their visualizations?", " 26 Lab: Nobel laureates In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article, they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This reality is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions. 26.1 Learning goals Replicating published results Data wrangling and visualization 26.2 Lab prep Task: Read the Buzzfeed article titled These Nobel Prize Winners Show Why Immigration Is So Important For American Science. We will be replicating this analysis in the lab. So its crucial that youre familiar with it ahead of time. 26.3 Getting started Go to the course GitHub organization and locate the lab repo, which should be named something like lab-03-nobel-laureates. Either Fork it or copy it as a template. Then clone it in RStudio. First, open the R Markdown document lab-03.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 26.3.1 Warm up Before we introduce the data, lets warm up with some simple exercises. Update the YAML, changing the author name to your name, and knit the document. Commit your changes with a meaningful commit message. Push your changes to GitHub. Go to your repo on GitHub and confirm that your changes are visible in your Rmd and md files. If anything is missing, commit and push again. 26.3.2 Packages Well use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console: library(tidyverse) 26.3.3 Data The dataset for this assignment can be found as a csv (comma separated values) file in the data folder of your repository. You can read it in using the following. nobel &lt;- read_csv(&quot;data/nobel.csv&quot;) The variable descriptions are as follows: id: ID number firstname: First name of laureate surname: Surname year: Year prize won category: Category of prize affiliation: Affiliation of laureate city: City of laureate in prize year country: Country of laureate in prize year born_date: Birth date of laureate died_date: Death date of laureate gender: Gender of laureate born_city: City where laureate was born born_country: Country where laureate was born born_country_code: Code of country where laureate was born died_city: City where laureate died died_country: Country where laureate died died_country_code: Code of country where laureate died overall_motivation: Overall motivation for recognition share: Number of other winners award is shared with motivation: Motivation for recognition In a few cases, the name of the city/country changed after laureate was given (e.g., in 1975, Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`. born_country_original: Original country where laureate was born born_city_original: Original city where laureate was born died_country_original: Original country where laureate died died_city_original: Original city where laureate died city_original: Original city where laureate lived at the time of winning the award country_original: Original country where laureate lived at the time of winning the award 26.4 Exercises 26.4.1 Get to know your data How many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent? There are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results. Create a new data frame called nobel_living that filters for laureates for whom country is available laureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender) laureates who are still alive (their died_date is NA) Confirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 26.4.2 Most living Nobel laureates were based in the US when they won their prizes  says the Buzzfeed article. Lets see if thats true. First, well create a new variable to identify whether the laureate was in the US when they won their prize. Well use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function were using to write this if statement is the condition were testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\". Note: We can achieve the same result using the fct_other() function weve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R. nobel_living &lt;- nobel_living %&gt;% mutate( country_us = if_else(country == &quot;USA&quot;, &quot;USA&quot;, &quot;Other&quot;) ) Next, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics. Note: Technically, the Nobel Prize in Economics is a memorial prize. It was established in 1968. If you want to annoy an economist, point that distinction out to them More info here if you want to learn some more. nobel_living_science &lt;- nobel_living %&gt;% filter(category %in% c(&quot;Physics&quot;, &quot;Medicine&quot;, &quot;Chemistry&quot;, &quot;Economics&quot;)) For the next exercise, work with the nobel_living_science data frame you created above. Youll need to define this data frame in your R Markdown document, even though the next exercise doesnt explicitly ask you to do so. Create a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data. Your visualization should be faceted by category. For each facet you should have two bars, one for winners in the US and one for Other. Flip the coordinates so the bars are horizontal, not vertical.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 26.5 But of those US-based Nobel laureates, many were born in other countries Hint: You should be able to cheat borrow from code you used earlier to create the country_us variable. Create a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US? Add a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeeds claim? Explain your reasoning in 1-2 sentences. Your final visualization should contain a facet for each category. Within each facet, there should be a bar for whether the laureate won the award in the US or not. Each bar should have segments for whether the laureate was born in the US or not.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 26.5.1 Heres where those immigrant Nobelists were born Note: Your bar plot wont exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published. In a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure youre happy with the final state of your work. Now go back through your write up to make sure youve answered all questions and all of your R chunks are properly labeled. 26.6 Interested in how Buzzfeed made their visualizations? The plots in the Buzzfeed article are called waffle plots. You can find the code used for making these plots in Buzzfeeds GitHub repo (yes, they have one!) here. Youre not expected to recreate them as part of your assignment, but youre welcomed to do so for fun! "],["welcome-to-data-diving-with-types.html", "27 Welcome to Data Diving with Types 27.1 Module Materials 27.2 Estimated Video Length", " 27 Welcome to Data Diving with Types This module is designed to dive into data types and data importing. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 27.1 Module Materials Slides from Lectures Data types and recoding Importing data Suggested Readings All subchapters of this module R4DS Wrangle Data Transformation Data Importing Activities Hotels, again! Data Import Lab Visualizing spatial data 27.2 Estimated Video Length No of videos : 9 Average length of video : 8 minutes, 28 seconds Total length of playlist : 1 hour, 16 minutes, 15 seconds "],["data-types-and-recoding.html", "28 Data types and recoding 28.1 Why should you care about data types? 28.2 Data types! 28.3 Special Values 28.4 Data classes 28.5 Working with factors 28.6 Working with Dates", " 28 Data types and recoding You can follow along with the slides here if they do not appear below. 28.1 Why should you care about data types? 28.2 Data types! 28.2.1 Another Hotels Activity You can find the materials for the Hotels activity here. The compiled version should look something like the following 28.3 Special Values 28.4 Data classes 28.5 Working with factors 28.5.1 (An) Another Hotels Activity You can find the materials for the Hotels activity here. The compiled version should look something like the following 28.6 Working with Dates "],["importing-data.html", "29 Importing data! 29.1 Importing data! 29.2 Importing and Variable Types! 29.3 Vroom!", " 29 Importing data! 29.1 Importing data! You can follow along with the slides here if they do not appear below. 29.1.1 Another Activity You can find the materials for the Nobels and sales activities here. 29.2 Importing and Variable Types! You can follow along with the slides here if they do not appear below. 29.2.1 More Activity You can find the materials for the Nobels and sales activities here. 29.3 Vroom! You can follow along with the slides here if they do not appear below. "],["import-export.html", "30 Writing and reading files 30.1 File I/O overview 30.2 Load the tidyverse 30.3 Locate the Gapminder data 30.4 Bring rectangular data in 30.5 Compute something worthy of export 30.6 Write rectangular data out 30.7 Invertibility 30.8 Reordering the levels of the country factor 30.9 saveRDS() and readRDS() 30.10 Retaining factor levels upon re-import 30.11 dput() and dget() 30.12 Other types of objects to use dput() or saveRDS() on 30.13 Clean up 30.14 Pitfalls of delimited files 30.15 Resources", " 30 Writing and reading files This deep dive has been adapted from Jenny Bryans Stat545. 30.1 File I/O overview For the most part, weve been working with (p)reprocessed data, like the Gapminder data from the gapminder data package or data from any of the labs. In other words, we havent been explicitly writing any data or derived results to file. In real life (and in this class), youll have to bring rectangular data into and out of R. Sometimes youll need to do same for non-rectangular objects. How do you do this? What issues should you think about? 30.1.1 Data import mindset Data import generally feels one of two ways: Surprise me! You probably have to adopt this attitude when you first get a dataset. You are just happy to import without an error. You start to explore. You discover flaws in the data and/or the import. You address them. Lather, rinse, repeat. Another day in paradise. This attitude is when you bring in a tidy dataset you have maniacally cleaned in one or more cleaning scripts. There should be no surprises. You should express your expectations about the data in formal assertions at the very start of these downstream scripts. In the second case, and as the first cases progresses, you actually know a lot about how the data is / should be. My main import advice: use the arguments of your import function to get as far as you can, as fast as possible. Novice code often has a great deal of unnecessary post import fussing around. Read the docs for the import functions and take maximum advantage of the arguments to control the import. 30.1.2 Data export mindset There will be many occasions when you need to write data from R. Two main examples: a tidy ready-to-analyze dataset that you heroically created from messy data a numerical result from data aggregation or modelling or statistical inference First tip: todays outputs are tomorrows inputs. Think back on all the pain you have suffered importing data and dont inflict such pain on yourself! Second tip: dont be too cute or clever. A plain text file that is readable by a human being in a text editor should be your default until you have actual proof that this will not work. Reading and writing to exotic or proprietary formats will be the first thing to break in the future or on a different computer. It also creates barriers for anyone who has a different toolkit than you do. Be software-agnostic. Aim for future-proof and forgetfu-proof. How does this approach fit with our emphasis on dynamic reporting via R Markdown? There is a time and place for everything. There are projects and documents where the scope and personnel will allow you to geek out with knitr and R Markdown. But there are lots of good reasons why (parts of) an analysis should not (only) be embedded in a dynamic report. Maybe you are just doing data cleaning to produce a valid input dataset. Maybe you are making a small but crucial contribution to a giant multi-author paper. Etc. Also remember there are other tools and workflows for making something reproducible. Im looking at you, make. 30.2 Load the tidyverse The main package we will be using is readr, which provides drop-in substitute functions for read.table() and friends. However, to make some points about data export and import, it is nice to reorder factor levels. For that, we will use the forcats package, which is also included in the tidyverse package. library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v ggplot2 3.3.3 v purrr 0.3.4 #&gt; v tibble 3.1.0 v dplyr 1.0.5 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() 30.3 Locate the Gapminder data We could load the data from the package as usual, but instead we will load it from tab delimited file. The gapminder package includes the data normally found in the gapminder data frame as a .tsv. So lets get the path to that file on your system using the fs package. library(fs) (gap_tsv &lt;- path_package(&quot;gapminder&quot;, &quot;extdata&quot;, &quot;gapminder.tsv&quot;)) #&gt; C:/Users/smaso/Documents/R/win-library/4.0/gapminder/extdata/gapminder.tsv 30.4 Bring rectangular data in The workhorse data import function of readr is read_delim(). Here well use a variant, read_tsv(), that anticipates tab-delimited data: gapminder &lt;- read_tsv(gap_tsv) #&gt; #&gt; -- Column specification -------------------------------------------------------- #&gt; cols( #&gt; country = col_character(), #&gt; continent = col_character(), #&gt; year = col_double(), #&gt; lifeExp = col_double(), #&gt; pop = col_double(), #&gt; gdpPercap = col_double() #&gt; ) str(gapminder, give.attr = FALSE) #&gt; spec_tbl_df[,6] [1,704 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ country : chr [1:1704] &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghani&quot;.. #&gt; $ continent: chr [1:1704] &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; ... #&gt; $ year : num [1:1704] 1952 1957 1962 1967 1972 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : num [1:1704] 8425333 9240934 10267083 11537966 13079460 ... #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... For full flexibility re: specifying the delimiter, you can always use readr::read_delim(). Theres a similar convenience wrapper for comma-separated values, read_csv(). The most noticeable difference between the readr functions and base is that readr does NOT convert strings to factors by default. In the grand scheme of things, this default behavior is better, although we go ahead and convert them to factor here. Do not be deceived  in general, you will do less post-import fussing if you use readr. gapminder &lt;- gapminder %&gt;% mutate(country = factor(country), continent = factor(continent)) str(gapminder) #&gt; spec_tbl_df[,6] [1,704 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : num [1:1704] 1952 1957 1962 1967 1972 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : num [1:1704] 8425333 9240934 10267083 11537966 13079460 ... #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... #&gt; - attr(*, &quot;spec&quot;)= #&gt; .. cols( #&gt; .. country = col_character(), #&gt; .. continent = col_character(), #&gt; .. year = col_double(), #&gt; .. lifeExp = col_double(), #&gt; .. pop = col_double(), #&gt; .. gdpPercap = col_double() #&gt; .. ) 30.4.1 Bring rectangular data in  summary Default to readr::read_delim() and friends. Use the arguments! The Gapminder data is too clean and simple to show off the great features of readr, so I encourage you to check out the part of the introduction vignette on column types. There are many variable types that you will be able to parse correctly upon import, thereby eliminating a great deal of post-import fussing. 30.5 Compute something worthy of export We need compute something worth writing to file. Lets create a country-level summary of maximum life expectancy. gap_life_exp &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% summarise(life_exp = max(lifeExp)) %&gt;% ungroup() #&gt; `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. gap_life_exp #&gt; # A tibble: 142 x 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 #&gt; 7 Austria Europe 79.8 #&gt; 8 Bahrain Asia 75.6 #&gt; 9 Bangladesh Asia 64.1 #&gt; 10 Belgium Europe 79.4 #&gt; # ... with 132 more rows The gap_life_exp data frame is an example of an intermediate result that we want to store for the future and for downstream analyses or visualizations. 30.6 Write rectangular data out The workhorse export function for rectangular data in readr is write_delim() and friends. Lets use write_csv() to get a comma-delimited file. write_csv(gap_life_exp, &quot;gap_life_exp.csv&quot;) Lets look at the first few lines of gap_life_exp.csv. If youre following along, you should be able to open this file or, in a shell, use head() on it. country,continent,life_exp Afghanistan,Asia,43.828 Albania,Europe,76.423 Algeria,Africa,72.301 Angola,Africa,42.731 Argentina,Americas,75.32 This output is pretty decent looking, though there is no visible alignment or separation into columns. Had we used the base function read.csv(), we would be seeing rownames and lots of quotes, unless we had explicitly shut that down. Nicer default behavior is the main reason we are using readr::write_csv() over write.csv(). Its not really fair to complain about the lack of visible alignment. Remember we are writing data for computers. If you really want to browse around the file, use View() in RStudio or open it in Microsoft Excel (!) but dont succumb to the temptation to start doing artisanal data manipulations there  get back to R and construct commands that you can re-run the next 15 times you import/clean/aggregate/export the same dataset. Trust me, it will happen. 30.7 Invertibility It turns out these self-imposed rules are often in conflict with one another: Write to plain text files Break analysis into pieces: the output of script i is an input for script i + 1 Be the boss of factors: order the levels in a meaningful, usually non-alphabetical way Avoid duplication of code and data Example: after performing the country-level summarization, we reorder the levels of the country factor, based on life expectancy. This reordering operation is conceptually important and must be embodied in R commands stored in a script. However, as soon as we write gap_life_exp to a plain text file, that meta-information about the countries is lost. Upon re-import with read_delim() and friends, we are back to alphabetically ordered factor levels. Any measure we take to avoid this loss immediately breaks another one of our rules. So what do I do? I must admit I save (and re-load) R-specific binary files. Right after I save the plain text file. Belt and suspenders. I have toyed with the idea of writing import helper functions for a specific project, that would re-order factor levels in principled ways. They could be defined in one file and called from many. This would also have a very natural implementation within a workflow where each analytical project is an R package. But so far it has seemed too much like yak shaving. Im intrigued by a recent discussion of putting such information in YAML frontmatter (see Martin Fenner blog post Using YAML frontmatter with CSV). 30.8 Reordering the levels of the country factor I reorder the country factor levels according to the life expectancy summary weve already computed. head(levels(gap_life_exp$country)) # alphabetical order #&gt; [1] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Angola&quot; &quot;Argentina&quot; #&gt; [6] &quot;Australia&quot; gap_life_exp &lt;- gap_life_exp %&gt;% mutate(country = fct_reorder(country, life_exp)) head(levels(gap_life_exp$country)) # in increasing order of maximum life expectancy #&gt; [1] &quot;Sierra Leone&quot; &quot;Angola&quot; &quot;Afghanistan&quot; &quot;Liberia&quot; &quot;Rwanda&quot; #&gt; [6] &quot;Mozambique&quot; head(gap_life_exp) #&gt; # A tibble: 6 x 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 Note that the row order of gap_life_exp has not changed. I could choose to reorder the rows of the data frame if, for example, I was about to prepare a table to present to people. But Im not, so I wont. 30.9 saveRDS() and readRDS() If you have a data frame AND you have exerted yourself to rationalize the factor levels, you have my blessing to save it to file in a way that will preserve this hard work upon re-import. Use saveRDS(). saveRDS(gap_life_exp, &quot;gap_life_exp.rds&quot;) saveRDS() serializes an R object to a binary file. Its not a file you will able to open in an editor, diff nicely with Git(Hub), or share with non-R friends. Its a special purpose, limited use function that I use in specific situations. The opposite of saveRDS() is readRDS(). You must assign the return value to an object. I highly recommend you assign back to the same name as before. Why confuse yourself?!? rm(gap_life_exp) gap_life_exp #&gt; Error in eval(expr, envir, enclos): object &#39;gap_life_exp&#39; not found gap_life_exp &lt;- readRDS(&quot;gap_life_exp.rds&quot;) gap_life_exp #&gt; # A tibble: 142 x 3 #&gt; country continent life_exp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 43.8 #&gt; 2 Albania Europe 76.4 #&gt; 3 Algeria Africa 72.3 #&gt; 4 Angola Africa 42.7 #&gt; 5 Argentina Americas 75.3 #&gt; 6 Australia Oceania 81.2 #&gt; 7 Austria Europe 79.8 #&gt; 8 Bahrain Asia 75.6 #&gt; 9 Bangladesh Asia 64.1 #&gt; 10 Belgium Europe 79.4 #&gt; # ... with 132 more rows saveRDS() has more arguments, in particular compress for controlling compression, so read the help for more advanced usage. It is also very handy for saving non-rectangular objects, like a fitted regression model, that took a nontrivial amount of time to compute. You will eventually hear about save() + load() and even save.image(). You may even see them in documentation and tutorials, but dont be tempted. Just say no. These functions encourage unsafe practices, like storing multiple objects together and even entire workspaces. There are legitimate uses of these functions, but not in your typical data analysis. 30.10 Retaining factor levels upon re-import Here is a concrete demonstration of how non-alphabetical factor level order is lost with write_delim() / read_delim() workflows but maintained with saveRDS() / readRDS(). (country_levels &lt;- tibble(original = head(levels(gap_life_exp$country)))) #&gt; # A tibble: 6 x 1 #&gt; original #&gt; &lt;chr&gt; #&gt; 1 Sierra Leone #&gt; 2 Angola #&gt; 3 Afghanistan #&gt; 4 Liberia #&gt; 5 Rwanda #&gt; 6 Mozambique write_csv(gap_life_exp, &quot;gap_life_exp.csv&quot;) saveRDS(gap_life_exp, &quot;gap_life_exp.rds&quot;) rm(gap_life_exp) head(gap_life_exp) # will cause error! proving gap_life_exp is really gone #&gt; Error in head(gap_life_exp): object &#39;gap_life_exp&#39; not found gap_via_csv &lt;- read_csv(&quot;gap_life_exp.csv&quot;) %&gt;% mutate(country = factor(country)) #&gt; #&gt; -- Column specification -------------------------------------------------------- #&gt; cols( #&gt; country = col_character(), #&gt; continent = col_character(), #&gt; life_exp = col_double() #&gt; ) gap_via_rds &lt;- readRDS(&quot;gap_life_exp.rds&quot;) country_levels &lt;- country_levels %&gt;% mutate(via_csv = head(levels(gap_via_csv$country)), via_rds = head(levels(gap_via_rds$country))) country_levels #&gt; # A tibble: 6 x 3 #&gt; original via_csv via_rds #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Sierra Leone Afghanistan Sierra Leone #&gt; 2 Angola Albania Angola #&gt; 3 Afghanistan Algeria Afghanistan #&gt; 4 Liberia Angola Liberia #&gt; 5 Rwanda Argentina Rwanda #&gt; 6 Mozambique Australia Mozambique Note how the original, post-reordering country factor levels are restored using the saveRDS() / readRDS() strategy but revert to alphabetical ordering using write_csv() / read_csv(). 30.11 dput() and dget() One last method of saving and restoring data deserves a mention: dput() and dget(). dput() offers this odd combination of features: it creates a plain text representation of an R object which still manages to be quite opaque. If you use the file = argument, dput() can write this representation to file but you wont be tempted to actually read that thing. dput() creates an R-specific-but-not-binary representation. Lets try it out. ## first restore gap_life_exp with our desired country factor level order gap_life_exp &lt;- readRDS(&quot;gap_life_exp.rds&quot;) dput(gap_life_exp, &quot;gap_life_exp-dput.txt&quot;) Now lets look at the first few lines of the file gap_life_exp-dput.txt. structure(list(country = structure(c(3L, 107L, 74L, 2L, 98L, 138L, 128L, 102L, 49L, 125L, 26L, 56L, 96L, 47L, 75L, 85L, 18L, 12L, 37L, 24L, 133L, 13L, 16L, 117L, 84L, 82L, 53L, 9L, 28L, 120L, 22L, 104L, 114L, 109L, 115L, 23L, 73L, 97L, 66L, 71L, 15L, 29L, 20L, 122L, 134L, 40L, 35L, 123L, 38L, 126L, 60L, 25L, 7L, 39L, 59L, 141L, 86L, 140L, 51L, 63L, 64L, 52L, 121L, 135L, 132L, Huh? Dont worry about it. Remember we are writing data for computers. The partner function dget() reads this representation back in. gap_life_exp_dget &lt;- dget(&quot;gap_life_exp-dput.txt&quot;) country_levels &lt;- country_levels %&gt;% mutate(via_dput = head(levels(gap_life_exp_dget$country))) country_levels #&gt; # A tibble: 6 x 4 #&gt; original via_csv via_rds via_dput #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Sierra Leone Afghanistan Sierra Leone Sierra Leone #&gt; 2 Angola Albania Angola Angola #&gt; 3 Afghanistan Algeria Afghanistan Afghanistan #&gt; 4 Liberia Angola Liberia Liberia #&gt; 5 Rwanda Argentina Rwanda Rwanda #&gt; 6 Mozambique Australia Mozambique Mozambique Note how the original, post-reordering country factor levels are restored using the dput() / dget() strategy. But why on earth would you ever do this? The main application of this is the creation of highly portable, self-contained minimal examples. For example, if you want to pose a question on a forum or directly to an expert, it might be required or just plain courteous to NOT attach any data files. You will need a monolithic, plain text blob that defines any necessary objects and has the necessary code. dput() can be helpful for producing the piece of code that defines the object. If you dput() without specifying a file, you can copy the return value from Console and paste into a script. Or you can write to file and copy from there or add R commands below. 30.12 Other types of objects to use dput() or saveRDS() on My special dispensation to abandon human-readable, plain text files is even broader than Ive let on. Above, I give my blessing to store data.frames via dput() and/or saveRDS(), when youve done some rational factor level re-ordering. The same advice and mechanics apply a bit more broadly: youre also allowed to use R-specific file formats to save vital non-rectangular objects, such as a fitted nonlinear mixed effects model or a classification and regression tree. 30.13 Clean up Weve written several files in this tutorial. Some of them are not of lasting value or have confusing filenames. I choose to delete them, while demonstrating some of the many functions R offers for interacting with the filesystem. Its up to you whether you want to submit this command or not. file.remove(list.files(pattern = &quot;^gap_life_exp&quot;)) #&gt; [1] TRUE TRUE TRUE 30.14 Pitfalls of delimited files If a delimited file contains fields where a human being has typed, be crazy paranoid because people do really nutty things. Especially people who arent in the business of programming and have never had to compute on text. Claim: a persons regular expression skill is inversely proportional to the skill required to handle the files they create. Implication: if someone has never heard of regular expressions, prepare for lots of pain working with their files. When the header fields (often, but not always, the variable names) or actual data contain the delimiter, it can lead to parsing and import failures. Two popular delimiters are the comma , and the TAB \\t and humans tend to use these when typing. If you can design this problem away during data capture, such as by using a drop down menu on an input form, by all means do so. Sometimes this is impossible or undesirable and you must deal with fairly free form text. Thats a good time to allow/force text to be protected with quotes, because it will make parsing the delimited file go more smoothly. Sometimes, instead of rigid tab-delimiting, whitespace is used as the delimiter. That is, in fact, the default for both read.table() and write.table(). Assuming you will write/read variable names from the first line (a.k.a. the header in write.table() and read.table()), they must be valid R variable names  or they will be coerced into something valid. So, for these two reasons, it is good practice to use one word variable names whenever possible. If you need to evoke multiple words, use snake_case or camelCase to cope. Example: the header entry for the field holding the subjects last name should be last_name or lastName NOT last name. With the readr package, column names are left as is, not munged into valid R identifiers (i.e. there is no check.names = TRUE). So you can get away with whitespace in variable names and yet I recommend that you do not. 30.15 Resources Data import chapter of R for Data Science by Hadley Wickham and Garrett Grolemund (2016). White et al.s Nine simple ways to make it easier to (re)use your data (2013). First appeared in PeerJ Preprints Published in Ideas in Ecology and Evolution in 2013 Section 4 Use Standard Data Formats is especially good reading. Wickhams paper on tidy data in the Journal of Statistical Software (2014). Available as a PDF here Data Manipulation in R by Phil Spector (2008). Available via SpringerLink Authors webpage GoogleBooks search "],["odd-transformations-data.html", "31 ODD: Transformations data! 31.1 Transforming Data: Tukeys Ladder of Powers 31.2 Vectorizing a function 31.3 Box Cox Transformation", " 31 ODD: Transformations data! This optional deep dive covers data transformations, and Tukeys ladder of powers. (It isnt complete right now) 31.1 Transforming Data: Tukeys Ladder of Powers This material is based on (fox2016applied?) ch. 4, pp. 28 - 80. We will use the following data set(s) from (fox2016applied?). # Download these files manually. # Make sure to the directory to the file in which this file was saved # using RStudio menus: Session &gt; Set Working Directory &gt; To Source File Directory #fox_data &lt;- &quot;http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/&quot; #download.file(paste0(fox_data,&#39;UnitedNations.txt&#39;),&#39;UnitedNations.txt&#39;) # devtools::install_github(&#39;gmonette/spida2&#39;) library(car) #&gt; Loading required package: carData library(spida2) library(latticeExtra) #&gt; Loading required package: lattice # read data un &lt;- read.table(&#39;UnitedNations.txt&#39;, header = TRUE) head(un) #&gt; region tfr contraception educationMale educationFemale lifeMale #&gt; Afghanistan Asia 6.90 NA NA NA 45.0 #&gt; Albania Europe 2.60 NA NA NA 68.0 #&gt; Algeria Africa 3.81 52 11.1 9.9 67.5 #&gt; American.Samoa Asia NA NA NA NA 68.0 #&gt; Andorra Europe NA NA NA NA NA #&gt; Angola Africa 6.69 NA NA NA 44.9 #&gt; lifeFemale infantMortality GDPperCapita economicActivityMale #&gt; Afghanistan 46.0 154 2848 87.5 #&gt; Albania 74.0 32 863 NA #&gt; Algeria 70.3 44 1531 76.4 #&gt; American.Samoa 73.0 11 NA 58.8 #&gt; Andorra NA NA NA NA #&gt; Angola 48.1 124 355 NA #&gt; economicActivityFemale illiteracyMale illiteracyFemale #&gt; Afghanistan 7.2 52.800 85.00 #&gt; Albania NA NA NA #&gt; Algeria 7.8 26.100 51.00 #&gt; American.Samoa 42.4 0.264 0.36 #&gt; Andorra NA NA NA #&gt; Angola NA NA NA un$country &lt;- rownames(un) Finding transformations to make regressions behave gd() xyplot(infantMortality ~ GDPperCapita, un) xyplot(log(infantMortality) ~ log(GDPperCapita), un) John Tukey suggested this simple toolkit, like a set drill bits of varying sizes, to modify the shape of distributions and the shape of relationships between variables. The basic idea stems from the fact that functions of the form \\[y&#39; = y^p, \\quad y &gt; 0\\] have a graph that is concave up if \\(p &gt;1\\), and concave down if \\(0&lt;p&lt;1\\). For \\(p &lt; 0\\), \\[y&#39; = - y^p, \\quad y &gt; 0\\] the graph is also concave down. This leaves out \\(p=0\\) but we will see shortly that \\(y&#39; = \\ln y\\) is the sensible transformation that corresponds to \\(p = 0\\). Now, we standardize the family of power transformations so that they have the value 0 when \\(y = 1\\) and so their derivative is equal to 1 when \\(y=1\\). For \\(p \\ne 0\\), this yields \\[y&#39; = \\frac{y^p - 1}{p}\\] Note that, by lHôpitals rule: \\[\\lim_{p \\to 0} \\frac{y^p - 1}{p}= \\lim_{p \\to 0}\\, e^{\\, p \\ln y} \\ln y = \\ln y\\] We define a function that produces this transformation. The easy way to define it is: pow &lt;- function(y, p) { if(p == 0) log(y) else (y^p - 1)/p } # test: x &lt;- seq(-1,3,.5) x # note that these transformations are really intended for y &gt; 0 #&gt; [1] -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 pow(x, 2) #&gt; [1] 0.000 -0.375 -0.500 -0.375 0.000 0.625 1.500 2.625 4.000 pow(x, 0) %&gt;% name(x) #&gt; Warning in log(y): NaNs produced #&gt; -1 -0.5 0 0.5 1 1.5 2 2.5 3 #&gt; NaN NaN -Inf -0.693 0.000 0.405 0.693 0.916 1.099 pow(x, -1) %&gt;% name(x) %&gt;% cbind #&gt; . #&gt; -1 2.000 #&gt; -0.5 3.000 #&gt; 0 -Inf #&gt; 0.5 -1.000 #&gt; 1 0.000 #&gt; 1.5 0.333 #&gt; 2 0.500 #&gt; 2.5 0.600 #&gt; 3 0.667 plot(exp) # easy plotting of a function plot(function(x) pow(x, p=2)) # anonymous function or &#39;lambda&#39; plot(function(x) pow(x, p=.5), xlim=c(0,3)) but this has the disadvantage that it works correctly only for a single value of \\(p\\), since the statement if(p == 0) only tests the first element of p. 31.2 Vectorizing a function Most operators in R are vectorized so they work element-wise when their arguments are vectors. When the arguments have incompatible lengths, the shorter argument is recycled to have the same length as the longer one. That is why the following produces sensible results: z &lt;- c(3,5,9) z + c(1,1,1) #&gt; [1] 4 6 10 z + 1 # 1 is recycled so the result is equivalent to the previous line #&gt; [1] 4 6 10 z + c(1,2,3) #&gt; [1] 4 7 12 z + c(1,2) # recycles but gives a warning #&gt; Warning in z + c(1, 2): longer object length is not a multiple of shorter object #&gt; length #&gt; [1] 4 7 10 z + z #&gt; [1] 6 10 18 z^2 #&gt; [1] 9 25 81 z^z #&gt; [1] 2.70e+01 3.12e+03 3.87e+08 We can use ifelse which works on a vector instead of a single value. pow &lt;- function(y, p) { p &lt;- rep(p, length.out = length(y)) y &lt;- rep(y, length.out = length(p)) ifelse(p==0, log(y), (y^p - 1)/p) } # test: pow(-1:4, c(2,0,-1,1,3)) #&gt; Warning in log(y): NaNs produced #&gt; [1] 0.00 -Inf 0.00 1.00 8.67 7.50 pow(-1:4, 2) #&gt; [1] 0.0 -0.5 0.0 1.5 4.0 7.5 With a bit more work, we can avoid unnecessary evaluations: pow &lt;- function(y, p) { p &lt;- rep(p, length.out = length(y)) y &lt;- rep(y, length.out = length(p)) y[p==0] &lt;- log(y[p==0]) y[p!=0] &lt;- (y[p!=0]^p[p!=0] -1) / p[p!=0] y } # Test: pow(1:10,0) == log(1:10) #&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE pow(1:10,-1) #&gt; [1] 0.000 0.500 0.667 0.750 0.800 0.833 0.857 0.875 0.889 0.900 pow(1:10,.5) #&gt; [1] 0.000 0.828 1.464 2.000 2.472 2.899 3.292 3.657 4.000 4.325 pow(1:10,-1:8) #&gt; [1] 0.00e+00 6.93e-01 2.00e+00 7.50e+00 4.13e+01 3.24e+02 3.36e+03 4.37e+04 #&gt; [9] 6.83e+05 1.25e+07 Lets plot this transformation for a range of values of \\(p\\). The value of expand.grid is a data frame whose rows consist of the Cartesian product (i.e. all possible combinations) of its arguments. expand.grid(a = c(&quot;A&quot;,&quot;B&quot;), x = 1:3) #&gt; a x #&gt; 1 A 1 #&gt; 2 B 1 #&gt; 3 A 2 #&gt; 4 B 2 #&gt; 5 A 3 #&gt; 6 B 3 dd &lt;- expand.grid(y = seq(.01,3,.01), p = c(-2,-1,-.5,0,.5,1,2,3)) dim(dd) #&gt; [1] 2400 2 head(dd) #&gt; y p #&gt; 1 0.01 -2 #&gt; 2 0.02 -2 #&gt; 3 0.03 -2 #&gt; 4 0.04 -2 #&gt; 5 0.05 -2 #&gt; 6 0.06 -2 some(dd) # 10 rows at random #&gt; y p #&gt; 98 0.98 -2.0 #&gt; 157 1.57 -2.0 #&gt; 349 0.49 -1.0 #&gt; 1282 0.82 0.5 #&gt; 1380 1.80 0.5 #&gt; 1851 0.51 2.0 #&gt; 1877 0.77 2.0 #&gt; 1907 1.07 2.0 #&gt; 2329 2.29 3.0 #&gt; 2343 2.43 3.0 dd$yval &lt;- with(dd, pow(y,p)) xyplot(yval ~ y| factor(p), dd, type = &#39;l&#39;) xyplot(yval ~ y| factor(p), dd, type = &#39;l&#39;, ylim =c(-2,max(dd$yval))) xyplot(yval ~ y , dd, groups = p, type = &#39;l&#39;, ylim =c(-2,max(dd$yval))) xyplot(yval ~ y , dd, groups = p, type = &#39;l&#39;, xlim = c(0,3), ylim =c(-2,max(dd$yval))) gd(8, lwd = 2) # number of colours needed xyplot(yval ~ y , dd, groups = p, type = &#39;l&#39;, xlim = c(0,3), ylim =c(-2,max(dd$yval))) xyplot(yval ~ y , dd, groups = p, type = &#39;l&#39;, auto.key = list(space = &#39;right&#39;,lines = T, points = F), xlim = c(0,3), ylim =c(-2,max(dd$yval))) Its much better to have the legend in the same order as the lines in the graph. We can turn p into a factor and reverse its order. dd$po &lt;- factor(dd$p) dd$po &lt;- reorder(dd$po, -dd$p) xyplot(yval ~ y , dd, groups = po, type = &#39;l&#39;, auto.key = list(space = &#39;right&#39;,lines = T, points = F, title=&#39;power&#39;), xlim = c(0,3), ylim =c(-2,max(dd$yval))) From quantile plots: Uniform quantiles library(spida2) xqplot(un) Normal quantiles xqplot(un, ptype = &#39;normal&#39;) We see that none of the numeric variables have normal distributions. age is somewhat platykurtic compared with a normal compositeHourlyWages has both a categorical (0) and a continuous component education is also platykurtic working is dichotomous familyIncome is skewed to the right Note that the fact that \\(x\\) or \\(y\\) variables are not normal does not mean that the conditional distribution of \\(y\\) given \\(x\\) is not normal. Lets explore wages of working women as a function of education. library(latticeExtra) un %&gt;% xyplot(infantMortality ~ GDPperCapita, .) + layer(panel.loess(..., lwd = 2)) # Scatterplot showing curvature in relationship trellis.focus() panel.identify(labels=rownames(un)) #&gt; integer(0) trellis.unfocus() un %&gt;% xyplot(log(infantMortality) ~ GDPperCapita | region, .) + layer(panel.loess(..., lwd = 2)) un %&gt;% subset(country %in% c(&#39;United.States&#39;,&#39;Canada&#39;)) #&gt; region tfr contraception educationMale educationFemale lifeMale #&gt; Canada America 1.61 66 17.2 17.8 76.1 #&gt; United.States America 1.96 71 15.4 16.2 73.4 #&gt; lifeFemale infantMortality GDPperCapita economicActivityMale #&gt; Canada 81.8 6 18943 72.4 #&gt; United.States 80.1 7 26037 74.9 #&gt; economicActivityFemale illiteracyMale illiteracyFemale #&gt; Canada 57.6 NA NA #&gt; United.States 59.3 2.24 2.23 #&gt; country #&gt; Canada Canada #&gt; United.States United.States between wage and education, and heteroskedasticity in wage as a function of education. # install.packages(&quot;p3d&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) library(p3d) #&gt; Loading required package: rgl #&gt; Loading required package: mgcv #&gt; Loading required package: nlme #&gt; #&gt; Attaching package: &#39;nlme&#39; #&gt; The following object is masked from &#39;package:spida2&#39;: #&gt; #&gt; getData #&gt; This is mgcv 1.8-34. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. #&gt; #&gt; Attaching package: &#39;p3d&#39; #&gt; The following objects are masked from &#39;package:spida2&#39;: #&gt; #&gt; cell, center, ConjComp, dell, disp, ell, ell.conj, ellbox, ellplus, #&gt; ellpt, ellptc, elltan, elltanc, uv #&gt; The following object is masked from &#39;package:car&#39;: #&gt; #&gt; Identify3d #&gt; The following object is masked from &#39;package:knitr&#39;: #&gt; #&gt; spin #slid %&gt;% # xyplot(sqrt(wage) ~ education, .) + # layer(panel.loess(...)) #Init3d() #Plot3d(log(infantMortality) ~ GDPperCapita + lifeFemale | region, un) #Id3d() #Id3d(&#39;United.States&#39;) #Id3d(&#39;Canada&#39;) #rownames(un) #names(un) 31.3 Box Cox Transformation This video was made by math et al. I like their channel and found this video to be a good one. 31.3.1 Additional Resources Salvatore S. Mangiaficos Summary and Analysis of Extension Program Evaluation in R, rcompanion.org/handbook/. Pdf version http://www.unige.ch/ses/sococ/cl//stat/eda/ladder.html https://www.statisticshowto.com/tukey-ladder-of-powers/ http://blackwell.math.yorku.ca/math4939/lectures/transforming_data_tukeys_ladder_of_powers.html https://thomaselove.github.io/431-notes/re-expression-tukeys-ladder-box-cox-plot.html "],["lab04.html", "32 Lab: Visualizing spatial data 32.1 La Quinta is Spanish for next to Dennys, Pt. 1 32.2 Getting started 32.3 Housekeeping 32.4 The data 32.5 Exercises", " 32 Lab: Visualizing spatial data 32.1 La Quinta is Spanish for next to Dennys, Pt. 1 Have you ever taken a road trip in the US and thought to yourself I wonder what La Quinta means. Well, the late comedian Mitch Hedberg has joked that its Spanish for next to Dennys. If youre not familiar with these two establishments, Dennys is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain. These two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab, we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data. This lab was inspired by John Reisers post in his New Jersey geographer blog. You can read that analysis here. Reisers blog post focuses on scraping data from Dennys and La Quinta Inn and Suites websites using Python. In this lab, we focus on visualization and analysis of those data. However, its worth noting that the data scraping was also done in R. Later in the course,we will discuss web scraping using R . But for now, were focusing on the data that has already been scraped and tidied up for you. 32.2 Getting started 32.2.1 Packages In this lab, we will use the tidyverse and dsbox packages. The *dsbox** package is not on CRAN yet; instead it is hosted on github. You will have to download and install it yourself. This piece of code should help get you started. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio-education/dsbox&quot;) library(tidyverse) library(dsbox) If you cannot get dsbox to install, you can also download the two datasets we will be using manually here and here . githubURL_1 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/laquinta.rda&quot; githubURL_2 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/dennys.rda&quot; load(url(githubURL_1)) load(url(githubURL_2)) 32.3 Housekeeping 32.3.1 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 03 - Visualizing spatial data. 32.3.2 Warm up Before we introduce the data, lets warm up with some simple exercises. 32.3.3 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 32.3.4 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This action will prompt a dialogue box where you first need to enter your user name, and then your password. 32.4 The data The datasets well use are called dennys and laquinta from the dsbox package. Note that these data were scraped from here and here, respectively. To help with our analysis we will also use a dataset on US states: states &lt;- read_csv(&quot;data/states.csv&quot;) Each observation in this dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles). 32.5 Exercises What are the dimensions of the Dennys dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables? What are the dimensions of the La Quintas dataset? What does each row in the dataset represent? What are the variables? We would like to limit our analysis to Dennys and La Quinta locations in the United States. Take a look at the websites that the data come from (linked above). Are there any La Quintas locations outside of the US? If so, which countries? What about Dennys? Now take a look at the data. What would be some ways of determining whether or not either establishment has any locations outside the US using just the data (and not the websites). Dont worry about whether you know how to implement this, just brainstorm some ideas. Write down at least one as your answer, but youre welcomed to write down a few options too. We will determine whether or not the establishment has a location outside the US using the state variable in the dn and lq datasets. We know exactly which states are in the US, and we have this information in the states dataframe we loaded. Find the Dennys locations that are outside the US, if any. To do so, filter the Dennys locations for observations where state is not in states$abbreviation. The code for this is given below. Note that the %in% operator matches the states listed in the state variable to those listed in states$abbreviation. The ! operator means not. Are there any Dennys locations outside the US? Filter for states that are not in states$abbreviation. dn %&gt;% filter(!(state %in% states$abbreviation)) Add a country variable to the Dennys dataset and set all observations equal to \"United States\". Remember, you can use the mutate function for adding a variable. Make sure to save the result of this as dn again so that the stored data frame contains the new variable going forward. Comment: We dont need to tell R how many times to repeat the character string United States to fill in the data for all observations, R takes care of that automatically. dn %&gt;% mutate(country = &quot;United States&quot;) Find the La Quinta locations that are outside the US, and figure out which country they are in. This might require some googling. Take notes, you will need to use this information in the next exercise. Add a country variable to the La Quinta dataset. Use the case_when function to populate this variable. Youll need to refer to your notes from Exercise 7 about which country the non-US locations are in. Here is some starter code to get you going: lq %&gt;% mutate(country = case_when( state %in% state.abb ~ &quot;United States&quot;, state %in% c(&quot;ON&quot;, &quot;BC&quot;) ~ &quot;Canada&quot;, state == &quot;ANT&quot; ~ &quot;Colombia&quot;, ... # fill in the rest )) Going forward we will work with the data from the United States only. All Dennys locations are in the United States, so we dont need to worry about them. However we do need to filter the La Quinta dataset for locations in United States. lq &lt;- lq %&gt;% filter(country == &quot;United States&quot;) Which states have the most and fewest Dennys locations? What about La Quinta? Is this surprising? Why or why not? Next, lets calculate which states have the most Dennys locations per thousand square miles. This requires joinining information from the frequency tables you created in the previous set with information from the states data frame. First, we count how many observations are in each state, which will give us a data frame with two variables: state and n. Then, we join this data frame with the states data frame. However note that the variables in the states data frame that has the two-letter abbreviations is called abbreviation. So when were joining the two data frames we specify that the state variable from the Dennys data should be matched by the abbreviation variable from the states data: dn %&gt;% count(state) %&gt;% inner_join(states, by = c(&quot;state&quot; = &quot;abbreviation&quot;)) Before you move on the the next question, run the code above and take a look at the output. In the next exercise, you will need to build on this pipe. Which states have the most Dennys locations per thousand square miles? What about La Quinta? Next, we put the two datasets together into a single data frame. However before we do so, we need to add an identifier variable. Well call this establishment and set the value to \"Denny's\" and \"La Quinta\" for the dn and lq data frames, respectively. dn &lt;- dn %&gt;% mutate(establishment = &quot;Denny&#39;s&quot;) lq &lt;- lq %&gt;% mutate(establishment = &quot;La Quinta&quot;) Because the two data frames have the same columns, we can easily bind them with the bind_rows function: dn_lq &lt;- bind_rows(dn, lq) We can plot the locations of the two establishments using a scatter plot, and color the points by the establishment type. Note that the latitude is plotted on the x-axis and the longitude on the y-axis. ggplot(dn_lq, mapping = aes(x = longitude, y = latitude, color = establishment)) + geom_point() The following two questions ask you to create visualizations. These vizualizations should follow best practices you learned in class, such as informative titles, axis labels, etc. See http://ggplot2.tidyverse.org/reference/labs.html for help with the syntax. You can also choose different themes to change the overall look of your plots, see http://ggplot2.tidyverse.org/reference/ggtheme.html for help with these. Filter the data for observations in North Carolina only, and recreate the plot. You should also adjust the transparency of the points, by setting the alpha level, so that its easier to see the overplotted ones. Visually, does Mitch Hedbergs joke appear to hold here? Now filter the data for observations in Texas only, and recreate the plot, with an appropriate alpha level. Visually, does Mitch Hedbergs joke appear to hold here? Thats it for now! In the next lab, we will take a more quantitative approach to answering these questions. "],["welcome-to-tips-for-effective-data-visualization.html", "33 Welcome to Tips for effective data visualization 33.1 Module Materials 33.2 Estimated Video Length", " 33 Welcome to Tips for effective data visualization This module is designed to introduce ideas related to effective data visualization. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 33.1 Module Materials Slides Tips for effective data visualization Deep Diving into ggplot Activities Brexit Lab More Dennys Suggested Readings A layered grammar of graphics All subchapters of this module r4ds Graphics for communication Data visualisation 33.2 Estimated Video Length No of videos : 7 Average length of video : 12 minutes, 23 seconds Total length of playlist : 1 hour, 26 minutes, 46 seconds Me to the first drafts of my data visualizations : pic.twitter.com/oorGsjxe9Z&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) February 16, 2021 "],["designing-effective-visualizations.html", "34 Designing effective visualizations 34.1 Principles for effective visualizations!", " 34 Designing effective visualizations You can follow along with the slides here if they do not appear below. 34.1 Principles for effective visualizations! You can follow along with the slides here if they do not appear below. "],["deeper-diving-into-ggplot2.html", "35 Deeper Diving into ggplot2! 35.1 What are the components of a plot? 35.2 Stats, Geoms, and Positions 35.3 Scales and Coordinates 35.4 How this all works with Minard", " 35 Deeper Diving into ggplot2! You can follow along with the slides here if they do not appear below. 35.1 What are the components of a plot? You can follow along with the slides here if they do not appear below. 35.2 Stats, Geoms, and Positions You can follow along with the slides here if they do not appear below. 35.2.1 Jitter to the rescue! now with jitter: pic.twitter.com/XziQHdYN4s&mdash; Mijke Rhemtulla (@mijkenijk) February 28, 2021 35.3 Scales and Coordinates You can follow along with the slides here if they do not appear below. 35.4 How this all works with Minard You can follow along with the slides here if they do not appear below. "],["plots-behaving-badly.html", "36 Plots Behaving Badly 36.1 General Principles 36.2 High correlation does not imply replication 36.3 Barpots for paired data 36.4 Gratuitous 3D 36.5 Ignoring important factors 36.6 Too many significant digits 36.7 Displaying data well 36.8 Some further reading:", " 36 Plots Behaving Badly This section is based on talk by Karl W. Broman titled How to Display Data Badly in which he described how the default plots offered by Microsoft Excel obscure your data and annoy your readers. His lecture was inspired by Howard Wainers 1984 paper: How to display data badly. American Statistician 38(2): 137147. Dr. Wainer was the first to elucidate the principles of the bad display of data. But according to Karl The now widespread use of Microsoft Excel has resulted in remarkable advances in the field. 36.1 General Principles General principles The aims of good data graphics is to display data accurately and clearly. Some rules for displaying data badly: Display as little information as possible. Obscure what you do show (with chart junk). Use pseudo-3d and color gratuitously. Make a pie chart (preferably in color and 3d). Use a poorly chosen scale. Ignore significant figures. 36.1.1 Piecharts Say we want the report the results from a poll asking about browser preference (taken in August 2013). The standard way of displaying these is with a piechart: pie(browsers,main=&quot;Browser Usage (August 2013)&quot;) But as stated by the help file for the pie function: Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data. To see this, look at the figure above an try to determine the percentages just from looking at the plot. Simply showing the numbers is not only clear but it saves on printing costs. browsers #&gt; Opera Safari Firefox IE Chrome #&gt; 1 9 20 26 44 If you do want to plot them, then a barplot is appropriate: barplot(browsers,main=&quot;Browser Usage (August 2013)&quot;) Note that we can now pretty easily determine the percentages by following a horizontal line to the x-axis. Do avoid 3-D version as the obfuscate the plot and remove this particular advantage. Note that even worse that piecharts are donut plots. The reason is that by removing the center we remove one of the visual cues for determining the different areas: the angles. There is no reason to ever use a donut to display data. 36.1.2 Barplots as data summaries While barplots are useful for showing percentages, they are incorrectly used to display data from two groups begin compared. Specifically, barplots are created with height equal to the group means and an antenna is added at the top to represent standard errors. This plot is simply showing two numbers per groups and the plot adds nothing: Much more informative is to summarizing with a boxplot. If the number of points is small enough, we might as well add them to the plot. When the number of points is too large for us to see them, just showing a boxplot is preferable. #library(&quot;downloader&quot;) #tmpfile &lt;- tempfile() #download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig1.RData&quot;) load(&quot;data/badgraphfig1.RData&quot;) library(rafalib) mypar(1,1) dat &lt;- list(Treatment=x,Control=y) boxplot(dat,xlab=&quot;Group&quot;,ylab=&quot;Response&quot;,cex=0) stripchart(dat,vertical=TRUE,method=&quot;jitter&quot;,pch=16,add=TRUE,col=1) Note how much more we see here: the center, spread, range and the points themselves while in the barplot we only see the mean and the SE and the SE has more to do with sample size than the spread of the data. This problem is magnified when our data has outliers or very large tails. Note that from this plot there appears to be very large and consistent difference between the two groups: A quick look at the data demonstrates that this difference is mostly driven by just two points. A version showing the data in the log-scale is much more informative. #download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig3.RData&quot;,tmpfile) ##overwrite #load(tmpfile) load(&quot;data/badgraphfig3.RData&quot;) library(rafalib) mypar(1,2) dat &lt;- list(Treatment=x,Control=y) boxplot(dat,xlab=&quot;Group&quot;,ylab=&quot;Response&quot;,xlab=&quot;Group&quot;,ylab=&quot;Response&quot;,cex=0) #&gt; Warning in bxp(list(stats = structure(c(0.0304325063967286, 0.175489573309659, : #&gt; Duplicated arguments xlab = &quot;Group&quot;, ylab = &quot;Response&quot; are disregarded stripchart(dat,vertical=TRUE,method=&quot;jitter&quot;,pch=16,add=TRUE,col=1) boxplot(dat,xlab=&quot;Group&quot;,ylab=&quot;Response&quot;,xlab=&quot;Group&quot;,ylab=&quot;Response&quot;,log=&quot;y&quot;,cex=0) #&gt; Warning in bxp(list(stats = structure(c(0.0304325063967286, 0.175489573309659, : #&gt; Duplicated arguments xlab = &quot;Group&quot;, ylab = &quot;Response&quot; are disregarded stripchart(dat,vertical=TRUE,method=&quot;jitter&quot;,pch=16,add=TRUE,col=1) 36.1.3 Show the scatterplot The purpose of many statistical analyses is to determine relationships between two variables. Sample correlations are typically reported and sometimes plots are displayed to show this. However, showing just the regression line is one way to display your data baldy as it hides the scatter. Surprisingly plots such as the following are commonly seen: #download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig4.RData&quot;,tmpfile) load(&quot;data/badgraphfig4.RData&quot;) plot(x,y,lwd=2,type=&quot;n&quot;) fit &lt;- lm(y~x) abline(fit$coef,lwd=2) b &lt;- round(fit$coef,4) text(78, 200, paste(&quot;y =&quot;, b[1], &quot;+&quot;, b[2], &quot;x&quot;), adj=c(0,0.5)) rho &lt;- round(cor(x,y),4) # 0.8567 text(78, 187,expression(paste(rho,&quot; = 0.8567&quot;)),adj=c(0,0.5)) Showing the data is much more informative: plot(x,y,lwd=2) fit &lt;- lm(y~x) abline(fit$coef,lwd=2) 36.2 High correlation does not imply replication When new technologies or laboratory techniques are introduced, we are often shown scatter plots and correlations from replicated samples. High correlations are used to demonstrate that the new technique is reproducible. But correlation can be very misleading. Below is a scatter plot showing data from replicated samples run on a high throughput technology. This technology outputs 12,626 simultaneously measurements. In the plot on the left we see the original data which shows very high correlation. But the data follows a distribution with very fat tails. Note that 95% of the data is below the green line. The plot on the right is in the log scale. # if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) # BiocManager::install(&quot;Biobase&quot;) # BiocManager::install(&quot;SpikeInSubset&quot;) library(Biobase) library(SpikeInSubset) data(mas95) mypar(1,2) r &lt;- exprs(mas95)[,1] ##original measures were not logged g &lt;- exprs(mas95)[,2] plot(r,g,lwd=2,cex=0.2,pch=16, xlab=expression(paste(E[1])), ylab=expression(paste(E[2])), main=paste0(&quot;corr=&quot;,signif(cor(r,g),3))) abline(0,1,col=2,lwd=2) f &lt;- function(a,x,y,p=0.95) mean(x&lt;=a &amp; y&lt;=a)-p a95 &lt;- uniroot(f,lower=2000,upper=20000,x=r,y=g)$root abline(a95,-1,lwd=2,col=1) text(8500,0,&quot;95% of data below this line&quot;,col=1,cex=1.2,adj=c(0,0)) r &lt;- log2(r) g &lt;- log2(g) plot(r,g,lwd=2,cex=0.2,pch=16, xlab=expression(paste(log[2], &quot; &quot;, E[1])), ylab=expression(paste(log[2], &quot; &quot;, E[2])), main=paste0(&quot;corr=&quot;,signif(cor(r,g),3))) abline(0,1,col=2,lwd=2) Although the correlation is reduced in the log-scale, it is very close to 1 in both cases. Does this mean these data are reproduced? To examine how well the second vector reproduces the first What we need to study the differences so we should instead plot that. In this plot we plot the difference (in the log scale) versus the average: mypar(1,1) plot((r+g)/2,(r-g),lwd=2,cex=0.2,pch=16, xlab=expression(paste(&quot;Ave{ &quot;,log[2], &quot; &quot;, E[1],&quot;, &quot;,log[2], &quot; &quot;, E[2],&quot; }&quot;)), ylab=expression(paste(log[2],&quot; { &quot;,E[1],&quot; / &quot;,E[2],&quot; }&quot;)), main=paste0(&quot;SD=&quot;,signif(sqrt(mean((r-g)^2)),3))) abline(h=0,col=2,lwd=2) These are referred to as Bland-Altman plots or MA plots in the genomics literature and will say more later. In this plot we see that the typical difference in the log (base 2) scale between two replicated measures is about 1. This means that when measurements should be the same we will, on average, observe 2 fold difference. We can now compare this variability to the differences we want to detect and decide if this technology is precise enough for our purposes. 36.3 Barpots for paired data A common task in data analysis is the comparison of two groups. When the dataset is small and data are paired, for example outcomes before and after a treatment, an unfortunate display that is used is the barplot with two colors: There are various better ways of showing these data to illustrate there is an increase after treatment. One is to simply make a scatterplot and which shows that most points are above the identity line. Another alternative is plot the differences against the before values. set.seed(12201970) before &lt;- runif(6, 5, 8) after &lt;- rnorm(6, before*1.05, 2) li &lt;- range(c(before, after)) ymx &lt;- max(abs(after-before)) mypar(1,2) plot(before, after, xlab=&quot;Before&quot;, ylab=&quot;After&quot;, ylim=li, xlim=li) abline(0,1, lty=2, col=1) plot(before, after-before, xlab=&quot;Before&quot;, ylim=c(-ymx, ymx), ylab=&quot;Change (After - Before)&quot;, lwd=2) abline(h=0, lty=2, col=1) Line plots are not a bad choice, although I find them harder to follow than the previous two. Boxplots show you the increase, but lose the paired information. z &lt;- rep(c(0,1), rep(6,2)) mypar(1,2) plot(z, c(before, after), xaxt=&quot;n&quot;, ylab=&quot;Response&quot;, xlab=&quot;&quot;, xlim=c(-0.5, 1.5)) axis(side=1, at=c(0,1), c(&quot;Before&quot;,&quot;After&quot;)) segments(rep(0,6), before, rep(1,6), after, col=1) boxplot(before,after,names=c(&quot;Before&quot;,&quot;After&quot;),ylab=&quot;Response&quot;) 36.4 Gratuitous 3D The follow figure shows three curves. Pseudo 3D is used but it is not clear way. Maybe to separate the three curves? Note how difficult it is to determine the values of the curves at any given point: This plot can be made better by simply using color to distinguish the three lines: #download(&quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv&quot;,tmpfile) x &lt;- read.table(&quot;data//fig8dat.csv&quot;, sep=&quot;,&quot;, header=TRUE) plot(x[,1],x[,2],xlab=&quot;log Dose&quot;,ylab=&quot;Proportion survived&quot;,ylim=c(0,1), type=&quot;l&quot;,lwd=2,col=1) lines(x[,1],x[,3],lwd=2,col=2) lines(x[,1],x[,4],lwd=2,col=3) legend(1,0.4,c(&quot;Drug A&quot;,&quot;Drug B&quot;,&quot;Drug C&quot;),lwd=2, col=1:3) 36.5 Ignoring important factors In this example we generate data with a simulation. We are studying a dose response relationship between two groups treatment and control. We have three groups of measurements for both control and treatment. Comparing treatment and control using the common barplot: Instead we should show each curve. We can use color to distinguish treatment and control and dashed and solid lines to distinguish the original data from the mean of the three groups. plot(x, y1, ylim=c(0,1), type=&quot;n&quot;, xlab=&quot;Dose&quot;, ylab=&quot;Response&quot;) for(i in 1:3) lines(x, z[,i], col=1, lwd=1, lty=2) for(i in 1:3) lines(x, y[,i], col=2, lwd=1, lty=2) lines(x, ym, col=1, lwd=2) lines(x, zm, col=2, lwd=2) legend(&quot;bottomleft&quot;, lwd=2, col=c(1, 2), c(&quot;Control&quot;, &quot;Treated&quot;)) 36.6 Too many significant digits By default, statistical software like R return many significant digits. This does not mean we should report them. Cutting and pasting directly from R is a bad idea as you might end up showing a table like this for, say, heights of basketball players: heights &lt;- cbind(rnorm(8,73,3),rnorm(8,73,3),rnorm(8,80,3), rnorm(8,78,3),rnorm(8,78,3)) colnames(heights)&lt;-c(&quot;SG&quot;,&quot;PG&quot;,&quot;C&quot;,&quot;PF&quot;,&quot;SF&quot;) rownames(heights)&lt;- paste(&quot;team&quot;,1:8) heights #&gt; SG PG C PF SF #&gt; team 1 76.4 76.2 81.7 75.3 77.2 #&gt; team 2 74.1 71.1 80.3 81.6 73.0 #&gt; team 3 71.5 69.0 85.8 80.1 72.8 #&gt; team 4 78.7 72.8 81.3 76.3 82.9 #&gt; team 5 73.4 73.3 79.2 79.7 80.3 #&gt; team 6 72.9 71.8 77.4 81.7 80.4 #&gt; team 7 68.4 73.0 79.1 71.2 77.2 #&gt; team 8 73.8 75.6 83.0 75.6 87.7 Note we are reporting precision up to 0.00001 inches. Do you know of a tape measure with that much precision? This can be easily remedied: round(heights,1) #&gt; SG PG C PF SF #&gt; team 1 76.4 76.2 81.7 75.3 77.2 #&gt; team 2 74.1 71.1 80.3 81.6 73.0 #&gt; team 3 71.5 69.0 85.8 80.1 72.8 #&gt; team 4 78.7 72.8 81.3 76.3 82.9 #&gt; team 5 73.4 73.3 79.2 79.7 80.3 #&gt; team 6 72.9 71.8 77.4 81.7 80.4 #&gt; team 7 68.4 73.0 79.1 71.2 77.2 #&gt; team 8 73.8 75.6 83.0 75.6 87.7 36.7 Displaying data well In general you should follow these principles: Be accurate and clear. Let the data speak. Show as much information as possible, taking care not to obscure the message. Science not sales: avoid unnecessary frills (esp. gratuitous 3d). In tables, every digit should be meaningful. Dont drop ending 0s. 36.8 Some further reading: ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Lets practice what we preach: Turning tables into graphs. The American Statistician 56:121-130 NB Robbins (2004) Creating more effective graphs. Wiley Nature Methods columns "],["odd-design-choices-in-data-visualization.html", "37 ODD: Design choices in data visualization 37.1 How to spot a misleading graph 37.2 Data Visualization and Misrepresentation 37.3 Vox on How coronavirus charts can mislead us 37.4 Vox on Shut up about the y-axis. It shouldnt always start at zero. 37.5 Gloriously Terrible Plots!!", " 37 ODD: Design choices in data visualization I have curated this collection of external video sources on design choices in data visualization. 37.1 How to spot a misleading graph When theyre used well, graphs can help us intuitively grasp complex data. But as visual software has enabled more usage of graphs throughout all media, it has also made them easier to use in a careless or dishonest way  and as it turns out, there are plenty of ways graphs can mislead and outright manipulate. Lea Gaslowitz shares some things to look out for. 37.2 Data Visualization and Misrepresentation This animation was produced by some of my colleagues at Brown. 37.3 Vox on How coronavirus charts can mislead us 37.4 Vox on Shut up about the y-axis. It shouldnt always start at zero. Also a nice example of appropriately choosing a y-axis window (cf. @sharoz) https://t.co/wCiOeyTo6k&mdash; Brenton Wiernik  (@bmwiernik) February 18, 2021 37.5 Gloriously Terrible Plots!! this is the worst figure i&#39;ve ever made in R. completely useless. 10/10 would make this aRt again pic.twitter.com/8BN9j5H2C0&mdash; Dr. latentchange (@latentchange) April 9, 2021 "],["secrets.html", "38 ODD: Secrets of a happy graphing life 38.1 Load gapminder and the tidyverse 38.2 Hidden data wrangling problems 38.3 Keep stuff in data frames 38.4 Worked example", " 38 ODD: Secrets of a happy graphing life This chapter is an optional deep dive (ODD) that might be useful to you. 38.1 Load gapminder and the tidyverse library(gapminder) library(tidyverse) 38.2 Hidden data wrangling problems If you are struggling to make a figure, dont assume its a problem between you and ggplot2. Stop and ask yourself which of these rules you are breaking: Keep stuff in data frames Keep your data frames tidy; be willing to reshape your data often Use factors and be the boss of them In my experience, the vast majority of graphing agony is due to insufficient data wrangling. Tackle your latent data storage and manipulation problems and your graphing problem often melts away. 38.3 Keep stuff in data frames I see a fair amount of early-career code where variables are copied out of a data frame, to exist as stand-alone objects in the workspace. life_exp &lt;- gapminder$lifeExp year &lt;- gapminder$year Historically, ggplot2 has had an incredibly strong preference for variables in data frames. It used to be a requirement for the main data frame underpinning a plot. Although this requirement is no longer the case, it is still a good idea to keep your variables associated with their data frames. ggplot(mapping = aes(x = year, y = life_exp)) + geom_jitter() Just leave the variables in place and pass the associated data frame! This advice applies to base and lattice graphics as well. It is not specific to ggplot2. ggplot(data = gapminder, aes(x = year, y = life_exp)) + geom_jitter() What if we wanted to filter the data by country, continent, or year? This is much easier to do safely if all affected variables live together in a data frame, not as individual objects that can get out of sync. Dont write-off ggplot2 as a highly opinionated outlier! In fact, keeping data in data frames and computing and visualizing it in situ are widely regarded as best practices. The option to pass a data frame via data = is a common feature of many high-use R functions, e.g. lm(), aggregate(), plot(), and t.test(), so make this your default modus operandi. 38.3.1 Explicit data frame creation via tibble::tibble() and tibble::tribble() If your data is already lying around and its not in a data frame, ask yourself why not? Did you create those variables? Maybe you should have created them in a data frame in the first place! The tibble() function is an improved version of the built-in data.frame(), which makes it possible to define one variable in terms of another and which wont turn character data into factor. If constructing tiny tibbles by hand, tribble() can be an even handier function, in which your code will be laid out like the table you are creating. These functions should remove the most common excuses for data frame procrastination and avoidance. my_dat &lt;- tibble(x = 1:5, y = x ^ 2, text = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;delta&quot;, &quot;epsilon&quot;)) ## if you&#39;re truly &quot;hand coding&quot;, tribble() is an alternative my_dat &lt;- tribble( ~ x, ~ y, ~ text, 1, 1, &quot;alpha&quot;, 2, 4, &quot;beta&quot;, 3, 9, &quot;gamma&quot;, 4, 16, &quot;delta&quot;, 5, 25, &quot;epsilon&quot; ) str(my_dat) #&gt; tibble[,3] [5 x 3] (S3: tbl_df/tbl/data.frame) #&gt; $ x : num [1:5] 1 2 3 4 5 #&gt; $ y : num [1:5] 1 4 9 16 25 #&gt; $ text: chr [1:5] &quot;alpha&quot; &quot;beta&quot; &quot;gamma&quot; &quot;delta&quot; ... ggplot(my_dat, aes(x, y)) + geom_line() + geom_text(aes(label = text)) Together with dplyr::mutate(), which adds new variables to a data frame, this gives you the tools to work within data frames whenever youre handling related variables of the same length. 38.3.2 Sidebar: with() Sadly, not all functions offer a data = argument. Take cor(), for example, which computes correlation. This does not work: cor(year, lifeExp, data = gapminder) #&gt; Error in cor(year, lifeExp, data = gapminder): unused argument (data = gapminder) Sure, you can always just repeat the data frame name like so: cor(gapminder$year, gapminder$lifeExp) #&gt; [1] 0.436 but people hate typing. I suspect subconscious dread of repeatedly typing gapminder is what motivates those who copy variables into stand-alone objects in the workspace. The with() function is a better workaround. Provide the data frame as the first argument. The second argument is an expression that will be evaluated in a special environment. It could be a single command or a multi-line snippet of code. Whats special is that you can refer to variables in the data frame by name. with(gapminder, cor(year, lifeExp)) #&gt; [1] 0.436 If you use the magrittr package, another option is to use the %$% operator to expose the variables inside a data frame for further computation: library(magrittr) gapminder %$% cor(year, lifeExp) #&gt; [1] 0.436 38.4 Worked example Inspired by this question from a student when we first started using ggplot2: How can I focus in on country, Japan for example, and plot all the quantitative variables against year? Your first instinct might be to filter the Gapminder data for Japan and then loop over the variables, creating separate plots which need to be glued together. And, indeed, this can be done. But in my opinion, the data reshaping route is more R native given our current ecosystem, than the loop way. 38.4.1 Reshape your data We filter the Gapminder data and keep only Japan. Then we use tidyr::gather() to gather up the variables pop, lifeExp, and gdpPercap into a single value variable, with a companion variable key. japan_dat &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) japan_tidy &lt;- japan_dat %&gt;% gather(key = var, value = value, pop, lifeExp, gdpPercap) dim(japan_dat) #&gt; [1] 12 6 dim(japan_tidy) #&gt; [1] 36 5 The filtered japan_dat has 12 rows. Since we are gathering or stacking three variables in japan_tidy, it makes sense to see three times as many rows, namely 36 in the reshaped result. 38.4.2 Iterate over the variables via faceting Now that we have the data we need in a tidy data frame, with a proper factor representing the variables we want to iterate over, we just have to facet. p &lt;- ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~ var, scales=&quot;free_y&quot;) p + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) 38.4.3 Recap Heres the minimal code to produce our Japan example. japan_tidy &lt;- gapminder %&gt;% filter(country == &quot;Japan&quot;) %&gt;% gather(key = var, value = value, pop, lifeExp, gdpPercap) ggplot(japan_tidy, aes(x = year, y = value)) + facet_wrap(~ var, scales=&quot;free_y&quot;) + geom_point() + geom_line() + scale_x_continuous(breaks = seq(1950, 2011, 15)) This snippet demonstrates the payoffs from the rules we laid out at the start: We isolate the Japan data into its own data frame. We reshape the data. We gather three columns into one, because we want to depict them via position along the y-axis in the plot. We use a factor to distinguish the observations that belong in each mini-plot, which then becomes a simple application of faceting. This is an example of expedient data reshaping. I dont actually believe that gdpPercap, lifeExp, and pop naturally belong together in one variable. But gathering them was by far the easiest way to get this plot. "],["lab05.html", "39 Lab: Wrangling spatial data 39.1 La Quinta is Spanish for next to Dennys, Pt. 2\" 39.2 Getting started 39.3 Warm up 39.4 The data 39.5 Exercises", " 39 Lab: Wrangling spatial data 39.1 La Quinta is Spanish for next to Dennys, Pt. 2\" In this lab, we revisit the Dennys and La Quinta Inn and Suites data we visualized in the previous lab. 39.2 Getting started Go to the course organization on GitHub. Find your lab repo. 39.2.1 Packages In this lab, we will use the tidyverse and dsbox packages. The *dsbox** package is not on CRAN yet; instead it is hosted on github. You will have to download and install it yourself. This piece of code should help get you started. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio-education/dsbox&quot;) library(tidyverse) library(dsbox) If you cannot get dsbox to install, you can also download the two datasets we will be using manually here and here. githubURL_1 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/laquinta.rda&quot; githubURL_2 &lt;- &quot;https://github.com/DataScience4Psych/DataScience4Psych/raw/main/data/raw-data/dennys.rda&quot; load(url(githubURL_1)) load(url(githubURL_2)) 39.2.2 Housekeeping 39.2.2.1 Password caching If you would like your git password cached for a week for this project, type the following in the Terminal: git config --global credential.helper &#39;cache --timeout 604800&#39; 39.2.2.2 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 05 - Wrangling spatial data. 39.3 Warm up Before we introduce the data, lets warm up with some simple exercises. 39.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 39.3.2 Commiting and pushing changes: Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This action will prompt a dialogue box where you first need to enter your user name, and then your password. 39.4 The data The datasets well use are called dennys and laquinta from the dsbox package. 39.5 Exercises Filter the Dennys dataframe for Alaska (AK) and save the result as dn_ak. How many Dennys locations are there in Alaska? dn_ak &lt;- dn %&gt;% filter(state == &quot;AK&quot;) nrow(dn_ak) Filter the La Quinta dataframe for Alaska (AK) and save the result as lq_ak. How many La Quinta locations are there in Alaska? lq_ak &lt;- lq %&gt;% filter(state == &quot;AK&quot;) nrow(lq_ak) Next well calculate the distance between all Dennys and all La Quinta locations in Alaska. Lets take this step by step: Step 1: There are 3 Dennys and 2 La Quinta locations in Alaska. (If you answered differently above, you might want to recheck your answers.) Step 2: Lets focus on the first Dennys location. Well need to calculate two distances for it: (1) distance between Dennys 1 and La Quinta 1 and (2) distance between Dennys 1 and La Quinta (2). Step 3: Now lets consider all Dennys locations. How many pairings are there between all Dennys and all La Quinta locations in Alaska, i.e., how many distances do we need to calculate between the locations of these establishments in Alaska? In order to calculate these distances, we need to first restructure our data to pair the Dennys and La Quinta locations. To do so, we will join the two data frames. We have six join options in R. Each of these join functions take at least three arguments: x, y, and by. x and y are data frames to join by is the variable(s) to join by Four of these join functions combine variables from the two dataframes: Note: These functions are called mutating joins. inner_join(): return all rows from x where there are matching values in y, and all columns from x and y. left_join(): return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. right_join(): return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. full_join(): return all rows and all columns from both x and y, where there are not matching values, returns NA for the one missing. And the other two join functions only keep cases from the left-hand data frame, and are called filtering joins. Well learn about these another time, but you can find out more about the join functions in the help files for any one of them, e.g. ?full_join. In practice, we mostly use mutating joins. In this case, we want to keep all rows and columns from both dn_ak and lq_ak data frames. So we will use a full_join. Full join of Dennys and La Quinta locations in AK Lets join the data on Dennys and La Quinta locations in Alaska, and take a look at what it looks like: dn_lq_ak &lt;- full_join(dn_ak, lq_ak, by = &quot;state&quot;) dn_lq_ak How many observations are in the joined dn_lq_ak data frame? What are the names of the variables in this data frame. .x in the variable names means the variable comes from the x data frame (the first argument in the full_join call, i.e. dn_ak), and .y means the variable comes from the y data frame. These variables are renamed to include .x and .y because the two data frames have the same variables and its not possible to have two variables in a data frame with the exact same name. Now that we have the data in the format we wanted, all that is left is to calculate the distances between the pairs. What function from the tidyverse do we use the add a new variable to a data frame while keeping the existing variables? One way of calculating the distance between any two points on the earth is to use the Haversine distance formula. This formula takes into account the fact that the earth is not flat, but instead spherical. This function is not available in R, but we have it saved in a file called haversine.R that we can load and then use: haversine &lt;- function(long1, lat1, long2, lat2, round = 3) { # convert to radians long1 = long1 * pi / 180 lat1 = lat1 * pi / 180 long2 = long2 * pi / 180 lat2 = lat2 * pi / 180 R = 6371 # Earth mean radius in km a = sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((long2 - long1)/2)^2 d = R * 2 * asin(sqrt(a)) return( round(d,round) ) # distance in km } This function takes five arguments: Longitude and latitude of the first location Longitude and latitude of the second location A parameter by which to round the responses Calculate the distances between all pairs of Dennys and La Quinta locations and save this variable as distance. Make sure to save this variable in THE dn_lq_ak data frame so that you can use it later. Calculate the minimum distance between a Dennys and La Quinta for each Dennys location. To do so we group by Dennys locations and calculate a new variable that stores the information for the minimum distance. dn_lq_ak_mindist &lt;- dn_lq_ak %&gt;% group_by(address.x) %&gt;% summarise(closest = min(distance)) Describe the distribution of the distances Dennys and the nearest La Quinta locations in Alaska. Also include an appripriate visualization and relevant summary statistics. Repeat the same analysis for North Carolina: (i) filter Dennys and La Quinta Data Frames for NC, (ii) join these data frames to get a complete list of all possible pairings, (iii) calculate the distances between all possible pairings of Dennys and La Quinta in NC, (iv) find the minimum distance between each Dennys and La Quinta location, (v) visualize and describe the distribution of these shortest distances using appropriate summary statistics. Repeat the same analysis for Texas. Repeat the same analysis for a state of your choosing, different than the ones we covered so far. Among the states you examined, where is Mitch Hedbergs joke most likely to hold true? Explain your reasoning. "],["mod06.html", "40 Welcome to Confounding and Communication! 40.1 Module Materials 40.2 Video Length", " 40 Welcome to Confounding and Communication! This module introduces ideas related to study design and science communication. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. Note that you only have to do one of the labs. Pick either (or both if you want) 40.1 Module Materials Slides from Lectures Scientific studies and confounding Communicating data science results effectively Suggested Readings Judea Pearl on Understanding Simpsons Paradox R4DS on communication, including Graphics for Communication All subchapters of this module Lab (Choose one or both) Ugly Charts Smokers 40.2 Video Length No of videos : 5 Average length of video : 11 minutes, 16 seconds Total length of playlist : 56 minutes, 20 seconds "],["scientific-studies-and-confounding.html", "41 Scientific studies and confounding! 41.1 Scientific studies 41.2 Climate Change: A Conditional Probability Case Study 41.3 Introducing Simpsons Paradox with a case study 41.4 Revisiting Simpsons Paradox", " 41 Scientific studies and confounding! You can follow along with the slides here if they do not appear below. 41.1 Scientific studies 41.2 Climate Change: A Conditional Probability Case Study 41.3 Introducing Simpsons Paradox with a case study 41.4 Revisiting Simpsons Paradox "],["communicating-data-science-results-effectively.html", "42 Communicating data science results effectively!", " 42 Communicating data science results effectively! You can follow along with the slides here if they do not appear below. This . If you want to be a collaborative statistician, you need to be able to talk to and help people who don&#39;t have the same training as you. It&#39;s not their job to be able to talk like they have a degree in statistics. https://t.co/XcvXazNHEw&mdash; Daphna Harel (@DaphnaHarel) March 2, 2021 "],["lab06a.html", "43 Lab A: Ugly charts 43.1 Getting started 43.2 Packages 43.3 Take a sad plot and make it better 43.4 Wrapping up 43.5 More ugly charts", " 43 Lab A: Ugly charts Given below are two data visualizations that violate many data visualization best practices. Improve these visualizations using R and the tips for effective visualizations that weve introduced. You should produce one visualization per dataset. Your visualization should be accompanied by a brief paragraph describing the choices you made in your improvement, specifically discussing what you didnt like in the original plots and why, and how you addressed them in the visualization you created. The learning goals for this lab are: Telling a story with data Data visualization best practices Reshaping data 43.1 Getting started Go to the course GitHub organization and locate your lab repo. Fork that lab and then clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document and Knit it. Make sure it compiles without errors. (Also, remember to check the final version after you upload!) The output will be in the file markdown .md file with the same name. 43.1.1 Housekeeping Remember: Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to do some configuration so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. Run the following (but update it for your name and email!) in the Console to configure git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 43.2 Packages Run the following code in the Console to load this package. library(tidyverse) 43.3 Take a sad plot and make it better 43.3.1 Instructional staff employment trends The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report compiled by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below. Lets start by loading the data used to create this plot. staff &lt;- read_csv(&quot;data/instructional-staff.csv&quot;) Each row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year. ## # A tibble: 5 x 12 ## faculty_type `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full-Time Tenu~ 29 27.6 25 24.8 21.8 20.3 19.3 17.8 17.2 ## 2 Full-Time Tenu~ 16.1 11.4 10.2 9.6 8.9 9.2 8.8 8.2 8 ## 3 Full-Time Non-~ 10.3 14.1 13.6 13.6 15.2 15.5 15 14.8 14.9 ## 4 Part-Time Facu~ 24 30.4 33.1 33.2 35.5 36 37 39.3 40.5 ## 5 Graduate Stude~ 20.5 16.5 18.1 18.8 18.7 19 20 19.9 19.5 ## # ... with 2 more variables: 2009 &lt;dbl&gt;, 2011 &lt;dbl&gt; In order to recreate this visualization we need to first reshape the data to have one variable for faculty type and one variable for year. In other words, we will convert the data from wide format to long format. But before we do so, a thought exercise: How many rows will the long-format data have? It will have a row for each combination of year and faculty type. If there are 5 faculty types and 11 years of data, how many rows will we have? We do the wide to long conversion using a new function: pivot_longer(). The animation below show how this function works, as well as its counterpart pivot_wider(). The function has the following arguments: pivot_longer(data, cols, names_to = &quot;name&quot;) The first argument is data as usual. The second argument, cols, is where you specify which columns to pivot into longer format  in this case all columns except for the faculty_type The third argument, names_to, is a string specifying the name of the column to create from the data stored in the column names of data  in this case year staff_long &lt;- staff %&gt;% pivot_longer(cols = -faculty_type, names_to = &quot;year&quot;) %&gt;% mutate(value = as.numeric(value)) Lets take a look at what the new longer data frame looks like. staff_long ## # A tibble: 55 x 3 ## faculty_type year value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Full-Time Tenured Faculty 1975 29 ## 2 Full-Time Tenured Faculty 1989 27.6 ## 3 Full-Time Tenured Faculty 1993 25 ## 4 Full-Time Tenured Faculty 1995 24.8 ## 5 Full-Time Tenured Faculty 1999 21.8 ## 6 Full-Time Tenured Faculty 2001 20.3 ## 7 Full-Time Tenured Faculty 2003 19.3 ## 8 Full-Time Tenured Faculty 2005 17.8 ## 9 Full-Time Tenured Faculty 2007 17.2 ## 10 Full-Time Tenured Faculty 2009 16.8 ## # ... with 45 more rows And now lets plot is as a line plot. A possible approach for creating a line plot where we color the lines by faculty type is the following: staff_long %&gt;% ggplot(aes(x = year, y = value, color = faculty_type)) + geom_line() ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? But note that this results in a message as well as an unexpected plot. The message is saying that there is only one observation for each faculty type year combination. We can fix this using the group aesthetic following. staff_long %&gt;% ggplot(aes(x = year, y = value, group = faculty_type, color = faculty_type)) + geom_line() Include the line plot you made above in your report and make sure the figure width is large enough to make it legible. &gt; Also fix the title, axis labels, and legend label. Suppose the objective of this plot was to show that the proportion of part-time faculty have gone up over time compared to other instructional staff types. &gt; What changes would you propose making to this plot to tell this story?   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 43.3.2 Fisheries Fisheries and Aquaculture Department of the Food and Agriculture Organization of the United Nations collects data on fisheries production of countries. This Wikipedia page lists fishery production of countries for 2016. For each country, tonnage from capture and aquaculture are listed. Note that countries whose total harvest was less than 100,000 tons are not included in the visualization. A researcher shared with you the following visualization they created based on these data . Can you help them make improve it? First, brainstorm how you would improve it. Then create the improved visualization and write up the changes/decisions you made as bullet points. Its ok if some of your improvements are aspirational, i.e. you dont know how to implement it, but you think its a good idea. Ask a tutor for help, but also keep an eye on the time. Implement what you can and leave note identifying the aspirational improvements. fisheries &lt;- read_csv(&quot;data/fisheries.csv&quot;)   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 43.4 Wrapping up Go back through your write up to make sure youre following coding style guidelines we discussed in class. Make any edits as needed. Also, make sure all of your R chunks are properly labeled, and your figures are reasonably sized. 43.5 More ugly charts Want to see more ugly charts? Flowing Data - Ugly Charts Reddit - Data is ugly Missed Opportunities and Graphical Failures (Mostly Bad) Graphics and Tables "],["lab06b.html", "44 Lab B: Smokers in Whickham 44.1 Simpsons paradox 44.2 Getting started 44.3 The data 44.4 Exercises", " 44 Lab B: Smokers in Whickham 44.1 Simpsons paradox 44.2 Getting started A study of conducted in Whickham, England recorded participants age, smoking status at baseline, and then 20 years later recorded their health outcome. 44.2.1 Packages In this lab, we will work with the tidyverse and mosaicData packages. Because this is first time were using the mosaicData package, you need to make sure to install it first by running install.packages(\"mosaicData\") in the console. library(tidyverse) library(mosaicData) Note that these packages are also loaded in your R Markdown document. 44.3 The data The data is in the mosaicData package. You can load it with data(Whickham) Take a peek at the codebook with ?Whickham 44.4 Exercises What type of study do you think these data comne from: observational or experiment? Why? How many observations are in this dataset? What does each observation represent? How many variables are in this dataset? What type of variable is each? Display each variable using an appropriate visualization. What would you expect the relationship between smoking status and health outcome to be? Create a visualization depicting the relationship between smoking status and health outcome. Briefly describe the relationship, and evaluate whether this meets your expectations. Additionally, calculate the relevant conditional probabilities to help your narrative. Here is some code to get you started: Whickham %&gt;% count(smoker, outcome) Create a new variable called age_cat using the following scheme: age &lt;= 44 ~ \"18-44\" age &gt; 44 &amp; age &lt;= 64 ~ \"45-64\" age &gt; 64 ~ \"65+\" Re-create the visualization depicting the relationship between smoking status and health outcome, faceted by age_cat. What changed? What might explain this change? Extend the contingency table from earlier by breaking it down by age category and use it to help your narrative. Whickham %&gt;% count(smoker, age_cat, outcome) "],["welcome-to-web-scraping.html", "45 Welcome to web scraping 45.1 Module Materials 45.2 Estimated Video Length", " 45 Welcome to web scraping This module is designed to introduce you to the basic ideas behind web scraping. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 45.1 Module Materials Slides from Lectures Webscraping Suggested Readings All subchapters of this module R4DS, Section on functions Wrap Scraping from An Introduction to Statistical Programming Methods with R Activities Scraping IMDB Lab [lab07] 45.2 Estimated Video Length No of videos : 3 Average length of video : 14 minutes, 40 seconds Total length of playlist : 44 minutes "],["scraping-the-web.html", "46 Scraping the web 46.1 Using the SelectorGadget! 46.2 Top 250 movies on IMDB! 46.3 Activity 08: IMDB!", " 46 Scraping the web You can follow along with the slides here if they do not appear below. 46.1 Using the SelectorGadget! 46.2 Top 250 movies on IMDB! 46.3 Activity 08: IMDB! You can find the materials for the IMDB activity here. "],["data-usually-finds-me.html", "47 Data usually finds me 47.1 I dont go looking for Data  Data usually finds me", " 47 Data usually finds me Eventually, Im going to write up my SAM talk on I dont go looking for Data  Data usually finds me, from last year. However, in the meantime, here is a link to the slides of that talk. Live look at me opening a &quot;dataset&quot; pic.twitter.com/eaBEguiA11&mdash; Darren Dahly (@statsepi) November 3, 2020 47.1 I dont go looking for Data  Data usually finds me The most interesting aspects of my work (or at least to me) are the aspects related to finding data. However, this part is also the least documented. In my case, it primarily lives in footnotes, personal statements, and appendices. "],["api-wrappers.html", "48 Use API-wrapping packages 48.1 Introduction 48.2 Click-and-Download 48.3 Data supplied on the web 48.4 Install-and-play", " 48 Use API-wrapping packages These readings are adapted from Jenny Bryans stat545 and were originally written by Andrew MacDonald. Ive added bits and pieces. Google trends analytics is helpful in the study of global web search patterns.The {gtrends} function from {gtrendsR}  helps extract and visualize this data for specified periods and geolocations https://t.co/yS01ELq5q4#rstats #DataScience pic.twitter.com/mhGTSXB2rN&mdash; R Function A Day (@rfunctionaday) April 13, 2021 48.1 Introduction All this and more is described at the rOpenSci repository of R tools for interacting with the internet. There are many ways to obtain data from the internet; lets consider four categories: Click-and-download - on the internet as a flat file, such as CSV, XLS. Install-and-play - an API for which someone has written a handy R package. API-query - published with an unwrapped API. Scraping - implicit in an HTML website. 48.2 Click-and-Download In the simplest case, the data you need is already on the internet in a tabular format. There are a couple of strategies here: Use read.csv or readr::read_csv to read the data straight into R. Use the command line program curl to do that work, and place it in a Makefile or shell script (see the section on make for more on this). The second case is most useful when the data you want has been provided in a format that needs cleanup. For example, the World Value Survey makes several datasets available as Excel sheets. The safest option here is to download the .xls file, then read it into R with readxl::read_excel() or something similar. An exception to this is data provided as Google Spreadsheets, which can be read straight into R using the googlesheets package. 48.2.1 From rOpenSci web services page: From rOpenScis CRAN Task View: Web Technologies and Services: downloader::download() for SSL. curl::curl() for SSL. httr::GET data read this way needs to be parsed later with read.table(). rio::import() can read a number of common data formats directly from an https:// URL. Isnt that very similar to the previous? What about packages that install data? 48.3 Data supplied on the web Many times, the data that you want is not already organized into one or a few tables that you can read directly into R. More frequently, you find this data is given in the form of an API. Application Programming Interfaces (APIs) are descriptions of the kind of requests that can be made of a certain piece of software, and descriptions of the kind of answers that are returned. Many sources of data  databases, websites, services  have made all (or part) of their data available via APIs over the internet. Computer programs (clients) can make requests of the server, and the server will respond by sending data (or an error message). This client can be many kinds of other programs or websites, including R running from your laptop. 48.4 Install-and-play Many common web services and APIs have been wrapped, i.e. R functions have been written around them which send your query to the server and format the response. Why would we want this? Provenance Reproducible Updating Ease Scaling 48.4.1 Load the tidyverse library(tidyverse) 48.4.2 Sightings of birds: rebird rebird is an R interface for the eBird database. eBird lets birders upload sightings of birds, and allows everyone access to those data. rebird is on CRAN. # install.packages(&quot;rebird&quot;) library(rebird) 48.4.2.1 Search birds by geography The eBird website categorizes some popular locations as Hotspots. These are areas where there are both lots of birds and lots of birders. One such location is at Iona Island, near Vancouver. You can see data for this Hotspot at http://ebird.org/ebird/hotspot/L261851. At that link, you will see a page like this: Figure 48.1: Iona Island The data already looks to be organized in a data frame! rebird allows us to read these data directly into R (the ID code for Iona Island is L261851). ebirdhotspot(locID = &quot;L261851&quot;) %&gt;% head() %&gt;% kable() We can use the function ebirdgeo() to get a list for an area (note that South and West are negative): vanbirds &lt;- ebirdgeo(lat = 49.2500, lng = -123.1000) vanbirds %&gt;% head() %&gt;% kable() Note: Check the defaults on this function (e.g. radius of circle, time of year). We can also search by region, which refers to short codes which serve as common shorthands for different political units. For example, France is represented by the letters FR. frenchbirds &lt;- ebirdregion(&quot;FR&quot;) frenchbirds %&gt;% head() %&gt;% kable() Find out when a bird has been seen in a certain place! Choosing a name from vanbirds above (the Bald Eagle): eagle &lt;- ebirdgeo(species = &#39;Haliaeetus leucocephalus&#39;, lat = 42, lng = -76) eagle %&gt;% head() %&gt;% kable() rebird knows where you are: ebirdgeo(species = &#39;Buteo lagopus&#39;) 48.4.3 Searching geographic info: geonames rOpenSci has a package called geonames for accessing the GeoNames API. First, install the geonames package from CRAN and load it. # install.packages(&quot;geonames&quot;) library(geonames) The geonames package website tells us that there are a few things we need to do before we can use geonames to access the GeoNames API: Go to the GeoNames site and create a new user account. Check your email and follow the instructions to activate your account. Click [here] to enable the free web services for your account (Note! You must be logged into your GeoNames account already for the link to work). Tell R your GeoNames username. To do the last step, we could run this line in R options(geonamesUsername=&quot;my_user_name&quot;) but this is insecure. We dont want to risk committing this line and pushing it to our public GitHub page! Instead, we can add this line to our .Rprofile so it will be hidden. One way to edit your .Rprofile is with the helper function edit_r_profile() from the usethis package. Install/load the usethis package and run edit_r_profile() in the R Console: # install.packages(&quot;usethis&quot;) library(usethis) edit_r_profile() This will open up your .Rprofile file. Add options(geonamesUsername=\"my_user_name\") on a new line (replace my_user_name with your GeoNames username). Important: Make sure your .Rprofile ends with a blank line! Save the file, close it, and restart R. Now were ready to start using geonames to search the GeoNames API. (Also see the Cache credentials for HTTPS chapter of Happy Git and GitHub for the useR.) 48.4.3.1 Using GeoNames What can we do? We can get access to lots of geographical information via the various GeoNames WebServices. countryInfo &lt;- GNcountryInfo() glimpse(countryInfo) This countryInfo dataset is very helpful for accessing the rest of the data because it gives us the standardized codes for country and language. 48.4.3.2 Remixing geonames and rebird: What are the cities of France? francedata &lt;- countryInfo %&gt;% filter(countryName == &quot;France&quot;) frenchcities &lt;- with(francedata, GNcities(north = north, east = east, south = south, west = west, maxRows = 500)) glimpse(frenchcities) 48.4.4 Wikipedia searching We can use geonames to search for georeferenced Wikipedia articles. Here are those within 20 km of Rio de Janerio, comparing results for English-language Wikipedia (lang = \"en\") and Portuguese-language Wikipedia (lang = \"pt\"): rio_english &lt;- GNfindNearbyWikipedia(lat = -22.9083, lng = -43.1964, radius = 20, lang = &quot;en&quot;, maxRows = 500) rio_portuguese &lt;- GNfindNearbyWikipedia(lat = -22.9083, lng = -43.1964, radius = 20, lang = &quot;pt&quot;, maxRows = 500) nrow(rio_english) nrow(rio_portuguese) 48.4.5 Searching the Public Library of Science: rplos PLOS ONE is an open-access journal. They allow access to an impressive range of search tools, and allow you to obtain the full text of their articles. rOpenSci has a package called rplos that we can use to interact with the PLOS API. They have a nice tutorial on the rOpenSci website that you can see here. First, install/load the rplos package from CRAN. # install.packages(&quot;rplos&quot;) library(rplos) 48.4.5.1 Searching PLOS ONE Lets follow along with the rOpenSci tutorial and do some searches: searchplos(q= &quot;Helianthus&quot;, fl= &quot;id&quot;, limit = 5) #&gt; $meta #&gt; # A tibble: 1 x 2 #&gt; numFound start #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 654 0 #&gt; #&gt; $data #&gt; # A tibble: 5 x 1 #&gt; id #&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pone.0198869 #&gt; 2 10.1371/journal.pone.0213065 #&gt; 3 10.1371/journal.pone.0148280 #&gt; 4 10.1371/journal.pone.0111982 #&gt; 5 10.1371/journal.pone.0212371 searchplos(&quot;materials_and_methods:France&quot;, fl = &quot;title, materials_and_methods&quot;) #&gt; $meta #&gt; # A tibble: 1 x 2 #&gt; numFound start #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 16413 0 #&gt; #&gt; $data #&gt; # A tibble: 10 x 2 #&gt; title materials_and_methods #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Borna Disease Virus Phosphoprotein~ &quot;\\nEthics statement\\nHuman fetuses were ~ #&gt; 2 Population Structure in Naegleria ~ &quot;\\nSampling\\nThe 47 N. fowleri strains a~ #&gt; 3 Prevalent hepatitis B surface anti~ &quot;\\nStudy site\\nThe study was carried out~ #&gt; 4 The Plasminogen Activation System ~ &quot;\\n Differentiation of ESCs\\n ~ #&gt; 5 Human Neural Cells Transiently Exp~ &quot;\\nBiological samples\\nTwelve human embr~ #&gt; 6 Immunity Traits in Pigs: Substanti~ &quot;\\n Ethics Statement\\n Our~ #&gt; 7 Different Oxidative Stress Respons~ &quot;\\n Ethics Statement\\n N~ #&gt; 8 Immunomodulation Stimulates the In~ &quot;\\nAnimals and tissues\\nAll procedures w~ #&gt; 9 Genetics of wild and mass-reared p~ &quot;\\nBiological material\\nWe considered bo~ #&gt; 10 Neonatal Hyperglycemia Inhibits An~ &quot;\\nEthics\\nAll experimental protocols an~ searchplos(&quot;materials_and_methods:study site&quot;, fl = &quot;title, materials_and_methods&quot;) #&gt; $meta #&gt; # A tibble: 1 x 2 #&gt; numFound start #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 114695 0 #&gt; #&gt; $data #&gt; # A tibble: 10 x 2 #&gt; title materials_and_methods #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Whi5 Regulation by Site Specific C~ &quot;\\nYeast Culture and Strains\\nCells were~ #&gt; 2 Remote Source Document Verificatio~ &quot;Two NIH-sponsored clinical trial networ~ #&gt; 3 Variance Component Analysis of a M~ &quot;\\n1) The study\\nAddona et al. [6] condu~ #&gt; 4 Obtaining Valid Laboratory Data in~ &quot;The study was a randomised, double-blin~ #&gt; 5 Transposable Prophage Mu Is Organi~ &quot;\\nStrain construction\\nAll strains used~ #&gt; 6 Effects of land-use change and rel~ &quot;\\nBiodiversity data and species status\\~ #&gt; 7 Structural and functional dissecti~ &quot;\\nBacterial strains and culture conditi~ #&gt; 8 The effect of intramuscular inject~ &quot;A systematic review protocol was develo~ #&gt; 9 Retention in Care and Outpatient C~ &quot;\\nEthics Statement\\nThe Boston Universi~ #&gt; 10 Global migration of clinical resea~ &quot;\\nPrimary data source\\nOur primary data~ searchplos(&quot;*:*&quot;, fl = &quot;id&quot;) #&gt; $meta #&gt; # A tibble: 1 x 2 #&gt; numFound start #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2534152 0 #&gt; #&gt; $data #&gt; # A tibble: 10 x 1 #&gt; id #&gt; &lt;chr&gt; #&gt; 1 10.1371/journal.pbio.1000146/title #&gt; 2 10.1371/journal.pbio.1000146/abstract #&gt; 3 10.1371/journal.pbio.1000146/references #&gt; 4 10.1371/journal.pbio.1000146/body #&gt; 5 10.1371/journal.pbio.1000146/supporting_information #&gt; 6 10.1371/journal.pbio.1000146/conclusions #&gt; 7 10.1371/journal.pbio.1000147/title #&gt; 8 10.1371/journal.pbio.1000147/abstract #&gt; 9 10.1371/journal.pbio.1000147/references #&gt; 10 10.1371/journal.pbio.1000147/body Here is a list of options for the search or you can run data(plosfields) followed by plosfields in the R Console. 48.4.5.2 Take a highbrow look! The highplos() function does highlighted searches on PLOS Journals full-text content. highlighted &lt;- highplos(q=&#39;alcohol&#39;, hl.fl = &#39;abstract&#39;, rows=10) We can then pass this output to highbrow(), which will open up our default browser where we can browse the highlighted fragments. When we run highbrow(highlighted) in our R Console this is what we see in our browser: Figure 48.2: Example rplos highlights 48.4.5.3 Plots over time We can use the plot_throughtime() function to visualize the results of a search over time. plot_throughtime(terms = &quot;phylogeny&quot;, limit = 200) 48.4.6 Is it a boy or a girl? gender-associated names throughout US history The gender package allows you access to data on the gender of names in the US. Because names change gender over the years, the probability of a name belonging to a man or a woman also depends on the year. First, install/load the gender package from CRAN. You may be prompted to also install the companion package, genderdata. Go ahead and say yes. If you dont see this message no need to worry, it is a one-time install. # install.packages(&quot;gender&quot;) library(gender) #&gt; PLEASE NOTE: The method provided by this package must be used cautiously #&gt; and responsibly. Please be sure to see the guidelines and warnings about #&gt; usage in the README or the package documentation. Lets do some searches for the name Kelsey. gender(&quot;Kelsey&quot;) #&gt; # A tibble: 1 x 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Kelsey 0.0314 0.969 female 1932 2012 gender(&quot;Kelsey&quot;, years = 1940) #&gt; # A tibble: 1 x 6 #&gt; name proportion_male proportion_female gender year_min year_max #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Kelsey 1 0 male 1940 1940 "],["diy-web-data.html", "49 DIY web data 49.1 Interacting with an API 49.2 Intro to JSON and XML 49.3 Introducing the easy way: httr 49.4 Scraping 49.5 Scraping via CSS selectors 49.6 Random observations on scraping 49.7 Extras", " 49 DIY web data These notes are adapted from Jenny Bryans stat545 and were originally written by Andrew MacDonald. No OMDb key available. Code chunks will not be evaluated. 49.1 Interacting with an API In Chapter 48 we experimented with several packages that wrapped APIs. That is, they handled the creation of the request and the formatting of the output. In this chapter were going to look at (part of) what these functions were doing. 49.1.1 Load the tidyverse We will be using the functions from the tidyverse throughout this chapter, so go ahead and load tidyverse package now. library(tidyverse) 49.1.2 Examine the structure of API requests using the Open Movie Database First were going to examine the structure of API requests via the Open Movie Database (OMDb). OMDb is very similar to IMDb, except it has a nice, simple API. We can go to the website, input some search parameters, and obtain both the XML query and the response from it. Exercise: determine the shape of an API request. Scroll down to the Examples section on the OMDb site and play around with the parameters. Take a look at the resulting API call and the query you get back. If we enter the following parameters: title = Interstellar, year = 2014, plot = full, response = JSON Here is what we see: The request URL is: http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=full Notice the pattern in the request. Lets try changing the response field from JSON to XML. Now the request URL is: http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=full&amp;r=xml Try pasting these URLs into your browser. You should see this if you tried the first URL: {&quot;Response&quot;:&quot;False&quot;,&quot;Error&quot;:&quot;No API key provided.&quot;} and this if you tried the second URL (where r=xml): &lt;root response=&quot;False&quot;&gt; &lt;error&gt;No API key provided.&lt;/error&gt; &lt;/root&gt; 49.1.3 Create an OMDb API Key This tells us that we need an API key to access the OMDb API. We will store our key for the OMDb API in our .Renviron file using the helper function edit_r_environ() from the usethis package. Follow these steps: Visit this URL and request your free API key: https://www.omdbapi.com/apikey.aspx Check your email and follow the instructions to activate your key. Install/load the usethis package and run edit_r_environ() in the R Console: # install.packages(&quot;usethis&quot;) library(usethis) edit_r_environ() Add OMDB_API_KEY=&lt;your-secret-key&gt; on a new line, press enter to add a blank line at the end (important!), save the file, and close it. Note that we use &lt;your-secret-key&gt; as a placeholder here and throughout these instructions. Your actual API key will look something like: p319s0aa (no quotes or other characters like &lt; or &gt; should go on the right of the = sign). Restart R. You can now access your OMDb API key from the R console and save it as an object: Sys.getenv(&quot;OMDB_API_KEY&quot;) We can use this to easily add our API key to the request URL. Lets make this API key an object we can refer to as movie_key: # save it as an object movie_key &lt;- Sys.getenv(&quot;OMDB_API_KEY&quot;) 49.1.3.1 Alternative strategy for keeping keys: .Rprofile Remember to protect your key! It is important for your privacy. You know, like a key. Now we follow the rOpenSci tutorial on API keys: Add .Rprofile to your .gitignore !! Make a .Rprofile file (windows tips; mac tips). Write the following in it: options(OMBD_API_KEY = &quot;YOUR_KEY&quot;) Restart R (i.e. reopen your RStudio project). This code adds another element to the list of options, which you can see by calling options(). Part of the work done by rplos::searchplos() and friends is to go and obtain the value of this option with the function getOption(\"OMBD_API_KEY\"). This indicates two things: Spelling is important when you set the option in your .Rprofile You can do a similar process for an arbitrary package or key. For example: ## in .Rprofile options(&quot;this_is_my_key&quot; = XXXX) ## later, in the R script: key &lt;- getOption(&quot;this_is_my_key&quot;) This is a simple means to keep your keys private, especially if you are sharing the same authentication across several projects. 49.1.3.2 A few timely reminders about your .Rprofile print(&quot;This is Andrew&#39;s Rprofile and you can&#39;t have it!&quot;) options(OMBD_API_KEY = &quot;XXXXXXXXX&quot;) It must end with a blank line! It lives in the projects working directory, i.e. the location of your .Rproj. It must be gitignored. Remember that using .Rprofile makes your code un-reproducible. In this case, that is exactly what we want! 49.1.4 Recreate the request URL in R How can we recreate the same request URLs in R? We could use the glue package to paste together the base URL, parameter labels, and parameter values: request &lt;- glue::glue(&quot;http://www.omdbapi.com/?t=Interstellar&amp;y=2014&amp;plot=short&amp;r=xml&amp;apikey={movie_key}&quot;) request This works, but it only works for movie titled Interstellar from 2014 where we want the short plot and the XML format. Lets try to pull out more variables and paste them in with glue: glue::glue(&quot;http://www.omdbapi.com/?t={title}&amp;y={year}&amp;plot={plot}&amp;r={format}&amp;apikey={api_key}&quot;, title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key) We could go even further and make this into a function called omdb() that we can reuse more easily. omdb &lt;- function(title, year, plot, format, api_key) { glue::glue(&quot;http://www.omdbapi.com/?t={title}&amp;y={year}&amp;plot={plot}&amp;r={format}&amp;apikey={api_key}&quot;) } 49.1.5 Get data using the curl package Now we have a handy function that returns the API query. We can paste in the link, but we can also obtain data from within R using the curl package. Install/load the curl package first. # install.packages(&quot;curl&quot;) library(curl) Using curl to get the data in XML format: request_xml &lt;- omdb(title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key) con &lt;- curl(request_xml) answer_xml &lt;- readLines(con, warn = FALSE) close(con) answer_xml Using curl to get the data in JSON format: request_json &lt;- omdb(title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;json&quot;, api_key = movie_key) con &lt;- curl(request_json) answer_json &lt;- readLines(con, warn = FALSE) close(con) answer_json We have two forms of data that are obviously structured. What are they? 49.2 Intro to JSON and XML There are two common languages of web services: JavaScript Object Notation (JSON) eXtensible Markup Language (XML) Heres an example of JSON (from this wonderful site): { &quot;crust&quot;: &quot;original&quot;, &quot;toppings&quot;: [&quot;cheese&quot;, &quot;pepperoni&quot;, &quot;garlic&quot;], &quot;status&quot;: &quot;cooking&quot;, &quot;customer&quot;: { &quot;name&quot;: &quot;Brian&quot;, &quot;phone&quot;: &quot;573-111-1111&quot; } } And here is XML (also from this site): &lt;order&gt; &lt;crust&gt;original&lt;/crust&gt; &lt;toppings&gt; &lt;topping&gt;cheese&lt;/topping&gt; &lt;topping&gt;pepperoni&lt;/topping&gt; &lt;topping&gt;garlic&lt;/topping&gt; &lt;/toppings&gt; &lt;status&gt;cooking&lt;/status&gt; &lt;/order&gt; You can see that both of these data structures are quite easy to read. They are self-describing. In other words, they tell you how they are meant to be read. There are easy means of taking these data types and creating R objects. 49.2.1 Parsing the JSON response with jsonlite Our JSON response above can be parsed using jsonlite::fromJSON(). First install/load the jsonlite package. # install.packages(&quot;jsonlite&quot;) library(jsonlite) Parsing our JSON response with fromJSON(): answer_json %&gt;% fromJSON() The output is a named list. A familiar and friendly R structure. Because data frames are lists and because this list has no nested lists-within-lists, we can coerce it very simply: answer_json %&gt;% fromJSON() %&gt;% as_tibble() %&gt;% glimpse() 49.2.2 Parsing the XML response using xml2 We can use the xml2 package to wrangle our XML response. # install.packages(&quot;xml2&quot;) library(xml2) Parsing our XML response with read_xml(): (xml_parsed &lt;- read_xml(answer_xml)) Not exactly the result we were hoping for! However, this does tell us about the XML documents structure: It has a &lt;root&gt; node, which has a single child node, &lt;movie&gt;. The information we want is all stored as attributes (e.g. title, year, etc.). The xml2 package has various functions to assist in navigating through XML. We can use the xml_children() function to extract all of the children nodes (i.e. the single child, &lt;movie&gt;): (contents &lt;- xml_contents(xml_parsed)) The xml_attrs() function retrieves all attribute values as a named character vector. Lets use this to extract the information that we want from the &lt;movie&gt; node: (attrs &lt;- xml_attrs(contents)[[1]]) We can transform this named character vector into a data frame with the help of dplyr::bind_rows(): attrs %&gt;% bind_rows() %&gt;% glimpse() 49.3 Introducing the easy way: httr httr is yet another star in the tidyverse. It is a package designed to facilitate all things HTTP from within R. This includes the major HTTP verbs, which are: GET() - Fetch an existing resource. The URL contains all the necessary information the server needs to locate and return the resource. POST() - Create a new resource. POST requests usually carry a payload that specifies the data for the new resource. PUT() - Update an existing resource. The payload may contain the updated data for the resource. DELETE() - Delete an existing resource. HTTP is the foundation for APIs; understanding how it works is the key to interacting with all the diverse APIs out there. An excellent beginning resource for APIs (including HTTP basics) is An Introduction to APIs by Brian Cooksey. httr also facilitates a variety of authentication protocols. httr contains one function for every HTTP verb. The functions have the same names as the verbs (e.g. GET(), POST()). They have more informative outputs than simply using curl and come with nice convenience functions for working with the output: # install.packages(&quot;httr&quot;) library(httr) Using httr to get the data in JSON format: request_json &lt;- omdb(title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;json&quot;, api_key = movie_key) response_json &lt;- GET(request_json) content(response_json, as = &quot;parsed&quot;, type = &quot;application/json&quot;) Using httr to get the data in XML format: request_xml &lt;- omdb(title = &quot;Interstellar&quot;, year = &quot;2014&quot;, plot = &quot;short&quot;, format = &quot;xml&quot;, api_key = movie_key) response_xml &lt;- GET(request_xml) content(response_xml, as = &quot;parsed&quot;) httr also gives us access to lots of useful information about the quality of our response. For example, the header: headers(response_xml) And also a handy means to extract specifically the HTTP status code: status_code(response_xml) In fact, we didnt need to create omdb() at all. httr provides a straightforward means of making an HTTP request with the query argument: the_martian &lt;- GET(&quot;http://www.omdbapi.com/?&quot;, query = list(t = &quot;The Martian&quot;, y = 2015, plot = &quot;short&quot;, r = &quot;json&quot;, apikey = movie_key)) content(the_martian) With httr, we are able to pass in the named arguments to the API call as a named list. We are also able to use spaces in movie names; httr encodes these in the URL before making the GET request. It is very good to learn your HTTP status codes. The documentation for httr includes a vignette of Best practices for writing an API package, which is useful for when you want to bring your favourite web resource into the world of R. 49.4 Scraping What if data is present on a website, but isnt provided in an API at all? It is possible to grab that information too. How easy that is to do depends a lot on the quality of the website that we are using. HTML is a structured way of displaying information. It is very similar in structure to XML (in fact many modern html sites are actually XHTML5, which is also valid XML). Two pieces of equipment: The rvest package (CRAN; GitHub). Install via install.packages(\"rvest)\". SelectorGadget: point and click CSS selectors. Install in your browser. Before we go any further, lets play a game together! 49.4.1 Obtain a table Lets make a simple HTML table and then parse it. Make a new, empty project Make a totally empty .Rmd file and save it as \"GapminderHead.Rmd\" Copy this into the body: --- output: html_document --- ```{r echo=FALSE, results=&#39;asis&#39;} library(gapminder) knitr::kable(head(gapminder)) ``` Knit the document and click View in Browser. It should look like this: We have created a simple HTML table with the head of gapminder in it! We can get our data back by parsing this table into a data frame again. Extracting data from HTML is called scraping, and we can do it in R with the rvest package: # install.packages(&quot;rvest&quot;) library(rvest) read_html(&quot;GapminderHead.html&quot;) %&gt;% html_table() 49.5 Scraping via CSS selectors Lets practice scraping websites using our newfound abilities. Here is a table of research publications by country. We can try to get this data directly into R using read_html() and html_table(): research &lt;- read_html(&quot;https://www.scimagojr.com/countryrank.php&quot;) %&gt;% html_table(fill = TRUE) If you look at the structure of research (i.e. via str(research)) youll see that weve obtained a list of data.frames. The top of the page contains another table element. This was also scraped! Can we be more specific about what we obtain from this page? We can, by highlighting that table with CSS selectors: research &lt;- read_html(&quot;http://www.scimagojr.com/countryrank.php&quot;) %&gt;% html_node(&quot;.tabla_datos&quot;) %&gt;% html_table() glimpse(research) 49.6 Random observations on scraping Make sure youve obtained ONLY what you want! Scroll over the whole page to ensure that SelectorGadget hasnt found too many things. If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking (e.g. being more precise). MOST IMPORTANTLY confirm that there is NO rOpenSci package and NO API before you spend hours scraping (the API was right here). 49.7 Extras 49.7.1 Airports First, go to this website about Airports. Follow the link to get your API key (you will need to click a confirmation email). List of all the airports on the planet: https://airport.api.aero/airport/?user_key={yourkey} List of all the airports matching Toronto: https://airport.api.aero/airport/match/toronto?user_key={yourkey} The distance between YVR and LAX: https://airport.api.aero/airport/distance/YVR/LAX?user_key={yourkey} Do you need just the US airports? This API does that (also see this) and is free. "],["lab07.html", "50 Lab: University of Edinburgh Art Collection 50.1 Getting started 50.2 R scripts vs. R Markdown documents 50.3 SelectorGadget 50.4 Functions 50.5 Iteration 50.6 Analysis", " 50 Lab: University of Edinburgh Art Collection The University of Edinburgh Art Collection supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.2. Note: See the sidebar here and note that there are 2909 pieces in the art collection were collecting data on. In this lab, well scrape data on all art pieces in the Edinburgh College of Art collection. The learning goals of this lab are: Web scraping from a single page Writing functions Iteration Writing data Before getting started, lets check that a bot has permissions to access pages on this domain. paths_allowed(&quot;https://collections.ed.ac.uk/art)&quot;) ## collections.ed.ac.uk ## [1] TRUE 50.1 Getting started Go to the course GitHub organization and locate your lab repo, which should be named lab-07-uoe-art. Grab the URL of the repo, and fork your own copy. Remember: Your email address is the address tied to your GitHub account. Run the following (but update it for your name and email!) in the Console to configure Git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 50.2 R scripts vs. R Markdown documents Today we will be using both R scripts and R Markdown documents: .R: R scripts are plain text files containing only code and brief comments, Well use R scripts in the web scraping stage and ultimately save the scraped data as a csv. .Rmd: R Markdown documents are plain text files containing. Well use an R Markdown document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage. Here is the organization of your repo, and the corresponding section in the lab that each file will be used for: |-data | |- README.md |-lab-07-uoe-art.Rmd # analysis |-lab-07-uoe-art.Rproj # project management |-README.md |-scripts # webscraping | |- 01-scrape-page-one.R # scraping a single page | |- 02-scrape-page-function.R # functions | |- 03-scrape-page-many.R # iteration 50.3 SelectorGadget For this lab, I recommend using Google Chrome as your web browser. In case you havent installed the SelectorGadget extension go to the SelectorGadget extension page on the Chrome Web Store and click on Add to Chrome (big blue button). A pop up window will ask Add SelectorGadget?, click Add extension. Another pop up window will ask whether you want to get your extensions on all your computer. If you want this, you can turn on sync, but you dont need to for the purpose of this lab. You should now be able to access SelectorGadget by clicking on the icon next to the search bar in the Chrome browser. 50.3.1 Scraping a single page **Tip:** To run the code you can highlight or put your cursor next to the lines of code you want to run and hit Command+Enter. Work in scripts/01-scrape-page-one.R. We will start off by scraping data on the first 10 pieces in the collection from here. First, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R. # set url first_url &lt;- &quot;https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0&quot; # read html page page &lt;- read_html(first_url) For the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame. 50.3.2 Titles Lets start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes: page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) ## {xml_nodeset (10)} ## [1] &lt;a href=&quot;./record/54751?highlight=*:*&quot;&gt;Orange and Lemon Playing Games I ... ## [2] &lt;a href=&quot;./record/111443?highlight=*:*&quot;&gt;Abstract ... ## [3] &lt;a href=&quot;./record/47453?highlight=*:*&quot;&gt;Distortion 2 ... ## [4] &lt;a href=&quot;./record/47452?highlight=*:*&quot;&gt;Distortion 1 ... ## [5] &lt;a href=&quot;./record/111442?highlight=*:*&quot;&gt;Emerging ... ## [6] &lt;a href=&quot;./record/111441?highlight=*:*&quot;&gt;Emerging ... ## [7] &lt;a href=&quot;./record/111446?highlight=*:*&quot;&gt;Landscape with Resting Harvester ... ## [8] &lt;a href=&quot;./record/111445?highlight=*:*&quot;&gt;Abstract ... ## [9] &lt;a href=&quot;./record/20769?highlight=*:*&quot;&gt;Three studies on a pink ground (g ... ## [10] &lt;a href=&quot;./record/113744?highlight=*:*&quot;&gt;Cotton Wool Brain ... Then we extract the text with html_text(): page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() ## [1] &quot;Orange and Lemon Playing Games I (1999)&quot; ## [2] &quot;Abstract &quot; ## [3] &quot;Distortion 2 (2014)&quot; ## [4] &quot;Distortion 1 (2014)&quot; ## [5] &quot;Emerging (1987)&quot; ## [6] &quot;Emerging (1987)&quot; ## [7] &quot;Landscape with Resting Harvesters (1953)&quot; ## [8] &quot;Abstract &quot; ## [9] &quot;Three studies on a pink ground (girl with clasped hands) (1949)&quot; ## [10] &quot;Cotton Wool Brain (2019)&quot; And get rid of all the spurious whitespace in the text with str_squish(): Take a look at the help docs for `str_squish()` (with `?str_squish`) to page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() ## [1] &quot;Orange and Lemon Playing Games I (1999)&quot; ## [2] &quot;Abstract&quot; ## [3] &quot;Distortion 2 (2014)&quot; ## [4] &quot;Distortion 1 (2014)&quot; ## [5] &quot;Emerging (1987)&quot; ## [6] &quot;Emerging (1987)&quot; ## [7] &quot;Landscape with Resting Harvesters (1953)&quot; ## [8] &quot;Abstract&quot; ## [9] &quot;Three studies on a pink ground (girl with clasped hands) (1949)&quot; ## [10] &quot;Cotton Wool Brain (2019)&quot; And finally save the resulting data as a vector of length 10: titles &lt;- page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() 50.3.3 Links The same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes. A mini HTML lesson! The following is how we define hyperlinked text in HTML: &lt;a href=&quot;https://www.google.com&quot;&gt;Seach on Google&lt;/a&gt; And this is how the text would look like on a webpage: Seach on Google. Here the text is Seach on Google and the href attribute contains the url of the website youd go to if you click on the hyperlinked text: https://www.google.com. The moral of the story is: the link is stored in the href attribute. page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% # same nodes html_node(&quot;h3 a&quot;) %&gt;% # as before html_attr(&quot;href&quot;) # but get href attribute instead of text ## [1] &quot;./record/54751?highlight=*:*&quot; &quot;./record/111443?highlight=*:*&quot; ## [3] &quot;./record/47453?highlight=*:*&quot; &quot;./record/47452?highlight=*:*&quot; ## [5] &quot;./record/111442?highlight=*:*&quot; &quot;./record/111441?highlight=*:*&quot; ## [7] &quot;./record/111446?highlight=*:*&quot; &quot;./record/111445?highlight=*:*&quot; ## [9] &quot;./record/20769?highlight=*:*&quot; &quot;./record/113744?highlight=*:*&quot; These dont really look like urls as we know then though. Theyre relative links. Note: See the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments. Click on one of art piece titles in your browser and take note of the url of the webpage it takes you to. How does that url compare to what we scraped above? How is it different? Using str_replace(), fix the URLs. 50.3.4 Artists Fill in the blanks to scrape artist names. 50.3.5 Put it altogether Fill in the blanks to organize everything in a tibble. 50.3.6 Scrape the next page Click on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten. 50.4 Functions Mason Comment: I know that I havent taught you explicitly how to write functions yet. But, you have learned enough from me (and wise Prof. Google) to teach yourself how to do it. Dont worry  Ill be covering how to write functions explicitly in the next module. (Try it on your own first, and then take a sneek peak ahead if you want some reassurance that youre doing it right. I believe in you! Work in scripts/02-scrape-page-function.R. Youve been using R functions, now its time to write your own! Lets start simple. Here is a function that takes in an argument x, and adds 2 to it. add_two &lt;- function(x){ x + 2 } Lets test it: add_two(3) ## [1] 5 add_two(10) ## [1] 12 The skeleton for defining functions in R is as follows: function_name &lt;- function(input){ # do something with the input(s) # return something } Then, a function for scraping a page should look something like: Tip: Function names should be short but evocative verbs. function_name &lt;- function(url){ # read page at url # extract title, link, artist info for n pieces on page # return a n x 3 tibble } Fill in the blanks using code you already developed in the previous exercises. Name the function scrape_page. Test out your new function by running the following in the console. Does the output look right? scrape_page(first_url) scrape_page(second_url) 50.5 Iteration Work in scripts/03-scrape-page-many.R. We went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection. Reminder: The collection has 2909 pieces in total. That means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 2909 rows and 3 columns. 50.5.1 List of URLs Click through the first few of the pages in the art collection and observe their URLs to confirm the following pattern: [sometext]offset=0 # Pieces 1-10 [sometext]offset=10 # Pieces 11-20 [sometext]offset=20 # Pieces 21-30 [sometext]offset=30 # Pieces 31-40 ... [sometext]offset=2900 # Pieces 2900-2909 We can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 2900. Two new functions are helpful for accomplishing this: paste0() for pasting two pieces of text and seq() for generating a sequence of numbers. Fill in the blanks to construct the list of URLs. 50.5.2 Mapping Finally, were ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R (which well learn about in more detail tomorrow), and they each take the following form: map([x], [function to apply to each element of x]) In our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function: map_dfr(urls, scrape_page) Fill in the blanks to scrape all pages, and to create a new data frame called uoe_art. 50.5.3 Write out data Finally write out the data frame you constructed into the data folder so that you can use it in the analysis section. 50.6 Analysis Work in lab-07-uoe-art.Rmd for the rest of the lab. Now that we have a tidy dataset that we can analyze, lets do that! Well start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up! First thing well try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to separate the title column at the first occurence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date Luckily, theres a function that does just this: separate()! And once we have completed separating the single title column into title and date, we need to do further cleanup in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable. Hint: Remember escaping special characters from lecture? Youll need to use that trick again. Fill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and thats OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where its convenient to do so. Print out a summary of the dataframe using the skim() function. How many pieces have artist info missing? How many have year info missing? Make a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary? Hint: Youll want to use mutate() and if_else() or case_when() to implement the correction. Find which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didnt capture the correct year information? Correct the error in the data frame and visualize the data again. Who is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them? Hint: Youll want to use a combination of filter() and str_detect(). You will want to read the help for str_detect() at a minimum, and consider how you might capture titles where the word appears as child and Child. Final question! How many art pieces have the word child in their title? See if you can figure it out, and ask for help if not. Source: https://collections.ed.ac.uk/art/about "],["welcome-to-functions-and-automation.html", "51 Welcome to Functions and Automation 51.1 Module Materials", " 51 Welcome to Functions and Automation This module is designed to introduce functions. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 51.1 Module Materials Slides from Lectures Functions and Automation Activities Functions and Art! Suggested Readings All subchapters of this module, including [Notes on Functions][#functions-part1] r4ds Sections on functions, and Iterations Lab [Conveying the right message through visualization][lab08] [Optional Lab Money in Politics][lab08b] "],["functions-1.html", "52 Functions! 52.1 Code Along pt 1 52.2 Functions for real! 52.3 Code Along pt 2 52.4 Writing Functions", " 52 Functions! Wow Im so humbly grateful much love to yall&mdash; Missy Elliott (@MissyElliott) April 26, 2021 You can follow along with the slides here if they do not appear below. 52.1 Code Along pt 1 You can find the materials for this activity here. 52.2 Functions for real! 52.3 Code Along pt 2 You can find the materials for this activity here. 52.4 Writing Functions "],["automation.html", "53 Automation 53.1 Code Along pt 3", " 53 Automation You can follow along with the slides here if they do not appear below. 53.1 Code Along pt 3 You can find the materials for this activity here. "],["functions-part1.html", "54 Write your own R functions 54.1 What and why? 54.2 Load the Gapminder data 54.3 Max - min 54.4 Get something that works 54.5 Turn the working interactive code into a function 54.6 Test your function 54.7 Check the validity of arguments 54.8 Wrap-up and whats next? 54.9 Where were we? Where are we going? 54.10 Load the Gapminder data 54.11 Restore our max minus min function 54.12 Generalize our function to other quantiles 54.13 Get something that works, again 54.14 Turn the working interactive code into a function, again 54.15 Argument names: freedom and conventions 54.16 What a function returns 54.17 Default values: freedom to NOT specify the arguments 54.18 Check the validity of arguments, again 54.19 Wrap-up and whats next? 54.20 Where were we? Where are we going? 54.21 Load the Gapminder data 54.22 Restore our max minus min function 54.23 Be proactive about NAs 54.24 The useful but mysterious ... argument 54.25 Use testthat for formal unit tests", " 54 Write your own R functions These notes are adapted from Jenny Bryans stat545. 54.1 What and why? My goal here is to reveal the process a long-time useR employs for writing functions. I also want to illustrate why the process is the way it is. Merely looking at the finished product, e.g. source code for R packages, can be extremely deceiving. Reality is generally much uglier  but more interesting! Why are we covering this now, smack in the middle of data aggregation? Powerful machines like dplyr, purrr, and the built-in apply family of functions, are ready and waiting to apply your purpose-built functions to various bits of your data. If you can express your analytical wishes in a function, these tools will give you great power. 54.2 Load the Gapminder data As usual, load gapminder. library(gapminder) str(gapminder) #&gt; tibble[,6] [1,704 x 6] (S3: tbl_df/tbl/data.frame) #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372.. #&gt; $ gdpPercap: num [1:1704] 779 821 853 836 740 ... 54.3 Max - min Say youve got a numeric vector, and you want to compute the difference between its max and min. lifeExp or pop or gdpPercap are great examples of a typical input. You can imagine wanting to get this statistic after we slice up the Gapminder data by year, country, continent, or combinations thereof. 54.4 Get something that works First, develop some working code for interactive use, using a representative input. Ill use Gapminders life expectancy variable. R functions that will be useful: min(), max(), range(). (Read their documentation: here and here) ## get to know the functions mentioned above min(gapminder$lifeExp) #&gt; [1] 23.6 max(gapminder$lifeExp) #&gt; [1] 82.6 range(gapminder$lifeExp) #&gt; [1] 23.6 82.6 ## some natural solutions max(gapminder$lifeExp) - min(gapminder$lifeExp) #&gt; [1] 59 with(gapminder, max(lifeExp) - min(lifeExp)) #&gt; [1] 59 range(gapminder$lifeExp)[2] - range(gapminder$lifeExp)[1] #&gt; [1] 59 with(gapminder, range(lifeExp)[2] - range(lifeExp)[1]) #&gt; [1] 59 diff(range(gapminder$lifeExp)) #&gt; [1] 59 Internalize this answer because our informal testing relies on you noticing departures from this. 54.4.1 Skateboard &gt;&gt; perfectly formed rear-view mirror This image widely attributed to the Spotify development team conveys an important point. Figure 54.1: From Your ultimate guide to Minimum Viable Product (+great examples) Build that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps the spirits high. This is related to the valuable Telescope Rule: It is faster to make a four-inch mirror then a six-inch mirror than to make a six-inch mirror. 54.5 Turn the working interactive code into a function Add NO new functionality! Just write your very first R function. max_minus_min &lt;- function(x) max(x) - min(x) max_minus_min(gapminder$lifeExp) #&gt; [1] 59 Check that youre getting the same answer as you did with your interactive code. Test it eyeball-o-metrically at this point. 54.6 Test your function 54.6.1 Test on new inputs Pick some new artificial inputs where you know (at least approximately) what your function should return. max_minus_min(1:10) #&gt; [1] 9 max_minus_min(runif(1000)) #&gt; [1] 0.997 I know that 10 minus 1 is 9. I know that random uniform [0, 1] variates will be between 0 and 1. Therefore max - min should be less than 1. If I take LOTS of them, max - min should be pretty close to 1. It is intentional that I tested on integer input as well as floating point. Likewise, I like to use valid-but-random data for this sort of check. 54.6.2 Test on real data but different real data Back to the real world now. Two other quantitative variables are lying around: gdpPercap and pop. Lets have a go. max_minus_min(gapminder$gdpPercap) #&gt; [1] 113282 max_minus_min(gapminder$pop) #&gt; [1] 1318623085 Either check these results by hand or apply the does that even make sense? test. 54.6.3 Test on weird stuff Now we try to break our function. Dont get truly diabolical (yet). Just make the kind of mistakes you can imagine making at 2am when, 3 years from now, you rediscover this useful function you wrote. Give your function inputs its not expecting. max_minus_min(gapminder) ## hey sometimes things &quot;just work&quot; on data.frames! #&gt; Error in FUN(X[[i]], ...): only defined on a data frame with all numeric variables max_minus_min(gapminder$country) ## factors are kind of like integer vectors, no? #&gt; Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;max&#39; not meaningful for factors max_minus_min(&quot;eggplants are purple&quot;) ## i have no excuse for this one #&gt; Error in max(x) - min(x): non-numeric argument to binary operator How happy are you with those error messages? You must imagine that some entire script has failed and that you were hoping to just source() it without re-reading it. If a colleague or future you encountered these errors, do you run screaming from the room? How hard is it to pinpoint the usage problem? 54.6.4 I will scare you now Here are some great examples where the function should break but it does not. max_minus_min(gapminder[c(&#39;lifeExp&#39;, &#39;gdpPercap&#39;, &#39;pop&#39;)]) #&gt; [1] 1.32e+09 max_minus_min(c(TRUE, TRUE, FALSE, TRUE, TRUE)) #&gt; [1] 1 In both cases, Rs eagerness to make sense of our requests is unfortunately successful. In the first case, a data.frame containing just the quantitative variables is eventually coerced into numeric vector. We can compute max minus min, even though it makes absolutely no sense at all. In the second case, a logical vector is converted to zeroes and ones, which might merit an error or at least a warning. 54.7 Check the validity of arguments For functions that will be used again  which is not all of them!  it is good to check the validity of arguments. This implements a rule from the Unix philosophy: Rule of Repair: When you must fail, fail noisily and as soon as possible. 54.7.1 stop if not stopifnot() is the entry level solution. I use it here to make sure the input x is a numeric vector. mmm &lt;- function(x) { stopifnot(is.numeric(x)) max(x) - min(x) } mmm(gapminder) #&gt; Error in mmm(gapminder): is.numeric(x) is not TRUE mmm(gapminder$country) #&gt; Error in mmm(gapminder$country): is.numeric(x) is not TRUE mmm(&quot;eggplants are purple&quot;) #&gt; Error in mmm(&quot;eggplants are purple&quot;): is.numeric(x) is not TRUE mmm(gapminder[c(&#39;lifeExp&#39;, &#39;gdpPercap&#39;, &#39;pop&#39;)]) #&gt; Error in mmm(gapminder[c(&quot;lifeExp&quot;, &quot;gdpPercap&quot;, &quot;pop&quot;)]): is.numeric(x) is not TRUE mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE)) #&gt; Error in mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE)): is.numeric(x) is not TRUE And we see that it catches all of the self-inflicted damage we would like to avoid. 54.7.2 if then stop stopifnot() doesnt provide a very good error message. The next approach is very widely used. Put your validity check inside an if() statement and call stop() yourself, with a custom error message, in the body. mmm2 &lt;- function(x) { if(!is.numeric(x)) { stop(&#39;I am so sorry, but this function only works for numeric input!\\n&#39;, &#39;You have provided an object of class: &#39;, class(x)[1]) } max(x) - min(x) } mmm2(gapminder) #&gt; Error in mmm2(gapminder): I am so sorry, but this function only works for numeric input! #&gt; You have provided an object of class: tbl_df In addition to a gratuitous apology, the error raised also contains two more pieces of helpful info: Which function threw the error. Hints on how to fix things: expected class of input vs actual class. If it is easy to do so, I highly recommend this template: you gave me THIS, but I need THAT. The tidyverse style guide has a very useful chapter on how to construct error messages. 54.7.3 Sidebar: non-programming uses for assertions Another good use of this pattern is to leave checks behind in data analytical scripts. Consider our repetitive use of Gapminder in this course. Every time we load it, we inspect it, hoping to see the usual stuff. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run ad nauseam on auto-pilot or non-interactively. 54.8 Wrap-up and whats next? Heres the function weve written so far: mmm2 #&gt; function(x) { #&gt; if(!is.numeric(x)) { #&gt; stop(&#39;I am so sorry, but this function only works for numeric input!\\n&#39;, #&gt; &#39;You have provided an object of class: &#39;, class(x)[1]) #&gt; } #&gt; max(x) - min(x) #&gt; } What weve accomplished: Weve written our first function. We are checking the validity of its input, argument x. Weve done a good amount of informal testing. Where to next? In part 2 we generalize this function to take differences in other quantiles and learn how to set default values for arguments. 54.9 Where were we? Where are we going? In part 1 we wrote our first R function to compute the difference between the max and min of a numeric vector. We checked the validity of the functions only argument and, informally, we verified that it worked pretty well. In this part, we generalize this function, learn more technical details about R functions, and set default values for some arguments. 54.10 Load the Gapminder data As usual, load gapminder. library(gapminder) 54.11 Restore our max minus min function Lets keep our previous function around as a baseline. mmm &lt;- function(x) { stopifnot(is.numeric(x)) max(x) - min(x) } 54.12 Generalize our function to other quantiles The max and the min are special cases of a quantile. Here are other special cases you may have heard of: median = 0.5 quantile 1st quartile = 0.25 quantile 3rd quartile = 0.75 quantile If youre familiar with box plots, the rectangle typically runs from the 1st quartile to the 3rd quartile, with a line at the median. If \\(q\\) is the \\(p\\)-th quantile of a set of \\(n\\) observations, what does that mean? Approximately \\(pn\\) of the observations are less than \\(q\\) and \\((1 - p)n\\) are greater than \\(q\\). Yeah, you need to worry about rounding to an integer and less/greater than or equal to, but these details arent critical here. Lets generalize our function to take the difference between any two quantiles. We can still consider the max and min, if we like, but were not limited to that. 54.13 Get something that works, again The eventual inputs to our new function will be the data x and two probabilities. First, play around with the quantile() function. Convince yourself you know how to use it, for example, by cross-checking your results with other built-in functions. quantile(gapminder$lifeExp) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.7 70.8 82.6 quantile(gapminder$lifeExp, probs = 0.5) #&gt; 50% #&gt; 60.7 median(gapminder$lifeExp) #&gt; [1] 60.7 quantile(gapminder$lifeExp, probs = c(0.25, 0.75)) #&gt; 25% 75% #&gt; 48.2 70.8 boxplot(gapminder$lifeExp, plot = FALSE)$stats #&gt; [,1] #&gt; [1,] 23.6 #&gt; [2,] 48.2 #&gt; [3,] 60.7 #&gt; [4,] 70.8 #&gt; [5,] 82.6 Now write a code snippet that takes the difference between two quantiles. the_probs &lt;- c(0.25, 0.75) the_quantiles &lt;- quantile(gapminder$lifeExp, probs = the_probs) max(the_quantiles) - min(the_quantiles) #&gt; [1] 22.6 54.14 Turn the working interactive code into a function, again Ill use qdiff as the base of our functions name. I copy the overall structure from our previous max minus min work but replace the guts of the function with the more general code we just developed. qdiff1 &lt;- function(x, probs) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } qdiff1(gapminder$lifeExp, probs = c(0.25, 0.75)) #&gt; [1] 22.6 IQR(gapminder$lifeExp) # hey, we&#39;ve reinvented IQR #&gt; [1] 22.6 qdiff1(gapminder$lifeExp, probs = c(0, 1)) #&gt; [1] 59 mmm(gapminder$lifeExp) #&gt; [1] 59 Again we do some informal tests against familiar results and external implementations. 54.15 Argument names: freedom and conventions I want you to understand the importance of argument names. I can name my arguments almost anything I like. Proof: qdiff2 &lt;- function(zeus, hera) { stopifnot(is.numeric(zeus)) the_quantiles &lt;- quantile(x = zeus, probs = hera) max(the_quantiles) - min(the_quantiles) } qdiff2(zeus = gapminder$lifeExp, hera = 0:1) #&gt; [1] 59 While I can name my arguments after Greek gods, its usually a bad idea. Take all opportunities to make things more self-explanatory via meaningful names. If you are going to pass the arguments of your function as arguments of a built-in function, consider copying the argument names. Unless you have a good reason to do your own thing (some argument names are bad!), be consistent with the existing function. Again, the reason is to reduce your cognitive load. This is what Ive been doing all along and now you know why: qdiff1 #&gt; function(x, probs) { #&gt; stopifnot(is.numeric(x)) #&gt; the_quantiles &lt;- quantile(x = x, probs = probs) #&gt; max(the_quantiles) - min(the_quantiles) #&gt; } #&gt; &lt;bytecode: 0x0000000018c5be28&gt; We took this detour so you could see there is no structural relationship between my arguments (x and probs) and those of quantile() (also x and probs). The similarity or equivalence of the names accomplishes nothing as far as R is concerned; it is solely for the benefit of humans reading, writing, and using the code. Which is very important! 54.16 What a function returns By this point, I expect someone will have asked about the last line in my functions body. Look above for a reminder of the functions definition. By default, a function returns the result of the last line of the body. I am just letting that happen with the line max(the_quantiles) - min(the_quantiles). However, there is an explicit function for this: return(). I could just as easily make this the last line of my functions body: return(max(the_quantiles) - min(the_quantiles)) You absolutely must use return() if you want to return early based on some condition, i.e. before execution gets to the last line of the body. Otherwise, you can decide your own conventions about when you use return() and when you dont. 54.17 Default values: freedom to NOT specify the arguments What happens if we call our function but neglect to specify the probabilities? qdiff1(gapminder$lifeExp) #&gt; Error in quantile(x = x, probs = probs): argument &quot;probs&quot; is missing, with no default Oops! At the moment, this causes a fatal error. It can be nice to provide some reasonable default values for certain arguments. In our case, it would be crazy to specify a default value for the primary input x, but very kind to specify a default for probs. We started by focusing on the max and the min, so I think those make reasonable defaults. Heres how to specify that in a function definition. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } Again we check how the function works, in old examples and new, specifying the probs argument and not. qdiff3(gapminder$lifeExp) #&gt; [1] 59 mmm(gapminder$lifeExp) #&gt; [1] 59 qdiff3(gapminder$lifeExp, c(0.1, 0.9)) #&gt; [1] 33.6 54.18 Check the validity of arguments, again Exercise: upgrade our argument validity checks in light of the new argument probs. ## problems identified during class ## we&#39;re not checking that probs is numeric ## we&#39;re not checking that probs is length 2 ## we&#39;re not checking that probs are in [0,1] 54.19 Wrap-up and whats next? Heres the function weve written so far: qdiff3 #&gt; function(x, probs = c(0, 1)) { #&gt; stopifnot(is.numeric(x)) #&gt; the_quantiles &lt;- quantile(x, probs) #&gt; max(the_quantiles) - min(the_quantiles) #&gt; } #&gt; &lt;bytecode: 0x00000000178638b8&gt; What weve accomplished: Weve generalized our first function to take a difference between arbitrary quantiles. Weve specified default values for the probabilities that set the quantiles. Where to next? Next, we tackle NAs, the special ... argument, and formal unit testing. 54.20 Where were we? Where are we going? Previously, we generalized our first R function so it could take the difference between any two quantiles of a numeric vector. We also set default values for the underlying probabilities, so that, by default, we compute the max minus the min. In this part, we tackle NAs, the special argument ... and formal testing. 54.21 Load the Gapminder data As usual, load gapminder. library(gapminder) 54.22 Restore our max minus min function Lets keep our previous function around as a baseline. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } 54.23 Be proactive about NAs I am being gentle by letting you practice with the Gapminder data. In real life, missing data will make your life a living hell. If you are lucky, it will be properly indicated by the special value NA, but dont hold your breath. Many built-in R functions have an na.rm = argument through which you can specify how you want to handle NAs. Typically the default value is na.rm = FALSE and typical default behavior is to either let NAs propagate or to raise an error. Lets see how quantile() handles NAs: z &lt;- gapminder$lifeExp z[3] &lt;- NA quantile(gapminder$lifeExp) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.7 70.8 82.6 quantile(z) #&gt; Error in quantile.default(z): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE quantile(z, na.rm = TRUE) #&gt; 0% 25% 50% 75% 100% #&gt; 23.6 48.2 60.8 70.8 82.6 So quantile() simply will not operate in the presence of NAs unless na.rm = TRUE. How shall we modify our function? If we wanted to hardwire na.rm = TRUE, we could. Focus on our call to quantile() inside our function definition. qdiff4 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = TRUE) max(the_quantiles) - min(the_quantiles) } qdiff4(gapminder$lifeExp) #&gt; [1] 59 qdiff4(z) #&gt; [1] 59 This works but it is dangerous to invert the default behavior of a well-known built-in function and to provide the user with no way to override this. We could add an na.rm = argument to our own function. We might even enforce our preferred default  but at least were giving the user a way to control the behavior around NAs. qdiff5 &lt;- function(x, probs = c(0, 1), na.rm = TRUE) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = na.rm) max(the_quantiles) - min(the_quantiles) } qdiff5(gapminder$lifeExp) #&gt; [1] 59 qdiff5(z) #&gt; [1] 59 qdiff5(z, na.rm = FALSE) #&gt; Error in quantile.default(x, probs, na.rm = na.rm): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE 54.24 The useful but mysterious ... argument You probably could have lived a long and happy life without knowing there are at least 9 different algorithms for computing quantiles. Go read about the type argument of quantile(). TLDR: If a quantile is not unambiguously equal to an observed data point, you must somehow average two data points. You can weight this average different ways, depending on the rest of the data, and type = controls this. Lets say we want to give the user of our function the ability to specify how the quantiles are computed, but we want to accomplish with as little fuss as possible. In fact, we dont even want to clutter our functions interface with this! This calls for the very special ... argument. In English, this set of three dots is frequently called an ellipsis. qdiff6 &lt;- function(x, probs = c(0, 1), na.rm = TRUE, ...) { the_quantiles &lt;- quantile(x = x, probs = probs, na.rm = na.rm, ...) max(the_quantiles) - min(the_quantiles) } The practical significance of the type = argument is virtually nonexistent, so we cant demo with the Gapminder data. Thanks to @wrathematics, heres a small example where we can (barely) detect a difference due to type. set.seed(1234) z &lt;- rnorm(10) quantile(z, type = 1) #&gt; 0% 25% 50% 75% 100% #&gt; -2.346 -0.890 -0.564 0.429 1.084 quantile(z, type = 4) #&gt; 0% 25% 50% 75% 100% #&gt; -2.346 -1.049 -0.564 0.353 1.084 all.equal(quantile(z, type = 1), quantile(z, type = 4)) #&gt; [1] &quot;Mean relative difference: 0.178&quot; Now we can call our function, requesting that quantiles be computed in different ways. qdiff6(z, probs = c(0.25, 0.75), type = 1) #&gt; [1] 1.32 qdiff6(z, probs = c(0.25, 0.75), type = 4) #&gt; [1] 1.4 While the difference may be subtle, its there. Marvel at the fact that we have passed type = 1 through to quantile() even though it was not a formal argument of our own function. The special argument ... is very useful when you want the ability to pass arbitrary arguments down to another function, but without constantly expanding the formal arguments to your function. This leaves you with a less cluttered function definition and gives you future flexibility to specify these arguments only when you need to. You will also encounter the ... argument in many built-in functions  read up on c() or list()  and now you have a better sense of what it means. It is not a breezy and so on and so forth. There are also downsides to ..., so use it with intention. In a package, you will have to work harder to create truly informative documentation for your user. Also, the quiet, absorbent properties of ... mean it can sometimes silently swallow other named arguments, when the user has a typo in the name. Depending on whether or how this fails, it can be a little tricky to find out what went wrong. The ellipsis package provides tools that help package developers use ... more safely. The in-progress tidyverse principles guide provides further guidance on the design of functions that take ... in Data, dots, details. 54.25 Use testthat for formal unit tests Until now, weve relied on informal tests of our evolving function. If you are going to use a function a lot, especially if it is part of a package, it is wise to use formal unit tests. The testthat package (CRAN; GitHub) provides excellent facilities for this, with a distinct emphasis on automated unit testing of entire packages. However, we can take it out for a test drive even with our one measly function. We will construct a test with test_that() and, within it, we put one or more expectations that check actual against expected results. You simply harden your informal, interactive tests into formal unit tests. Here are some examples of tests and indicative expectations. library(testthat) test_that(&#39;invalid args are detected&#39;, { expect_error(qdiff6(&quot;eggplants are purple&quot;)) expect_error(qdiff6(iris)) }) test_that(&#39;NA handling works&#39;, { expect_error(qdiff6(c(1:5, NA), na.rm = FALSE)) expect_equal(qdiff6(c(1:5, NA)), 4) }) No news is good news! Lets see what test failure would look like. Lets revert to a version of our function that does no NA handling, then test for proper NA handling. We can watch it fail. qdiff_no_NA &lt;- function(x, probs = c(0, 1)) { the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } test_that(&#39;NA handling works&#39;, { expect_that(qdiff_no_NA(c(1:5, NA)), equals(4)) }) Similar to the advice to use assertions in data analytical scripts, I recommend you use unit tests to monitor the behavior of functions you (or others) will use often. If your tests cover the functions important behavior, then you can edit the internals freely. Youll rest easy in the knowledge that, if you broke anything important, the tests will fail and alert you to the problem. A function that is important enough for unit tests probably also belongs in a package, where there are obvious mechanisms for running the tests as part of overall package checks. "],["functions-practicum.html", "55 Function-writing practicum 55.1 Overview 55.2 Load the Gapminder data 55.3 Get data to practice with 55.4 Get some code that works 55.5 Turn working code into a function 55.6 Test on other data and in a clean workspace 55.7 Are we there yet? 55.8 Resources", " 55 Function-writing practicum 55.1 Overview We recently learned how to write our own R functions (part 1, part 2, part 3). Now we use that knowledge to write another useful function, within the context of the Gapminder data: Input: a data.frame that contains (at least) a life expectancy variable lifeExp and a variable for year year Output: a vector of estimated intercept and slope, from a linear regression of lifeExp on year The ultimate goal is to apply this function to the Gapminder data for a specific country. We will eventually scale up to all countries using external machinery, e.g., the dplyr::group_by() + dplyr::do(). 55.2 Load the Gapminder data As usual, load gapminder. Load ggplot2 because well make some plots and load dplyr too. library(gapminder) library(ggplot2) library(dplyr) 55.3 Get data to practice with I extract the data for one country in order to develop some working code interactively. j_country &lt;- &quot;France&quot; # pick, but do not hard wire, an example (j_dat &lt;- gapminder %&gt;% filter(country == j_country)) #&gt; # A tibble: 12 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 France Europe 1952 67.4 42459667 7030. #&gt; 2 France Europe 1957 68.9 44310863 8663. #&gt; 3 France Europe 1962 70.5 47124000 10560. #&gt; 4 France Europe 1967 71.6 49569000 13000. #&gt; 5 France Europe 1972 72.4 51732000 16107. #&gt; 6 France Europe 1977 73.8 53165019 18293. #&gt; 7 France Europe 1982 74.9 54433565 20294. #&gt; 8 France Europe 1987 76.3 55630100 22066. #&gt; 9 France Europe 1992 77.5 57374179 24704. #&gt; 10 France Europe 1997 78.6 58623428 25890. #&gt; 11 France Europe 2002 79.6 59925035 28926. #&gt; 12 France Europe 2007 80.7 61083916 30470. Always always always plot the data. Yes, even now. p &lt;- ggplot(j_dat, aes(x = year, y = lifeExp)) p + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; 55.4 Get some code that works Fit the regression: j_fit &lt;- lm(lifeExp ~ year, j_dat) coef(j_fit) #&gt; (Intercept) year #&gt; -397.765 0.239 Whoa, check out that crazy intercept! Apparently the life expectancy in France around year 0 A.D. was minus 400 years! Never forget to sanity check a model. In this case, a reparametrization is in order. I think it makes more sense for the intercept to correspond to life expectancy in 1952, the earliest date in our dataset. Estimate the intercept eye-ball-o-metrically from the plot and confirm that weve got something sane and interpretable now. j_fit &lt;- lm(lifeExp ~ I(year - 1952), j_dat) coef(j_fit) #&gt; (Intercept) I(year - 1952) #&gt; 67.790 0.239 55.4.1 Sidebar: regression stuff There are two things above that might prompt questions. First, how did I know to get the estimated coefficients from a fitted model via coef()? Years of experience. But how might a novice learn such things? Read the documentation for lm(), in this case. The See also section advises us about many functions that can operate on fitted linear model objects, including, but by no means limited to, coef(). Read the documentation on coef() too. Second, what am I doing here: lm(lifeExp ~ I(year - 1952))? I want the intercept to correspond to 1952 and an easy way to accomplish that is to create a new predictor on the fly: year minus 1952. The way I achieve that in the model formula, I(year - 1952), uses the I() function which inhibits interpretation/conversion of objects. By protecting the expression year - 1952, I ensure it is interpreted in the obvious arithmetical way. 55.5 Turn working code into a function Create the basic definition of a function and drop your working code inside. Add arguments and edit the inner code to match. Apply it to the practice data. Do you get the same result as before? le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) coef(the_fit) } le_lin_fit(j_dat) #&gt; (Intercept) I(year - offset) #&gt; 67.790 0.239 I had to decide how to handle the offset. Given that I will scale this up to many countries, which, in theory, might have data for different dates, I chose to set a default of 1952. Strategies that compute the offset from data, either the main Gapminder dataset or the excerpt passed to this function, are also reasonable to consider. I loathe the names on this return value. This is not my first rodeo and I know that, downstream, these will contaminate variable names and factor levels and show up in public places like plots and tables. Fix names early! le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) setNames(coef(the_fit), c(&quot;intercept&quot;, &quot;slope&quot;)) } le_lin_fit(j_dat) #&gt; intercept slope #&gt; 67.790 0.239 Much better! 55.6 Test on other data and in a clean workspace Its always good to rotate through examples during development. The most common error this will help you catch is when you accidentally hard-wire your example into your function. If youre paying attention to your informal tests, you will find it creepy that your function returns exactly the same results regardless which input data you give it. This actually happened to me while I was writing this document, I kid you not! I had left j_fit inside the call to coef(), instead of switching it to the_fit. How did I catch that error? I saw the fitted line below, which clearly did not have an intercept in the late 60s and a positive slope, as my first example did. Figures are a mighty weapon in the fight against nonsense. I dont trust analyses that have few/no figures. j_country &lt;- &quot;Zimbabwe&quot; (j_dat &lt;- gapminder %&gt;% filter(country == j_country)) #&gt; # A tibble: 12 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zimbabwe Africa 1952 48.5 3080907 407. #&gt; 2 Zimbabwe Africa 1957 50.5 3646340 519. #&gt; 3 Zimbabwe Africa 1962 52.4 4277736 527. #&gt; 4 Zimbabwe Africa 1967 54.0 4995432 570. #&gt; 5 Zimbabwe Africa 1972 55.6 5861135 799. #&gt; 6 Zimbabwe Africa 1977 57.7 6642107 686. #&gt; 7 Zimbabwe Africa 1982 60.4 7636524 789. #&gt; 8 Zimbabwe Africa 1987 62.4 9216418 706. #&gt; 9 Zimbabwe Africa 1992 60.4 10704340 693. #&gt; 10 Zimbabwe Africa 1997 46.8 11404948 792. #&gt; 11 Zimbabwe Africa 2002 40.0 11926563 672. #&gt; 12 Zimbabwe Africa 2007 43.5 12311143 470. p &lt;- ggplot(j_dat, aes(x = year, y = lifeExp)) p + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; le_lin_fit(j_dat) #&gt; intercept slope #&gt; 55.221 -0.093 The linear fit is comically bad, but yes I believe the visual line and the regression results match up. Its also a good idea to clean out the workspace, rerun the minimum amount of code, and retest your function. This will help you catch another common mistake: accidentally relying on objects that were lying around in the workspace during development but that are not actually defined in your function nor passed as formal arguments. rm(list = ls()) le_lin_fit &lt;- function(dat, offset = 1952) { the_fit &lt;- lm(lifeExp ~ I(year - offset), dat) setNames(coef(the_fit), c(&quot;intercept&quot;, &quot;slope&quot;)) } le_lin_fit(gapminder %&gt;% filter(country == &quot;Zimbabwe&quot;)) #&gt; intercept slope #&gt; 55.221 -0.093 55.7 Are we there yet? Yes. Given how I plan to use this function, I dont feel the need to put it under formal unit tests or put in argument validity checks. 55.8 Resources Packages for runtime assertions (the last 3 seem to be under more active development than assertthat): assertthat on CRAN and GitHub - the Hadleyverse option ensurer on CRAN and GitHub - general purpose, pipe-friendly assertr on CRAN and GitHub - explicitly data pipeline oriented assertive on CRAN and Bitbucket - rich set of built-in functions Hadley Wickhams book Advanced R (2015): Section on defensive programming Section on function arguments Section on return values Unit testing with testthat On CRAN, development on GitHub, main webpage Wickham and Bryans R Packages book (in progress) Testing chapter Wickhams testthat: Get Started with Testing article in The R Journal (2011a). Maybe this is completely superseded by the newer chapter above? Be aware that parts could be out of date, but I recall it was a helpful read. "],["lab08.html", "56 Lab 08 56.1 Conveying the right message through visualization 56.2 Learning goals 56.3 Getting started 56.4 Exercises", " 56 Lab 08 56.1 Conveying the right message through visualization In this lab, our goal is to reconstruct and improve a data visualization on COVID and mask wearing. 56.2 Learning goals Critiquing visualizations that misrepresent data Improving data visualizations to better convey the right message 56.3 Getting started Go to the course GitHub organization and locate the template. Clone it in RStudio and open the R Markdown document. Knit the document to make sure it compiles without errors. 56.3.1 Warm up Lets warm up with some simple exercises. Update the YAML of your R Markdown file with your information, knit, commit, and push your changes. Make sure to commit with a meaningful commit message. Then, go to your repo on GitHub and confirm that your changes are visible in your Rmd and md files. If anything is missing, commit and push again. 56.3.2 Packages Well use the tidyverse package for much of the data wrangling and visualization. This package is already installed for you. You can load it by running the following in your Console: library(tidyverse) 56.3.3 Data In this lab, youll construct the dataset! 56.4 Exercises The following visualization was shared on Twitter as extraordinary misleading. Hey, @maddow and @MaddowBlog @SecNormanas as much as we&#39;d all hope everyone would wear masks, this chart is extraordinary misleading. If you don&#39;t believe me, ask @AlbertoCairo who wrote the book on it. Check the scale on the two axes. pic.twitter.com/JLxxgxzbua&mdash; Jon Boeckenstedt (@JonBoeckenstedt) August 7, 2020 Before you dive in, think about what is misleading about this visualization and how you might go about fixing it. Create a data frame that can be used to re-construct this visualization. You may need to guess some of the numbers, thats ok. You should first think about how many rows and columns youll need and what you want to call your variables. Then, you can use the tribble() function for this. For example, if you wanted to construct the following data frame df #&gt; # A tibble: 3 x 2 #&gt; date count #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1/1/2020 15 #&gt; 2 2/1/2020 20 #&gt; 3 3/1/2020 22 you can write df &lt;- tribble( ~date, ~count, &quot;1/1/2020&quot;, 15, &quot;2/1/2020&quot;, 20, &quot;3/1/2020&quot;, 22, ) Make a visualization that more accurately (and honestly) tells the story. What message is more clear in your visualization than it was in the original visualization? What, if any, useful information do these data and your visualization tell us about mask wearing and COVID? Itll be difficult to set aside what you already know about mask wearing, but you should try to focus only on what this visualization tells. Feel free to also comment on whether that lines up with what you know about mask wearing.    Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure youre happy with the final state of your work. "],["lab08b.html", "57 Optional Lab 08b 57.1 Packages 57.2 Data collection via web scraping 57.3 Data cleaning 57.4 Data visualization and interpretation", " 57 Optional Lab 08b This lab is optional. You are welcome to try it and can count it either as a lab or as a portfolio piece. Disclaimer It is a more extensive lab that Im not quite happy with. Specifically, I think it is a bit too time consuming and Ive not been able to calibrate the difficulty. Tired of typing your password? Chances are your browser has already saved your password, but if not, you can ask Git to save (cache) your password for a period of time, where you indicate the period of time in seconds. For example, if you want it to cache your password for 1 hour, that would be 3,600 seconds. To do so, run the following in the console, usethis::use_git_config(credential.helper = \"cache --timeout=3600\"). If you want to cache it for a longer amount of time, you can adjust the number in the code. 57.1 Packages In this assignment we will use the following packages: tidyverse: a collection of packages for doing data analysis in a tidy way robotstxt: provides functions to download and parse robots.txt files, making it easy to check if bots (spiders, crawler, scrapers, ) are allowed to access specific resources on a domain rvest: helps you scrape information from web pages scales: provides the internal scaling infrastructure used by ggplot2 and gives you tools to override the default breaks, labels, transformations and palettes 57.2 Data collection via web scraping The data come from OpenSecrets.org, a website tracking the influence of money on U.S. politics, and how that money affects policy and citizens lives. This website is hosted by The Center for Responsive Politics, which is a nonpartisan, independent nonprofit that tracks money in U.S. politics and its effect on elections and public policy.3 Before getting started, lets check that a bot has permissions to access pages on this domain. library(robotstxt) paths_allowed(&quot;https://www.opensecrets.org&quot;) #&gt; [1] TRUE Our goal is to scrape data for contributions in all election years Open Secrets has data for. Since that means repeating a task many times, lets first write a function that works on the first page. Confirm it works on a few others. Then iterate it over pages for all years. Complete the following set of steps in the scrape-pac.R file in the scripts folder of your repository. This file already contains some starter code to help you out. Write a function called scrape_pac() that scrapes information from the Open Secrets webpage for foreign-contected PAC contributions in a given year. This function should have one input: the URL of the webpage and should return a data frame. rename variables scraped, using snake_case naming. clean up the Country of Origin/Parent Company variable with str_squish(). add a new column to the data frame for year. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesnt take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the str_sub() function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify last 4 characters. Define the URLs for 2020, 2018, and 1998 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do? Construct a vector called urls that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year. Map the scrape_pac() function over urls in a way that will result in a data frame called pac_all. Write the data frame to a csv file called pac-all.csv in the data folder.  If you havent yet done so, now is definitely a good time to commit and push your changes to GitHub with an appropriate commit message (e.g. Data scraping complete). Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. In your R Markdown file, load pac-all.csv and report its number of observations and variables using inline code. 57.3 Data cleaning In this section, we clean the pac_all data frame to prepare it for analysis and visualization. We have two goals in data cleaning: Separate the country_parent into two such that country and parent company appear in different columns for country-level analysis. Convert contribution amounts in total, dems, and repubs from character strings to numeric values. The following exercises walk you through how to make these fixes to the data. Use the separate() function to separate country_parent into country and parent columns. Note that country and parent company names are separated by \\ (which will need to be specified in your function) and also note that there are some entries where the \\ sign appears twice and in these cases we want to only split the value at the first occurrence of \\. This can be accomplished by setting the extra argument in to \"merge\" so that the cell is split into only 2 segments, e.g. we want \"Denmark/Novo Nordisk A/S\" to be split into \"Denmark\" and \"Novo Nordisk A/S\". (See help for separate() for more on this.) End your code chunk by printing out the top 10 rows of your data frame (if you just type the data frame name it should automatically do this for you). Remove the character strings including $ and , signs in the total, dems,and repubs columns and convert these columns to numeric. End your code chunk by printing out the top 10 rows of your data frame (if you just type the data frame name it should automatically do this for you). A couple hints to help you out: The $ character is a special character so it will need to be escaped. Some contribution amounts are in the millions (e.g. Anheuser-Busch contributed a total of $1,510,897 in 2008). In this case we need to remove all occurrences of ,, which we can do by using str_remove_all() instead of str_remove().    Now is a good time to knit your document, and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 57.4 Data visualization and interpretation Create a line plot of total contributions from all foreign-connected PACs in the Canada and Mexico over the years. Once you have made the plot, write a brief interpretation of what the graph reveals. Few hints to help you out: Filter for only Canada and Mexico. Calculate sum of total contributions from PACs for each year for each country by using a sequence of group_by() then summarise(). Make a plot of total contributions (y-axis) by year (x-axis) where two lines identified by different colours represent each of Canada and Mexico. Recreate the following visualization. Once you have made the plot, write a brief interpretation of what the graph reveals. Note that these are only UK contributions. You will need to make use of functions from the scales package for axis labels as well as from ggplot2. Remember, if you cant figure out certain bits, you can always ask on the class github!    Knit your document, and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Source: Open Secrets - About. "],["welcome-to-data-and-ethics.html", "58 Welcome to Data and Ethics 58.1 Module Materials", " 58 Welcome to Data and Ethics This module is a bit different than the typical module. Well be introducing ideas related to privacy and ethics in the context of data science. There is only one video this week. This week is dedicated to your mid-semester check-in with Mason. I encourage you to watch the video and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 58.1 Module Materials Slides from Lectures Ethics Algorithmic bias Readings How to make a racist AI without really trying How to write a racist AI in R without really trying Check out the annotated bibliography. Activities Annotated Bibliography Lab Lab Me hearing my students make nuanced and important points during a class discussion of Data Science Ethics: pic.twitter.com/7TUupLb9MH&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) March 31, 2021 "],["data-science-and-ethics.html", "59 Data Science and Ethics 59.1 Module Commentary 59.2 Curated Videography 59.3 Annotated Bibliography Instructions", " 59 Data Science and Ethics 59.1 Module Commentary Bonus slides here and here if they do not appear below. 59.2 Curated Videography 59.2.1 Data Science Ethics in 6 Minutes 59.2.2 AI for Good in the R and Python ecosystems 59.2.3 Are We Automating Racism? 59.2.4 Big Techs B.S. about AI ethics After spending a few years cutting through Big Tech&#39;s B.S. about AI ethics, I&#39;ve created a glossary to help you decode what all of their favorite terms actually mean.I had way too much fun working on this! Threading some of my favorites below. https://t.co/h0TGqoFpHA&mdash; Karen Hao (@_KarenHao) April 13, 2021 59.3 Annotated Bibliography Instructions An annotated bibliography is a list of citations but with commentary! Ok, more like just an enhanced list where you summarize the source and explain why it is important to include. Your mission is to add either a citation or an annotation (or both) to this list of Data Science and Ethics Readings. Carole Cadwalladr and Emma Graham-Harrison. How Cambridge Analytica turned Facebook likes into a lucrative political tool. The Guardian. 17 March 2018. Chen Wenhong and Anabel Quan-Haase. Big Data Ethics and Politics: Toward New Understandings. Social Science Computer Review. 14 November 2018. Nitasha Tiku. [Google hired Timnit Gebru to be an outspoken critic of unethical AI. Then she was fired for it.]. (https://www.washingtonpost.com/technology/2020/12/23/google-timnit-gebru-ai-ethics/). The Washington Post. 23 December 2020. Dan Swinhoe. [The biggest data breach fines, penalties, and settlements so far]. (https://www.csoonline.com/article/3410278/the-biggest-data-breach-fines-penalties-and-settlements-so-far.html). CSO. 5 March 2021. Sara Morrison.Why you should care about data privacy even if you have nothing to hide. Vox. Jan 28 2021. Richard Van Noorden. The ethical questions that haunt facial-recognition research. Nature. 18 November 2020. Karen Hao. Big Techs guide to talking about AI ethics. MIT Technology Review. April 13 2021 Catherine DIgnazio and Lauren F. Klein. Data feminism. Mit Press, 2020. "],["lab09.html", "60 Lab: Professor attractiveness and course evaluations 60.1 Modeling with a single predictor 60.2 Getting started 60.3 Warm up 60.4 The data 60.5 Exercises", " 60 Lab: Professor attractiveness and course evaluations 60.1 Modeling with a single predictor 60.2 Getting started Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings. (Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. http://www.sciencedirect.com/science/article/pii/S0272775704001165.) For this assignment, you will analyze the data from this study in order to learn what goes into a positive professor evaluation. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. 60.2.1 Packages In this lab, we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 60.2.2 Housekeeping 60.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 60.2.2.2 Project name Update the name of your project to match the labs title. 60.3 Warm up Before we introduce the data, lets warm up with some simple exercises. 60.3.1 YAML Open the R Markdown (Rmd) file in your project and knit the document. 60.3.2 Committing and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update name in the Commit message box and hit Commit. Click on Push. This will prompt a dialog box where you first need to enter your user name, and then your password. 60.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 60.5 Exercises 60.5.1 Part 1: Exploratory Data Analysis Visualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response. Visualize and describe the relationship between score and the new variable you created, bty_avg. Hint: See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html. Replot the scatterplot from Exercise 3, but this time use geom_jitter()? What does jitter mean? What was misleading about the initial scatterplot? 60.5.2 Part 2: Linear regression with a numerical predictor Recall: Linear model is in the form \\(\\hat{y} = b_0 + b_1 x\\). Lets see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model. Replot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line. Interpret the slope of the linear model in context of the data. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context. Determine the \\(R^2\\) of the model and interpret it in context of the data. 60.5.3 Part 3: Linear regression with a categorical predictor Fit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data. What is the equation of the line corresponding to male professors? What is it for female professors? Fit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Create a new variable called rank_relevel where \"tenure track\" is the baseline level. Fit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. Create another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\". Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. "],["welcome-to-modeling-the-tidy-way.html", "61 Welcome to modeling the tidy way! 61.1 Module Materials", " 61 Welcome to modeling the tidy way! This module introduces you to the ideas behind fitting and interpreting models within the framework of tidymodels. Please watch the videos and work your way through the notes. The videos start on the next page. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. In the spirit of transparency, I have strong [non-positive][tidymodelthoughts] thoughts on tidymodels, but I also think that it is useful for you to learn them. I have similar feelings about the necessary evil that is ANOVA or SPSS. 61.1 Module Materials Videos Located in the subchapters of this module Slidedecks Language of Models Fitting and interpreting models Suggested Readings All subchapters of this module Articles Rodgers, J. L. (2010). The epistemology of mathematical and statistical modeling: A quiet methodological revolution. American Psychologist, 65, 1-12. R4DS Model Section, including Model Basics Model Building No Lab this week! "],["language-of-models.html", "62 Language of Models! 62.1 What is a model? 62.2 Modeling the relationship between variables", " 62 Language of Models! You can follow along with the slides here if they do not appear below. 62.1 What is a model? 62.2 Modeling the relationship between variables "],["fitting-and-interpreting-models.html", "63 Fitting and interpreting models 63.1 Models with numerical explanatory variables 63.2 A More Technical Worked Example 63.3 Models with categorical explanatory variables 63.4 My Thoughts on Tidy Modelings {tidymodelthoughts}", " 63 Fitting and interpreting models You can follow along with the slides here if they do not appear below. 63.1 Models with numerical explanatory variables 63.2 A More Technical Worked Example Lets load our standard libraries: library(lattice) library(ggplot2) library(tidyverse) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- #&gt; v tibble 3.1.0 v dplyr 1.0.5 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; v purrr 0.3.4 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() If youve taken a regression course, you might recognize this model as a special case of a linear model. If you havent, well, it doesnt really matter much except we can use the lm() function to fit the model. The advantage is that lm() easily splits the data into fitted values and residuals: Observed value = Fitted value + residual Lets get the fitted values and residuals for each voice part: singer_lm = lm(height ~ 0 + voice.part, data=singer) We can extract the fitted values using fitted.values(singer.lm) and the residuals with residuals(singer.lm) or singer.lm$residuals. For convenience, we create a data frame with two columns: the voice parts and the residuals. singer_res = data.frame(voice_part = singer$voice.part, residual = residuals(singer_lm)) We can also do this with group_by and mutate: fits = singer %&gt;% group_by(voice.part) %&gt;% mutate(fit = mean(height), residual = height - mean(height)) 63.2.1 Does the linear model fit? To assess whether the linear model is a good fit to the data, we need to know whether the errors look like they come from normal distributions with the same variance. The residuals are our estimates of the errors, and so we need to check both normality and homoscedasticity. 63.2.2 Homoscedasticity There are a few ways we can look at the residuals. Side-by-side boxplots give a broad overview: ggplot(singer_res, aes(x = voice_part, y = residual)) + geom_boxplot() We can also look at the ecdfs of the residuals for each voice part. ggplot(singer_res, aes(x = residual, color = voice_part)) + stat_ecdf() From these plots, it seems like the residuals in each group have approximately the same variance. 63.2.3 Normality We also want to examine normality of the residuals, broken up by voice part. We can do this by faceting: ggplot(singer_res, aes(sample = residual)) + stat_qq() + facet_wrap(~ voice_part, ncol=4) Not only do the lines look reasonably straight, the scales look similar for all eight voice parts. This suggests a model where all of the errors are normal with the same standard deviation. This is convenient because it is the form of a standard linear model: Singer height = Average height for their voice part + Normal(\\(0, \\sigma^2\\)) error. 63.2.4 Normality of pooled residuals If the linear model holds, then all the residuals come from the same normal distribution. Weve already checked for normality of the residuals within each voice part, but to get a little more power to see divergence from normality, we can pool the residuals and make a normal QQ plot of all the residuals together. ggplot(singer_res, aes(sample = residual)) + stat_qq() Its easier to check normality if we plot the line that the points should fall on: if we think the points come from a \\(N(\\mu, \\sigma^2)\\) distribution, they should lie on a line with intercept \\(\\mu\\) and slope \\(\\sigma\\) (the standard deviation). In the linear model, we assume that the mean of the error terms is zero. We dont know what their variance should be, but we can estimate it using the variance of the residuals. Therefore, we add a line with the mean of the residuals (which should be zero) as the intercept, and the SD of the residuals as the slope. This is: ggplot(singer_res, aes(sample = residual)) + stat_qq() + geom_abline(aes(intercept = 0, slope = sd(singer_res$residual))) #&gt; Warning: Use of `singer_res$residual` is discouraged. Use `residual` instead. 63.2.5 The actually correct way Pedantic note: We should use an \\(n-8\\) denominator instead of \\(n-1\\) in the SD calculation for degrees of freedom reasons. The \\(n-8\\) part is necessary because there are 7 different variables associated with the model we fitted with singer_lm. We can get the SD directly from the linear model: sd(singer_res$residual) #&gt; [1] 2.47 round(summary(singer_lm)$sigma, 3) #&gt; [1] 2.5 However, the difference between this and the SD above is negligible. Add the line: ggplot(singer_res, aes(sample = residual)) + stat_qq() + geom_abline(intercept = mean(singer_res$residual), slope=summary(singer_lm)$sigma) The straight line isnt absolutely perfect, but its doing a pretty good job. 63.2.6 Our final model Since the errors seem to be pretty normal, our final model is: Singer height = Average height for their voice part + Normal(\\(0, 2.5^2\\)) error. Note: Although normality (or lack thereof) can be important for probabilistic prediction or (sometimes) for inferential data analysis, its relatively unimportant for EDA. If your residuals are about normal thats nice, but as long as theyre not horribly skewed theyre probably not a problem. 63.2.7 What have we learned? About singers: Weve seen that average height increases as the voice part range decreases. Within each voice part, the residuals look like they come from a normal distribution with the same variance for each voice part. This suggests that theres nothing further we need to do to explain singer heights: we have an average for each voice part, and there is no suggestion of systematic differences beyond that due to voice part. About data analysis: We can use some of our univariate visualization tools, particularly boxplots and ecdfs, to look for evidence of heteroscedasticity. We can use normal QQ plots on both pooled and un-pooled residuals to look for evidence of non-normality. If we wanted to do formal tests or parameter estimation for singer heights, we would feel pretty secure using results based on normal theory. 63.3 Models with categorical explanatory variables 63.4 My Thoughts on Tidy Modelings {tidymodelthoughts} That&#39;s a good distinction -- I think tidyverse is good for data cleaning and onboarding undergrads (my lab is very undergrad heavy); but that tidymodels can die in a fire because they don&#39;t report f-tests!!!!&mdash; S. Mason Garrison, PhD (@SMasonGarrison) April 28, 2021 "],["classic-things-in-r.html", "64 Classic Things in R 64.1 Descriptive Statistics 64.2 Specific Measures of Central Tendency 64.3 Relationship between mean, median, and mode 64.4 Variability/spread 64.5 Spread around the Mean 64.6 R Examples 64.7 Bessels Correction", " 64 Classic Things in R This chapter is a collection of basic base R code that Ive used in my undergraduate intro stats class. Eventually, there will be a lot more commentary/ context. In the meantime however ######## Levels of Measurement Examples # Ratio Example library(HistData) # loads the HistData package data(&quot;Galton&quot;) # loads the Galton dataset attach(Galton) # attaches the dataset head(Galton, n=10) # First ten rows of data #&gt; parent child #&gt; 1 70.5 61.7 #&gt; 2 68.5 61.7 #&gt; 3 65.5 61.7 #&gt; 4 64.5 61.7 #&gt; 5 64.0 61.7 #&gt; 6 67.5 62.2 #&gt; 7 67.5 62.2 #&gt; 8 67.5 62.2 #&gt; 9 66.5 62.2 #&gt; 10 66.5 62.2 hist(child) # Histogram hist(Galton$child) is the same, use if you do not want to attach the data file plot(density(child)) # Density Plot detach(Galton) # Remember to detach you dataset! # Interval Example library(datasets) data(&quot;nottem&quot;) nottem[1:10] # First ten rows of data #&gt; [1] 40.6 40.8 44.4 46.7 54.1 58.5 57.7 56.4 54.3 50.5 hist(nottem) # Histogram plot(density(nottem)) # Density Plot # Ordinal Example library(ggplot2movies) data(movies) variable&lt;-movies$rating head(movies, n=10) # First ten rows of data #&gt; title year length budget rating votes r1 r2 r3 r4 #&gt; 1 $ 1971 121 NA 6.4 348 4.5 4.5 4.5 4.5 #&gt; 2 $1000 a Touchdown 1939 71 NA 6.0 20 0.0 14.5 4.5 24.5 #&gt; 3 $21 a Day Once a Month 1941 7 NA 8.2 5 0.0 0.0 0.0 0.0 #&gt; 4 $40,000 1996 70 NA 8.2 6 14.5 0.0 0.0 0.0 #&gt; 5 $50,000 Climax Show, The 1975 71 NA 3.4 17 24.5 4.5 0.0 14.5 #&gt; 6 $pent 2000 91 NA 4.3 45 4.5 4.5 4.5 14.5 #&gt; 7 $windle 2002 93 NA 5.3 200 4.5 0.0 4.5 4.5 #&gt; 8 &#39;15&#39; 2002 25 NA 6.7 24 4.5 4.5 4.5 4.5 #&gt; 9 &#39;38 1987 97 NA 6.6 18 4.5 4.5 4.5 0.0 #&gt; 10 &#39;49-&#39;17 1917 61 NA 6.0 51 4.5 0.0 4.5 4.5 #&gt; r5 r6 r7 r8 r9 r10 mpaa Action Animation Comedy Drama Documentary #&gt; 1 14.5 24.5 24.5 14.5 4.5 4.5 0 0 1 1 0 #&gt; 2 14.5 14.5 14.5 4.5 4.5 14.5 0 0 1 0 0 #&gt; 3 0.0 24.5 0.0 44.5 24.5 24.5 0 1 0 0 0 #&gt; 4 0.0 0.0 0.0 0.0 34.5 45.5 0 0 1 0 0 #&gt; 5 14.5 4.5 0.0 0.0 0.0 24.5 0 0 0 0 0 #&gt; 6 14.5 14.5 4.5 4.5 14.5 14.5 0 0 0 1 0 #&gt; 7 24.5 24.5 14.5 4.5 4.5 14.5 R 1 0 0 1 0 #&gt; 8 4.5 14.5 14.5 14.5 4.5 14.5 0 0 0 0 1 #&gt; 9 0.0 0.0 34.5 14.5 4.5 24.5 0 0 0 1 0 #&gt; 10 4.5 44.5 14.5 4.5 4.5 4.5 0 0 0 0 0 #&gt; Romance Short #&gt; 1 0 0 #&gt; 2 0 0 #&gt; 3 0 1 #&gt; 4 0 0 #&gt; 5 0 0 #&gt; 6 0 0 #&gt; 7 0 0 #&gt; 8 0 1 #&gt; 9 0 0 #&gt; 10 0 0 hist(variable) # Histogram plot(density(variable)) # Density Plot # Nominal Example library(vcd) #&gt; Loading required package: grid data(Arthritis) Arthritis[1:10, ] # First ten rows of data #&gt; ID Treatment Sex Age Improved #&gt; 1 57 Treated Male 27 Some #&gt; 2 46 Treated Male 29 None #&gt; 3 77 Treated Male 30 None #&gt; 4 17 Treated Male 32 Marked #&gt; 5 36 Treated Male 46 Marked #&gt; 6 23 Treated Male 58 Marked #&gt; 7 75 Treated Male 59 None #&gt; 8 39 Treated Male 59 Marked #&gt; 9 33 Treated Male 63 None #&gt; 10 55 Treated Male 63 None variable&lt;-Arthritis$Treatment hist(variable) # Histogram error #&gt; Error in hist.default(variable): &#39;x&#39; must be numeric barplot_fix&lt;-prop.table(table(variable)) barplot(barplot_fix) # Sometimes, R is silly ######## Summary Examples # Bar chart library(car) #&gt; Loading required package: carData counts &lt;- table(mtcars$gear) barplot(counts, main=&quot;Car Distribution&quot;, xlab=&quot;Number of Gears&quot;) hist(counts) # Pie chart mytable &lt;- table(iris$Species) lbls &lt;- paste(names(mytable), &quot;\\n&quot;, mytable, sep=&quot;&quot;) pie(mytable, labels = lbls, main=&quot;Pie Chart of Species\\n (with sample sizes)&quot;) # Histogram library(MASS) variable&lt;-cats$Bwt hist(variable) variable&lt;-variable*2.2 #Convert to Imperial hist(variable) # Stem and Leaf plot stem(faithful$eruptions,scale=1) #&gt; #&gt; The decimal point is 1 digit(s) to the left of the | #&gt; #&gt; 16 | 070355555588 #&gt; 18 | 000022233333335577777777888822335777888 #&gt; 20 | 00002223378800035778 #&gt; 22 | 0002335578023578 #&gt; 24 | 00228 #&gt; 26 | 23 #&gt; 28 | 080 #&gt; 30 | 7 #&gt; 32 | 2337 #&gt; 34 | 250077 #&gt; 36 | 0000823577 #&gt; 38 | 2333335582225577 #&gt; 40 | 0000003357788888002233555577778 #&gt; 42 | 03335555778800233333555577778 #&gt; 44 | 02222335557780000000023333357778888 #&gt; 46 | 0000233357700000023578 #&gt; 48 | 00000022335800333 #&gt; 50 | 0370 ######## Central Tendency library(pscl) #&gt; Classes and Methods for R developed in the #&gt; Political Science Computational Laboratory #&gt; Department of Political Science #&gt; Stanford University #&gt; Simon Jackman #&gt; hurdle and zeroinfl functions by Achim Zeileis data(prussian) variable&lt;-prussian$y #horse kick fatality by year # Mean mean(variable) #&gt; [1] 0.7 #Median median(variable) #&gt; [1] 0 #Mode mode(variable) # That&#39;s not mode! #&gt; [1] &quot;numeric&quot; # Function to examine mode Mode &lt;- function(x) { ux &lt;- unique(x) #finds all unique values ux[which.max(tabulate(match(x, ux)))] #returns the value which is most frequent } Mode(variable) #&gt; [1] 0 ## summary hist(variable) library(Hmisc) #&gt; Loading required package: lattice #&gt; Loading required package: survival #&gt; Loading required package: Formula #&gt; Loading required package: ggplot2 #&gt; #&gt; Attaching package: &#39;Hmisc&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; format.pval, units describe(variable) #&gt; variable #&gt; n missing distinct Info Mean Gmd #&gt; 280 0 5 0.828 0.7 0.8752 #&gt; #&gt; lowest : 0 1 2 3 4, highest: 0 1 2 3 4 #&gt; #&gt; Value 0 1 2 3 4 #&gt; Frequency 144 91 32 11 2 #&gt; Proportion 0.514 0.325 0.114 0.039 0.007 ######## Spread # Range range(variable) #&gt; [1] 0 4 max(variable) -min(variable) #&gt; [1] 4 # 5 Number Summary / Quartiles summary(variable) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0 0.0 0.0 0.7 1.0 4.0 # Variance var(variable) #&gt; [1] 0.763 # Standard Deviation sqrt(var(variable)) #&gt; [1] 0.873 sd(variable) #&gt; [1] 0.873 ################ Rescaling variable&lt;-movies$rating scale(variable)[1:10] #&gt; [1] 0.3008 0.0432 1.4598 1.4598 -1.6309 -1.0514 -0.4075 0.4940 0.4296 #&gt; [10] 0.0432 plot(density(variable)) # no scaling plot(density(scale(variable))) # with scaling ## bandwidth part of the smoothing method -- is a bit of a function of standard deviation #### Normal Distribution # Display the normal distributions with various means x &lt;- seq(-80, 80, length=1000) hx &lt;- dnorm(x) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;,&quot;green&quot;, &quot;green&quot;, &quot;gold&quot;, &quot;black&quot;) plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Comparison of normal Distributions&quot;,xlim=c(-5, 7)) location&lt;-c(2,4,-2) for (i in 1:3){ lines(x, dnorm(x,mean=location[i]), lwd=1, col=colors[i]) } # Display the normal distributions with various standard deviations plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Comparison of normal Distributions&quot;,xlim=c(-10, 10)) for (i in c(.5,2,4,6)){ lines(x, dnorm(x,sd=i), lwd=1, col=colors[i]) } #### Z-Score Problem # Data IQ &lt;-c( 114, 130, 74, 100, 120, 112, 104, 132, 107, 89, 111, 103, 102, 128, 98, 91, 118, 96, 114, 119, 112, 114, 86, 112, 103, 72, 93, 105, 111, 108, 103) # Stem plot stem(IQ,scale=1) #&gt; #&gt; The decimal point is 1 digit(s) to the right of the | #&gt; #&gt; 7 | 24 #&gt; 8 | 69 #&gt; 9 | 1368 #&gt; 10 | 023334578 #&gt; 11 | 1122244489 #&gt; 12 | 08 #&gt; 13 | 02 # Mean and SD mean(IQ) #&gt; [1] 106 sd(IQ) #&gt; [1] 14.3 # What proportion of scores are within one standard deviation of the mean? ## 1 SD above above1&lt;- mean(IQ)+ 1*sd(IQ) ## 1 SD below below1&lt;- mean(IQ)- 1*sd(IQ) ## numbers within range x&lt;-IQ y = c( below1, above1) (x = x[ x &gt;= y[1] &amp; x &lt;= y[2]]) #&gt; [1] 114 100 120 112 104 107 111 103 102 98 118 96 114 119 112 114 112 103 93 #&gt; [20] 105 111 108 103 # What would these proportions be in an exactly Normal distribution? ## Hard way pnorm(above1,mean=mean(IQ),sd=sd(IQ)) - pnorm(below1,mean=mean(IQ),sd=sd(IQ)) #&gt; [1] 0.683 ## Easy Way pnorm(1) - pnorm(-1) #&gt; [1] 0.683 ## What about 2 SD? ### Hypothesis Testing and P-values ## probability X &gt;= 145 1 - pnorm(145, 100, 15) #&gt; [1] 0.00135 ## probability X &lt;= 55 pnorm(55, 100, 15) #&gt; [1] 0.00135 pnorm(55, 100, 15)*2 #&gt; [1] 0.0027 # more significant? 1 - pt(2.54, 24) #&gt; [1] 0.00899 2 * (1 - pt(2.54, 24)) #&gt; [1] 0.018 ### SLICING DATA library(HSAUR) #&gt; Loading required package: tools data(&quot;womensrole&quot;) data=womensrole[womensrole$education&lt;5,] data2=womensrole[womensrole$sex==&quot;Male&quot;,] #### Correlation # Correlation summary(womensrole$agree) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0 3.2 13.0 24.3 28.8 190.0 attach(womensrole) cor(x=education,y=agree) #&gt; [1] 0.0256 plot(x=education,y=agree) # Two variables, y and x together detach(womensrole) womensrole$percentage_agree&lt;-womensrole$agree/(womensrole$agree+womensrole$disagree) summary(womensrole$percentage_agree) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 0.000 0.182 0.446 0.458 0.667 1.000 1 attach(womensrole) cor(x=education,y=percentage_agree) #&gt; [1] NA cor(x=education,y=percentage_agree,use=&quot;pairwise.complete&quot;) # missing data #&gt; [1] -0.895 plot(x=education,y=agree) plot(x=education,y=percentage_agree,xlab=&quot;Years of Education&quot;,ylab=&quot;Percent who agree&quot;) detach(womensrole) ## Continuous predictor -&gt; continuous Outcome fit.object &lt;- lm(data=womensrole,percentage_agree ~ education) summary(fit.object) #&gt; #&gt; Call: #&gt; lm(formula = percentage_agree ~ education, data = womensrole) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.2299 -0.0811 -0.0309 0.0895 0.2963 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.89655 0.04056 22.1 &lt;2e-16 *** #&gt; education -0.04298 0.00343 -12.5 3e-15 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.132 on 39 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; Multiple R-squared: 0.801, Adjusted R-squared: 0.796 #&gt; F-statistic: 157 on 1 and 39 DF, p-value: 2.98e-15 cor(womensrole$education,womensrole$percentage_agree,use=&quot;pairwise.complete&quot;) #&gt; [1] -0.895 plot(womensrole$education,womensrole$percentage_agree) abline(fit.object, col = &quot;red&quot;) ## predict ### at what levels of education do you want estimated agreement? new.df &lt;- data.frame(education=c(0, 6, 12)) predict(fit.object, new.df) #&gt; 1 2 3 #&gt; 0.897 0.639 0.381 ## Categoreical predictor -&gt; continuous Outcome fit.object &lt;- lm(data=womensrole,percentage_agree ~ sex) summary(fit.object) #&gt; #&gt; Call: #&gt; lm(formula = percentage_agree ~ sex, data = womensrole) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.4676 -0.2858 -0.0211 0.2004 0.5504 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.4496 0.0644 6.98 2.3e-08 *** #&gt; sexFemale 0.0179 0.0922 0.19 0.85 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.295 on 39 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; Multiple R-squared: 0.000971, Adjusted R-squared: -0.0246 #&gt; F-statistic: 0.0379 on 1 and 39 DF, p-value: 0.847 plot(womensrole$sex,womensrole$percentage_agree) abline(fit.object, col = &quot;red&quot;) ## continuous predictor -&gt; catergorical Outcome # #Predict classification of being &quot;short movie&quot; from length of running time options(scipen=10) # to remove scientific notation from print out fit.object &lt;- glm(data=movies,as.factor(Short) ~ length,family=binomial()) #&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(fit.object) #&gt; #&gt; Call: #&gt; glm(formula = as.factor(Short) ~ length, family = binomial(), #&gt; data = movies) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.592 -0.006 -0.002 0.000 8.490 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 10.79235 0.28567 37.8 &lt;2e-16 *** #&gt; length -0.24966 0.00621 -40.2 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 51866.5 on 58787 degrees of freedom #&gt; Residual deviance: 1721.5 on 58786 degrees of freedom #&gt; AIC: 1726 #&gt; #&gt; Number of Fisher Scoring iterations: 12 plot(as.factor(movies$Short),movies$length) abline(fit.object, col = &quot;red&quot;) exp(coef(fit.object)) #&gt; (Intercept) length #&gt; 48647.045 0.779 ## catergorical predictor -&gt; catergorical Outcome # #Predict classification of being &quot;short movie&quot; from whether it is an animated film or not fit.object &lt;- glm(data=movies,as.factor(Short) ~ as.factor(Animation),family=binomial()) summary(fit.object) #&gt; #&gt; Call: #&gt; glm(formula = as.factor(Short) ~ as.factor(Animation), family = binomial(), #&gt; data = movies) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.929 -0.494 -0.494 -0.494 2.079 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.0396 0.0133 -152.8 &lt;2e-16 *** #&gt; as.factor(Animation)1 3.7313 0.0473 78.8 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 51866 on 58787 degrees of freedom #&gt; Residual deviance: 42536 on 58786 degrees of freedom #&gt; AIC: 42540 #&gt; #&gt; Number of Fisher Scoring iterations: 4 exp(coef(fit.object)) #&gt; (Intercept) as.factor(Animation)1 #&gt; 0.13 41.73 64.1 Descriptive Statistics 64.1.1 Measures of Central tendency / location Central tendency aims to capture the center of the distribution. # Display the normal distributions with various means x &lt;- seq(-80, 80, length=1000) hx &lt;- dnorm(x) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;,&quot;green&quot;, &quot;purple&quot;, &quot;black&quot;) plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;,ylab=&quot;Density&quot;, main=&quot;Distributions with Different Means&quot;,xlim=c(-7, 7)) location&lt;-c(2,4,-2,-4) labels=paste0(&quot;Mean = &quot;,location) labels[length(labels)+1]=&quot;Mean = 0&quot; for (i in 1:length(location)){ lines(x, dnorm(x,mean=location[i]), lwd=1, col=colors[i]) } legend(&quot;topright&quot;, inset=.05, title=&quot;Distributions&quot;, labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors) 64.2 Specific Measures of Central Tendency Measures of Central Tendency Mean Median Mode 64.2.1 Central tendency: Mean Mean (\\(\\mu\\); \\(\\bar{X}\\)) arithmetic average \\(\\bar{X}\\) is used for samples Mu (\\(\\mu\\)) is used for population \\(\\bar{X}= \\frac{1}{n} \\sum^{n}_{i=1}x_{i}\\) Properties Is the balance point of the distribution (in terms of center of mass) \\(\\sum^{n}_{i=1}(x_{i}-\\bar{x})=0\\) Least squares property The sum of squared deviations about the mean is small highly sensitive to outliers (extreme scores) (weakness; it means that the mean is not so good as a measure of central tendency in highly skewed distributions) Is not a robust statistic (low robust = sensitive to outliers; high robust = not sensitive to outliers) Very good with quantitative data (interval and ratio data, especially bell shaped distributions) Very popular statistic library(pscl) data(prussian) #horse kick fatalities by year prussian$y #&gt; [1] 0 2 2 1 0 0 1 1 0 3 0 2 1 0 0 1 0 1 0 1 0 0 0 2 0 3 0 2 0 0 0 1 1 1 0 2 0 #&gt; [38] 3 1 0 0 0 0 2 0 2 0 0 1 1 0 0 2 1 1 0 0 2 0 0 0 0 0 1 1 1 2 0 2 0 0 0 1 0 #&gt; [75] 1 2 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 2 1 0 0 1 0 0 #&gt; [112] 1 0 1 1 1 1 1 1 0 0 0 1 0 2 0 0 1 2 0 1 1 3 1 1 1 0 3 0 0 1 0 1 0 0 0 1 0 #&gt; [149] 1 1 0 0 2 0 0 2 1 0 2 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 #&gt; [186] 2 1 1 1 0 2 1 1 0 1 2 0 1 0 0 0 0 1 1 0 1 0 2 0 2 0 0 0 0 2 1 3 0 1 1 0 0 #&gt; [223] 0 0 2 4 0 1 3 0 1 1 1 1 2 1 3 1 3 1 1 1 2 1 1 3 0 4 0 1 0 3 2 1 0 2 1 1 0 #&gt; [260] 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 2 2 0 0 0 0 variable&lt;-prussian$y # Mean mean(variable) #&gt; [1] 0.7 64.2.2 Central Tendency: Median Median (Md) Def: central score in a distribution If n is odd then Med = value of the \\(\\frac{n+1}{2}\\) item term. If n is even then Med = average of the \\(\\frac{n+}{2}\\) and \\(\\frac{n+1}{2}\\) item terms. Properties Balance point of scores Highly robust to outliers (less sensitive than the mean to outliers) Sum of absolute deviations is smaller than any other constant (c) \\(\\sum^{n}_{i=1}(\\left|X-c\\right|)\\) Often used for ordinal data #Median median(variable) #&gt; [1] 0 # By hand # Sample Size? (sample.size=length(variable)) #&gt; [1] 280 # Even or Odd ## Test if number is divisible by 2. ## If yes, then even ## Else, is odd sample.size %% 2 == 0 #&gt; [1] TRUE # Sort our values variable=sort(variable) # if odd, grab the midpoint value sample size / 2 variable[sample.size/2] #&gt; [1] 0 # if even grab the average of the midpoint values sample size / 2 and sample size+1 / 2 (variable[sample.size/2] + variable[1+sample.size/2])/2 #&gt; [1] 0 64.2.3 Mode Mode: most common score Local modes are the highest point with a subset of the distribution, there can be multiple ones #Mode mode(variable) # That&#39;s not mode! #&gt; [1] &quot;numeric&quot; # Function to examine mode Mode &lt;- function(x) { ux &lt;- unique(x) #finds all unique values ux[which.max(tabulate(match(x, ux)))] #returns the value which is most frequent } Mode(variable) #&gt; [1] 0 64.3 Relationship between mean, median, and mode When we have a symmetric unimodal distribution Mean=median=mode positively skewed mode &lt; median &lt; mean negatively skewed mean &lt; median &lt; mode knitr::include_graphics(&quot;img/mmmskew.jpg&quot;) ## summary hist(variable) library(Hmisc) describe(variable) #&gt; variable #&gt; n missing distinct Info Mean Gmd #&gt; 280 0 5 0.828 0.7 0.8752 #&gt; #&gt; lowest : 0 1 2 3 4, highest: 0 1 2 3 4 #&gt; #&gt; Value 0 1 2 3 4 #&gt; Frequency 144 91 32 11 2 #&gt; Proportion 0.514 0.325 0.114 0.039 0.007 64.4 Variability/spread Variability describes how spread out the data are from that center Low variance has a less wide distribution, with the bulk of the mass in the center High variance has a very wide distribution, bulk of distribution is spread out # Display the normal distributions with various standard deviations plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Distributions with Different Standard Deviations&quot;,xlim=c(-7, 7)) spread=c(.5,2,4) labels=paste0(&quot;SD = &quot;,spread) labels[length(labels)+1]=&quot;SD = 1&quot; for (i in 1:length(spread)){ lines(x, dnorm(x,sd=spread[i]), lwd=1, col=colors[i]) } legend(&quot;topright&quot;, inset=.05, title=&quot;Distributions&quot;, labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors[c(1:3,5)]) ## Measures of Spread around the Median Range maximum value - minimum value \\(max(x_{i})-min(x_{i})\\) Non-robust to outliers Quartiles Lower quartile (Q1), 25th percentile Second quartile (Q2), 50th percentile / median Upper quartile (Q3), 75th percentile Interquartile Range (IQR) Q3-Q1 Sometimes called h-spread; h = hinges 64.4.1 R Examples # Range range(variable) #&gt; [1] 0 4 max(variable) -min(variable) #&gt; [1] 4 # 5 Number Summary / Quartiles summary(variable) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0 0.0 0.0 0.7 1.0 4.0 64.5 Spread around the Mean Variance (\\(\\sigma^{2}\\);\\(s^{2}\\)) Measure of spread around the mean Goal of the measure to use every score \\(\\sigma^{2}\\) = \\(\\frac{\\sum^{n}_{i=1}(x_{i}-\\mu)^{2}}{N}\\) \\(s^{2}\\) = \\(\\frac{\\sum^{n}_{i=1}(x_{i}-\\bar{x})^{2}}{n-1}\\) Standard Deviation (\\(\\sigma\\);s) \\(\\sigma\\) = \\(\\sqrt{\\sigma^{2}}\\) s = \\(\\sqrt{s^{2}}\\) 64.6 R Examples # Variance var(variable) #&gt; [1] 0.763 # Standard Deviation sqrt(var(variable)) #&gt; [1] 0.873 sd(variable) #&gt; [1] 0.873 64.7 Bessels Correction \\(s^{2}\\) is nearly the average squared deviation \\(s^{2}\\) uses n-1 instead of N Otherwise we get biased estimates This adjustment is called Bessels correction In our formula, we are using the sample mean (x) instead of the true mean (\\(\\mu\\)) this results in underestimating each \\(x_{i}  \\mu\\) by \\(x  \\mu\\). Properties Std is in raw score units Both the variance and standard deviation are highly NOT robust "],["welcome-to-prediction-and-overfitting.html", "65 Welcome to prediction and overfitting 65.1 Module Materials", " 65 Welcome to prediction and overfitting This module expands upon ideas introduced in the previous module. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 65.1 Module Materials Slides Modeling non-linear relationships Models with multiple predictors Suggested Readings All subchapters of this module Lab 10 "],["modeling-non-linear-relationships.html", "66 Modeling non-linear relationships", " 66 Modeling non-linear relationships You can follow along with the slides here if they do not appear below. "],["modeling-with-multiple-predictors.html", "67 Modeling with multiple predictors 67.1 The linear model with multiple predictors 67.2 Two numerical predictors", " 67 Modeling with multiple predictors You can follow along with the slides here if they do not appear below. 67.1 The linear model with multiple predictors 67.2 Two numerical predictors "],["notes-on-logistic-regression.html", "68 Notes on Logistic Regression 68.1 Predicting categorical data 68.2 Sensitivity and specificity", " 68 Notes on Logistic Regression 68.1 Predicting categorical data 68.1.0.1 Spam filters Data from 3921 emails and 21 variables on them Outcome: whether the email is spam or not Predictors: number of characters, whether the email had Re: in the subject, time at which email was sent, number of times the word inherit shows up in the email, etc. library(openintro) data(email) glimpse(email) #&gt; Rows: 3,921 #&gt; Columns: 21 #&gt; $ spam &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, ~ #&gt; $ sent_email &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, ~ #&gt; $ time &lt;dttm&gt; 2012-01-01 01:16:41, 2012-01-01 02:03:59, 2012-01-01 11:~ #&gt; $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ dollar &lt;dbl&gt; 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, ~ #&gt; $ winner &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n~ #&gt; $ inherit &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ password &lt;dbl&gt; 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~ #&gt; $ num_char &lt;dbl&gt; 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421~ #&gt; $ line_breaks &lt;int&gt; 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191~ #&gt; $ format &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, ~ #&gt; $ re_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, ~ #&gt; $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~ #&gt; $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ #&gt; $ exclaim_mess &lt;dbl&gt; 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0~ #&gt; $ number &lt;fct&gt; big, small, small, small, none, none, big, small, small, ~ Question: Would you expect longer or shorter emails to be spam? #&gt; Picking joint bandwidth of 1.18 #&gt; # A tibble: 2 x 2 #&gt; spam mean_num_char #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 0 11.3 #&gt; 2 1 5.44 Question: Would you expect emails that have subjects starting with Re: RE: re: or rE: to be spam or not? 68.1.0.2 Modeling spam Both number of characters and whether the message has re: in the subject might be related to whether the email is spam. How do we come up with a model that will let us explore this relationship? For simplicity, well focus on the number of characters (num_char) as predictor, but the model we describe can be expanded to take multiple predictors as well. This isnt something we can reasonably fit a linear model to  we need something different! 68.1.0.3 Framing the problem We can treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials Bernoulli trial: a random experiment with exactly two possible outcomes, success and failure, in which the probability of success is the same every time the experiment is conducted Each Bernoulli trial can have a separate probability of success \\[ y_i  Bern(p) \\] We can then use the predictor variables to model that probability of success, \\(p_i\\) We cant just use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range 68.1.0.4 Generalized linear models This is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs) Logistic regression is just one example 68.1.0.5 Three characteristics of GLMs All GLMs have the following three characteristics: A probability distribution describing a generative model for the outcome variable A linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\] A link function that relates the linear model to the parameter of the outcome distribution 68.1.0.6 Logistic regression Logistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors To finish specifying the Logistic model we just need to define a reasonable link function that connects \\(\\eta_i\\) to \\(p_i\\): logit function Logit function: For \\(0\\le p \\le 1\\) \\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\] 68.1.0.7 Logit function, visualized d &lt;- tibble(p = seq(0.001, 0.999, length.out = 1000)) %&gt;% mutate(logit_p = log(p/(1-p))) ggplot(d, aes(x = p, y = logit_p)) + geom_line() + xlim(0,1) + ylab(&quot;logit(p)&quot;) + labs(title = &quot;logit(p) vs. p&quot;) 68.1.0.8 Properties of the logit The logit function takes a value between 0 and 1 and maps it to a value between \\(-\\infty\\) and \\(\\infty\\) Inverse logit (logistic) function: \\[g^{-1}(x) = \\frac{\\exp(x)}{1+\\exp(x)} = \\frac{1}{1+\\exp(-x)}\\] The inverse logit function takes a value between \\(-\\infty\\) and \\(\\infty\\) and maps it to a value between 0 and 1 This formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success  more on this later 68.1.0.9 The logistic regression model Based on the three GLM criteria, we have \\(y_i \\sim \\text{Bern}(p_i)\\) \\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\) \\(\\text{logit}(p_i) = \\eta_i\\) From which we get \\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\] 68.1.0.10 Modeling spam In R, we fit a GLM in the same way as a linear model except we specify the model with logistic_reg() use \"glm\" instead of \"lm\" as the engine define family = \"binomial\" for the link function to be used in the model spam_fit &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% fit(spam ~ num_char, data = email, family = &quot;binomial&quot;) tidy(spam_fit) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -1.80 0.0716 -25.1 2.04e-139 #&gt; 2 num_char -0.0621 0.00801 -7.75 9.50e- 15 68.1.0.11 Spam model tidy(spam_fit) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -1.80 0.0716 -25.1 2.04e-139 #&gt; 2 num_char -0.0621 0.00801 -7.75 9.50e- 15 Model: \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times \\text{num_char}\\] 68.1.0.12 P(spam) for an email with 2000 characters \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times 2\\] \\[\\frac{p}{1-p} = \\exp(-1.9242) = 0.15 \\rightarrow p = 0.15 \\times (1 - p)\\] \\[p = 0.15 - 0.15p \\rightarrow 1.15p = 0.15\\] \\[p = 0.15 / 1.15 = 0.13\\] Question: What is the probability that an email with 15000 characters is spam? What about an email with 40000 characters? 2K chars: P(spam) = 0.13 15K chars, P(spam) = 0.06 40K chars, P(spam) = 0.01 Question: Would you prefer an email with 2000 characters to be labeled as spam or not? How about 40,000 characters? 68.2 Sensitivity and specificity 68.2.0.1 False positive and negative Email is spam Email is not spam Email labeled spam True positive False positive (Type 1 error) Email labeled not spam False negative (Type 2 error) True negative False negative rate = P(Labeled not spam | Email spam) = FN / (TP + FN) False positive rate = P(Labeled spam | Email not spam) = FP / (FP + TN) 68.2.0.2 Sensitivity and specificity Email is spam Email is not spam Email labeled spam True positive False positive (Type 1 error) Email labeled not spam False negative (Type 2 error) True negative Sensitivity = P(Labeled spam | Email spam) = TP / (TP + FN) Sensitivity = 1  False negative rate Specificity = P(Labeled not spam | Email not spam) = TN / (FP + TN) Specificity = 1  False positive rate Question: If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision? "],["lab10.html", "69 Lab 10: Modeling with multiple predictors 69.1 Professor attractiveness and course evaluations, Pt. 2 69.2 Getting started 69.3 Warm up 69.4 The data 69.5 Exercises", " 69 Lab 10: Modeling with multiple predictors 69.1 Professor attractiveness and course evaluations, Pt. 2 In this lab, we revisit the professor evaluations data we modeled in the previous lab. In the last lab, we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores. If you dont remember the data, review the previous labs introduction before continuing to the exercises. 69.2 Getting started 69.2.1 Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 69.2.2 Housekeeping 69.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 69.2.2.2 Project name Update the name of your project to match the labs title. 69.3 Warm up Before we introduce the data, lets warm up with some simple exercises. 69.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 69.3.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 69.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 69.5 Exercises Load the data by including the appropriate code in your R Markdown file. 69.5.1 Part 1: Simple linear regression Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). 69.5.2 Part 2: Multiple linear regression Fit a linear model (one you have fit before): m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). Interpret the slope and intercept of m_bty_gen in context of the data. What percent of the variability in score is explained by the model m_bty_gen. What is the equation of the line corresponding to just male professors? For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score? How does the relationship between beauty and evaluation scorevary between male and female professors? How do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beaty score of the professor. Compare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg? Create a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data. 69.5.3 Part 3: The search for the best model Going forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg. Which variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professors score. Check your suspicions from the previous exercise. Include the model outputfor that variable in your response. Suppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why? Fit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question. Using backward-selection with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on. Interpret the slopes of one numerical and one categorical predictor based on your final model. Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a highb evaluation score. Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not? "],["welcome-to-cross-validation.html", "70 Welcome to Cross Validation 70.1 Module Materials", " 70 Welcome to Cross Validation This module is designed to introduce you to cross-validation. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. This week is a little bit delayed Don&#39;t do it. But also pic.twitter.com/yrsvU0Xj1K&mdash; Bill Chopik (@Chops310) April 14, 2021 70.1 Module Materials Slides from Lectures Overfitting Cross-validation Suggested Readings All subchapters of this module Articles de Rooij, M., &amp; Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248-263. MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40. R4DS Many Models "],["overfitting.html", "71 Overfitting! 71.1 Prediction 71.2 Workflow", " 71 Overfitting! You can follow along with the slides here if they do not appear below. 71.1 Prediction 71.2 Workflow "],["notes-on-feature-engineering.html", "72 Notes on Feature Engineering 72.1 Feature engineering 72.2 Modeling workflow, revisited 72.3 Building recipes 72.4 Building workflows 72.5 Making decisions", " 72 Notes on Feature Engineering 72.1 Feature engineering We prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity Variables that go into the model and how they are represented are just as critical to success of the model Feature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance) 72.1.1 Same training and testing sets as before # Fix random numbers by setting the seed # Enables analysis to be reproducible when random numbers are used set.seed(1066) # Put 80% of the data into the training set email_split &lt;- initial_split(email, prop = 0.80) # Create data frames for the two sets: train_data &lt;- training(email_split) test_data &lt;- testing(email_split) 72.1.2 A simple approach: mutate() train_data %&gt;% mutate( date = date(time), dow = wday(time), month = month(time) ) %&gt;% select(time, date, dow, month) %&gt;% sample_n(size = 5) # shuffle to show a variety #&gt; # A tibble: 5 x 4 #&gt; time date dow month #&gt; &lt;dttm&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2012-01-27 09:07:50 2012-01-27 6 1 #&gt; 2 2012-03-22 04:25:38 2012-03-22 5 3 #&gt; 3 2012-03-16 22:06:15 2012-03-16 6 3 #&gt; 4 2012-03-05 15:26:57 2012-03-05 2 3 #&gt; 5 2012-02-13 12:31:43 2012-02-13 2 2 72.2 Modeling workflow, revisited Create a recipe for feature engineering steps to be applied to the training data Fit the model to the training data after these steps have been applied Using the model estimates from the training data, predict outcomes for the test data Evaluate the performance of the model on the test data 72.3 Building recipes 72.3.1 Initiate a recipe email_rec &lt;- recipe( spam ~ ., # formula data = train_data # data to use for cataloguing names and types of variables ) summary(email_rec) #&gt; # A tibble: 21 x 4 #&gt; variable type role source #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 to_multiple numeric predictor original #&gt; 2 from numeric predictor original #&gt; 3 cc numeric predictor original #&gt; 4 sent_email numeric predictor original #&gt; 5 time date predictor original #&gt; 6 image numeric predictor original #&gt; 7 attach numeric predictor original #&gt; 8 dollar numeric predictor original #&gt; 9 winner nominal predictor original #&gt; 10 inherit numeric predictor original #&gt; 11 viagra numeric predictor original #&gt; 12 password numeric predictor original #&gt; 13 num_char numeric predictor original #&gt; 14 line_breaks numeric predictor original #&gt; 15 format numeric predictor original #&gt; 16 re_subj numeric predictor original #&gt; 17 exclaim_subj numeric predictor original #&gt; 18 urgent_subj numeric predictor original #&gt; 19 exclaim_mess numeric predictor original #&gt; 20 number nominal predictor original #&gt; 21 spam nominal outcome original 72.3.2 Remove certain variables email_rec &lt;- email_rec %&gt;% step_rm(from, sent_email) #&gt; Data Recipe #&gt; #&gt; Inputs: #&gt; #&gt; role #variables #&gt; outcome 1 #&gt; predictor 20 #&gt; #&gt; Operations: #&gt; #&gt; Delete terms from, sent_email 72.3.3 Feature engineer date email_rec &lt;- email_rec %&gt;% step_date(time, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% step_rm(time) #&gt; Data Recipe #&gt; #&gt; Inputs: #&gt; #&gt; role #variables #&gt; outcome 1 #&gt; predictor 20 #&gt; #&gt; Operations: #&gt; #&gt; Delete terms from, sent_email #&gt; Date features from time #&gt; Delete terms time 72.3.4 Discretize numeric variables Procedue with major caution! And please be sure to read MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7, 19-40. and play around with the demo data from Kriss website: http://www.quantpsy.org/mzpr.htm email_rec &lt;- email_rec %&gt;% step_cut(cc, attach, dollar, breaks = c(0, 1)) %&gt;% step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) #&gt; Data Recipe #&gt; #&gt; Inputs: #&gt; #&gt; role #variables #&gt; outcome 1 #&gt; predictor 20 #&gt; #&gt; Operations: #&gt; #&gt; Delete terms from, sent_email #&gt; Date features from time #&gt; Delete terms time #&gt; Cut numeric for cc, attach, dollar #&gt; Cut numeric for inherit, password 72.3.5 Create dummy variables email_rec &lt;- email_rec %&gt;% step_dummy(all_nominal(), -all_outcomes()) #&gt; Data Recipe #&gt; #&gt; Inputs: #&gt; #&gt; role #variables #&gt; outcome 1 #&gt; predictor 20 #&gt; #&gt; Operations: #&gt; #&gt; Delete terms from, sent_email #&gt; Date features from time #&gt; Delete terms time #&gt; Cut numeric for cc, attach, dollar #&gt; Cut numeric for inherit, password #&gt; Dummy variables from all_nominal(), -all_outcomes() 72.3.6 Remove zero variance variables Variables that contain only a single value email_rec &lt;- email_rec %&gt;% step_zv(all_predictors()) #&gt; Data Recipe #&gt; #&gt; Inputs: #&gt; #&gt; role #variables #&gt; outcome 1 #&gt; predictor 20 #&gt; #&gt; Operations: #&gt; #&gt; Delete terms from, sent_email #&gt; Date features from time #&gt; Delete terms time #&gt; Cut numeric for cc, attach, dollar #&gt; Cut numeric for inherit, password #&gt; Dummy variables from all_nominal(), -all_outcomes() #&gt; Zero variance filter on all_predictors() 72.3.7 All in one place email_rec &lt;- recipe(spam ~ ., data = email) %&gt;% step_rm(from, sent_email) %&gt;% step_date(time, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% step_rm(time) %&gt;% step_cut(cc, attach, dollar, breaks = c(0, 1)) %&gt;% step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) 72.4 Building workflows 72.4.1 Define model email_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) email_mod #&gt; Logistic Regression Model Specification (classification) #&gt; #&gt; Computational engine: glm 72.4.2 Define workflow Workflows bring together models and recipes so that they can be easily applied to both the training and test data. email_wflow &lt;- workflow() %&gt;% add_model(email_mod) %&gt;% add_recipe(email_rec) #&gt; == Workflow ======================================================================================== #&gt; Preprocessor: Recipe #&gt; Model: logistic_reg() #&gt; #&gt; -- Preprocessor ------------------------------------------------------------------------------------ #&gt; 7 Recipe Steps #&gt; #&gt; * step_rm() #&gt; * step_date() #&gt; * step_rm() #&gt; * step_cut() #&gt; * step_cut() #&gt; * step_dummy() #&gt; * step_zv() #&gt; #&gt; -- Model ------------------------------------------------------------------------------------------- #&gt; Logistic Regression Model Specification (classification) #&gt; #&gt; Computational engine: glm 72.4.3 Fit model to training data email_fit &lt;- email_wflow %&gt;% fit(data = train_data) tidy(email_fit) %&gt;% print(n = 31) #&gt; # A tibble: 31 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -1.23 0.272 -4.51 6.56e- 6 #&gt; 2 to_multiple -2.83 0.396 -7.14 9.32e-13 #&gt; 3 image -1.17 0.638 -1.83 6.72e- 2 #&gt; 4 viagra 2.30 182. 0.0127 9.90e- 1 #&gt; 5 num_char 0.0691 0.0227 3.04 2.34e- 3 #&gt; 6 line_breaks -0.00581 0.00135 -4.31 1.66e- 5 #&gt; 7 format -0.779 0.160 -4.88 1.06e- 6 #&gt; 8 re_subj -2.82 0.408 -6.92 4.58e-12 #&gt; 9 exclaim_subj -0.203 0.295 -0.689 4.91e- 1 #&gt; 10 urgent_subj 3.73 1.02 3.65 2.66e- 4 #&gt; 11 exclaim_mess 0.00791 0.00177 4.47 8.00e- 6 #&gt; 12 cc_X.1.68. 0.250 0.441 0.568 5.70e- 1 #&gt; 13 attach_X.1.20. 2.03 0.406 5.00 5.86e- 7 #&gt; 14 dollar_X.1.64. -0.0614 0.231 -0.265 7.91e- 1 #&gt; 15 winner_yes 1.88 0.423 4.44 8.99e- 6 #&gt; 16 inherit_X.1.5. -9.18 763. -0.0120 9.90e- 1 #&gt; 17 inherit_X.5.10. 3.06 1.46 2.09 3.66e- 2 #&gt; 18 password_X.1.5. -1.61 0.743 -2.17 2.98e- 2 #&gt; 19 password_X.5.10. -12.7 499. -0.0254 9.80e- 1 #&gt; 20 password_X.10.20. -13.9 803. -0.0173 9.86e- 1 #&gt; 21 password_X.20.28. -13.9 803. -0.0173 9.86e- 1 #&gt; 22 number_small -1.02 0.170 -5.98 2.30e- 9 #&gt; 23 number_big -0.00783 0.240 -0.0326 9.74e- 1 #&gt; 24 time_dow_Mon 0.212 0.316 0.670 5.03e- 1 #&gt; 25 time_dow_Tue 0.595 0.290 2.06 3.98e- 2 #&gt; 26 time_dow_Wed 0.0847 0.297 0.286 7.75e- 1 #&gt; 27 time_dow_Thu 0.415 0.295 1.41 1.59e- 1 #&gt; 28 time_dow_Fri 0.393 0.293 1.34 1.81e- 1 #&gt; 29 time_dow_Sat 0.496 0.319 1.55 1.20e- 1 #&gt; 30 time_month_Feb 0.767 0.184 4.16 3.14e- 5 #&gt; 31 time_month_Mar 0.554 0.184 3.00 2.68e- 3 72.4.4 Make predictions for test data email_pred &lt;- predict(email_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(test_data) #&gt; Warning: There are new levels in a factor: NA email_pred #&gt; # A tibble: 784 x 23 #&gt; .pred_0 .pred_1 spam to_multiple from cc sent_email time image attach dollar #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.996 0.00361 0 1 1 0 1 2012-01-01 12:55:06 0 0 0 #&gt; 2 0.919 0.0813 0 0 1 0 0 2012-01-01 16:08:59 0 0 0 #&gt; 3 0.957 0.0435 0 0 1 0 1 2012-01-01 13:23:44 0 0 0 #&gt; 4 0.999 0.00114 0 0 1 1 1 2012-01-01 14:38:32 0 0 0 #&gt; 5 0.994 0.00550 0 0 1 0 1 2012-01-01 20:40:40 0 0 0 #&gt; 6 0.994 0.00602 0 0 1 0 1 2012-01-01 21:07:20 0 0 0 #&gt; 7 0.888 0.112 0 0 1 0 0 2012-01-01 21:08:14 0 0 0 #&gt; 8 0.994 0.00570 0 0 1 0 0 2012-01-01 21:51:24 0 0 0 #&gt; 9 0.995 0.00521 0 0 1 1 0 2012-01-01 22:16:39 0 0 0 #&gt; 10 1.00 0.000249 0 1 1 3 0 2012-01-02 08:41:11 0 0 0 #&gt; # ... with 774 more rows, and 12 more variables: winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, #&gt; # password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;dbl&gt;, re_subj &lt;dbl&gt;, #&gt; # exclaim_subj &lt;dbl&gt;, urgent_subj &lt;dbl&gt;, exclaim_mess &lt;dbl&gt;, number &lt;fct&gt; 72.4.5 Evaluate the performance email_pred %&gt;% roc_curve( truth = spam, .pred_1, event_level = &quot;second&quot; ) %&gt;% autoplot() email_pred %&gt;% roc_auc( truth = spam, .pred_1, event_level = &quot;second&quot; ) #&gt; # A tibble: 1 x 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 roc_auc binary 0.865 72.5 Making decisions 72.5.1 Cutoff probability: 0.5 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5. cutoff_prob &lt;- 0.5 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 699 68 Email labelled spam 6 10 NA 1 NA 72.5.2 Cutoff probability: 0.25 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25. cutoff_prob &lt;- 0.25 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 667 49 Email labelled spam 38 29 NA 1 NA 72.5.3 Cutoff probability: 0.75 Suppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75. cutoff_prob &lt;- 0.75 email_pred %&gt;% mutate( spam = if_else(spam == 1, &quot;Email is spam&quot;, &quot;Email is not spam&quot;), spam_pred = if_else(.pred_1 &gt; cutoff_prob, &quot;Email labelled spam&quot;, &quot;Email labelled not spam&quot;) ) %&gt;% count(spam_pred, spam) %&gt;% pivot_wider(names_from = spam, values_from = n) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Email is not spam&quot;, &quot;Email is spam&quot;)) Email is not spam Email is spam Email labelled not spam 704 72 Email labelled spam 1 6 NA 1 NA "],["cross-validation.html", "73 Cross-Validation 73.1 V-Fold", " 73 Cross-Validation You can follow along with the slides here if they do not appear below. 73.1 V-Fold "],["odd-notes-on-cross-validation.html", "74 ODD: Notes on Cross validation 74.1 Example: Regression 74.2 Example: Mixture models 74.3 Better Solution: Cross validation 74.4 Example 74.5 Choice of \\(K\\) 74.6 Summing up", " 74 ODD: Notes on Cross validation A too technical and optional deep dive set of notes on cross-validation. We have: Data \\(X_1, \\ldots, X_n\\). A tuning parameter \\(\\theta\\). Each value of \\(\\theta\\) corresponds to a different set of models. A function \\(L\\) that takes a fitted model and a data point and returns a measure of model quality. We would like to choose one model from the set of candidate models indexed by \\(\\theta\\). 74.1 Example: Regression Data: Pairs of predictors and response variables, \\((y_i, X_i)\\), \\(i = 1,\\ldots, n\\), \\(y_i \\in \\mathbb R\\), \\(X_i \\in \\mathbb R^p\\) Models: \\(y_i = X \\beta + \\epsilon\\), \\(\\beta_j = 0, j \\in S_\\theta\\), where \\(S_\\theta \\subseteq \\{1,\\ldots, p\\}\\). Model quality: Squared-error loss. If \\(\\hat \\beta_\\theta\\) are our estimates of the regression coefficients in model \\(\\theta\\), model quality is measured by \\[ L(\\hat \\beta_\\theta, (y_i, X_i)) = (y_i - X_i^T \\hat \\beta_\\theta)^2 \\] We want to choose a subset of the predictors that do the best job of explaining the response. Naive solution: Find the model that has the lowest value for the squared-error loss. Why doesnt this work? 74.2 Example: Mixture models Data: \\(x_1,\\ldots, x_n\\), \\(x_i \\in \\mathbb R\\) Models: Gaussian mixture models with \\(\\theta\\) mixture components. Model quality: Negative log likelihood of the data. If \\(\\hat p_\\theta\\) is the density of the fitted model with \\(\\theta\\) components, model quality is measured by \\(L(\\hat p_\\theta, x_i) = -\\log \\hat p_\\theta(x_i)\\). We want to choose the number of mixture components that best explains the data. Naive solution: Choose the number of mixture components that minimizes the negative log likelihood of the data. 74.3 Better Solution: Cross validation Idea: Instead of measuring model quality on the same data we used to fit the model, we estimate model quality on new data. If we knew the true distribution of the data, we could simulate new data and use a Monte Carlo estimate based on the simulations. We cant actually get new data, and so we hold some back when we fit the model and then pretend that the held back data is new data. Procedure: Divide the data into \\(K\\) folds Let \\(X^{(k)}\\) denote the data in fold \\(k\\), and let \\(X^{(-k)}\\) denote the data in all the folds except for \\(k\\). For each fold and each value of the tuning parameter \\(\\theta\\), fit the model on \\(X^{(-k)}\\) to get \\(\\hat f_\\theta^{(k)}\\) Compute \\[ \\text{CV}(\\theta) = \\frac{1}{n} \\sum_{k=1}^K \\sum_{x \\in X^{(k)}} L(\\hat f_\\theta^{(k)}, x) \\] Choose \\(\\hat \\theta = \\text{argmin}_{\\theta} \\text{CV}(\\theta)\\) 74.4 Example n = 100 p = 20 X = matrix(rnorm(n * p), nrow = n) y = rnorm(n) get_rss_submodels = function(n_predictors, y, X) { if(n_predictors == 0) { lm_submodel = lm(y ~ 0) } else { lm_submodel = lm(y ~ 0 + X[,1:n_predictors, drop = FALSE]) } return(sum(residuals(lm_submodel)^2)) } p_vec = 0:p rss = sapply(p_vec, get_rss_submodels, y, X) plot(rss ~ p_vec) get_cv_error = function(n_predictors, y, X, folds) { cv_vec = numeric(length(unique(folds))) for(f in unique(folds)) { cv_vec[f] = rss_on_held_out( n_predictors, y_train = y[folds != f], X_train = X[folds != f,], y_test = y[folds == f], X_test = X[folds == f,]) } return(mean(cv_vec)) } rss_on_held_out = function(n_predictors, y_train, X_train, y_test, X_test) { if(n_predictors == 0) { lm_submodel = lm(y_train ~ 0) preds_on_test = rep(0, length(y_test)) } else { lm_submodel = lm(y_train ~ 0 + X_train[,1:n_predictors, drop = FALSE]) preds_on_test = X_test[,1:n_predictors, drop= FALSE] %*% coef(lm_submodel) } return(sum((y_test - preds_on_test)^2)) } K = 5 ## normally you would do this at random folds = rep(1:K, each = n / K) p_vec = 0:p cv_errors = sapply(p_vec, get_cv_error, y, X, folds) plot(cv_errors ~ p_vec) 74.5 Choice of \\(K\\) Considerations: Larger \\(K\\) means more computation (although sometimes there is a shortcut for leave-one-out cross validation) Larger \\(K\\) means less bias in the estimate of model accuracy Larger \\(K\\) also means more variance in the estimate, so we dont necessarily want \\(K = n\\) Usually choose \\(K = 5\\) or \\(K = 10\\) If your problem is structured (e.g. time series, spatial), you should choose the folds to respect the structure. 74.6 Summing up We can use simulations to estimate arbitrary functions of our random variables. If we know the underlying distribution, we can simply simulate from it (Monte Carlo integration). If we dont know the underlying distribution, we can simulate from the data by resampling from the data (cross validation; ish). Resampling methods will do well to the extent that the observed data reflect the true data-generating distribution. "],["welcome-to-quantifying-uncertainty.html", "75 Welcome to Quantifying Uncertainty 75.1 Module Materials", " 75 Welcome to Quantifying Uncertainty This module is designed to introduce ideas related to representing uncertainty. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 75.1 Module Materials Slides from Lectures Quantifying Uncertainty Bootstrapping Suggested Readings All subchapters of this module About the Needle * The NYTs election forecast needle is stressing people out with fake jitter * Kopf, D. (2020, November 3). Why its okay to look at the New York Times election needle. Quartz "],["quantifying-uncertainty.html", "76 Quantifying Uncertainty!", " 76 Quantifying Uncertainty! You can follow along with the slides here if they do not appear below. "],["bootstrapping.html", "77 Bootstrapping", " 77 Bootstrapping You can follow along with the slides here if they do not appear below. "],["notes-on-hypothesis-testing.html", "78 Notes on Hypothesis Testing 78.1 Hypothesis testing for a single proportion 78.2 One vs. two sided hypothesis tests 78.3 Testing for independence", " 78 Notes on Hypothesis Testing 78.1 Hypothesis testing for a single proportion ## Packages library(tidyverse) library(tidymodels) 78.1.1 Organ donors People providing an organ for donation sometimes seek the help of a special medical consultant. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultants clients. One consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!). 78.1.2 Data organ_donor %&gt;% count(outcome) #&gt; # A tibble: 2 x 2 #&gt; outcome n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 complication 3 #&gt; 2 no complication 59 78.1.3 Parameter vs. statistic A parameter for a hypothesis test is the true value of interest. We typically estimate the parameter using a sample statistic as a point estimate. \\(p~\\): true rate of complication \\(\\hat{p}~\\): rate of complication in the sample = \\(\\frac{3}{62}\\) = 0.048 78.1.4 Correlation vs. causation Is it possible to assess the consultants claim using the data? No. The claim is that there is a causal connection, but the data are observational. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate. Although it is not possible to assess the causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of \\(\\hat{p}\\) = 0.048 be due to chance? 78.1.5 Two claims Null hypothesis: There is nothing going on Complication rate for this consultant is no different than the US average of 10% Alternative hypothesis: There is something going on Complication rate for this consultant is lower than the US average of 10% 78.1.6 Hypothesis testing as a court trial Null hypothesis, \\(H_0\\): Defendant is innocent Alternative hypothesis, \\(H_A\\): Defendant is guilty Present the evidence: Collect data Judge the evidence: Could these data plausibly have happened by chance if the null hypothesis were true? Yes: Fail to reject \\(H_0\\) No: Reject \\(H_0\\) 78.1.7 Hypothesis testing framework Start with a null hypothesis, \\(H_0\\), that represents the status quo Set an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what were testing for Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true) if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis if they do, then reject the null hypothesis in favor of the alternative 78.1.8 Setting the hypotheses Which of the following is the correct set of hypotheses? \\(H_0: p = 0.10\\); \\(H_A: p \\ne 0.10\\) \\(H_0: p = 0.10\\); \\(H_A: p &gt; 0.10\\) \\(H_0: p = 0.10\\); \\(H_A: p &lt; 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} \\ne 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} &gt; 0.10\\) \\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} &lt; 0.10\\) 78.1.9 Simulating the null distribution Since \\(H_0: p = 0.10\\), we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10. Describe how you would simulate the null distribution for this study using a bag of chips. How many chips? What colors? What do the colors indicate? How many draws? With replacement or without replacement? 78.1.10 What do we expect? When sampling from the null distribution, what is the expected proportion of success (complications)? 78.1.11 Simulation Here are some simulations. #&gt; sim1 #&gt; complication no complication #&gt; 3 59 #&gt; [1] 0.0484 #&gt; sim2 #&gt; complication no complication #&gt; 9 53 #&gt; [1] 0.145 #&gt; sim3 #&gt; complication no complication #&gt; 8 54 #&gt; [1] 0.129 This is getting boring We need a way to automate this process! 78.1.12 Using tidymodels to generate the null distribution null_dist &lt;- organ_donor %&gt;% specify(response = outcome, success = &quot;complication&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = c(&quot;complication&quot; = 0.10, &quot;no complication&quot; = 0.90)) %&gt;% generate(reps = 100, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) #&gt; # A tibble: 100 x 2 #&gt; replicate stat #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.161 #&gt; 2 2 0.081 #&gt; 3 3 0.161 #&gt; 4 4 0.145 #&gt; 5 5 0.097 #&gt; 6 6 0.145 #&gt; 7 7 0.081 #&gt; 8 8 0.097 #&gt; 9 9 0.161 #&gt; 10 10 0.048 #&gt; # ... with 90 more rows 78.1.13 Visualizing the null distribution What would you expect the center of the null distribution to be? ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.01) + labs(title = &quot;Null distribution&quot;) 78.1.14 Calculating the p-value, visually What is the p-value, i.e. in what % of the simulations was the simulated sample proportion at least as extreme as the observed sample proportion? 78.1.15 Calculating the p-value, directly null_dist %&gt;% filter(stat &lt;= (3/62)) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 x 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.12 78.1.16 Significance level We often use 5% as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. This cutoff value is called the significance level, \\(\\alpha\\). If p-value &lt; \\(\\alpha\\), reject \\(H_0\\) in favor of \\(H_A\\): The data provide convincing evidence for the alternative hypothesis. If p-value &gt; \\(\\alpha\\), fail to reject \\(H_0\\) in favor of \\(H_A\\): The data do not provide convincing evidence for the alternative hypothesis. 78.1.17 Conclusion What is the conclusion of the hypothesis test? Since the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than 10% (overall US complication rate). 78.1.18 Lets get real 100 simulations is not sufficient We usually simulate around 15,000 times to get an accurate distribution, but well do 1,000 here for efficiency. 78.1.19 Run the test null_dist &lt;- organ_donor %&gt;% specify(response = outcome, success = &quot;complication&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = c(&quot;complication&quot; = 0.10, &quot;no complication&quot; = 0.90)) %&gt;% generate(reps = 1000, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) 78.1.20 Visualize and calculate ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = 3/62, color = &quot;red&quot;) null_dist %&gt;% filter(stat &lt;= 3/62) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 x 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.124 78.2 One vs. two sided hypothesis tests 78.2.1 Types of alternative hypotheses One sided (one tailed) alternatives: The parameter is hypothesized to be less than or greater than the null value, &lt; or &gt; Two sided (two tailed) alternatives: The parameter is hypothesized to be not equal to the null value, \\(\\ne\\) Calculated as two times the tail area beyond the observed sample statistic More objective, and hence more widely preferred Average systolic blood pressure of people with Stage 1 Hypertension is 150 mm Hg. Suppose we want to use a hypothesis test to evaluate whether a new blood pressure medication has an effect on the average blood pressure of heart patients. What are the hypotheses? 78.3 Testing for independence 78.3.1 Is yawning contagious? Do you think yawning is contagious? An experiment conducted by the MythBusters tested if a person can be subconsciously influenced into yawning if another person near them yawns. https://www.discovery.com/tv-shows/mythbusters/videos/is-yawning-contagious-2 78.3.2 Study description In this study, 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a control group where they didnt see someone yawn (control). The data are in the openintro package: yawn yawn %&gt;% count(group, result) #&gt; # A tibble: 4 x 3 #&gt; group result n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 ctrl not yawn 12 #&gt; 2 ctrl yawn 4 #&gt; 3 trmt not yawn 24 #&gt; 4 trmt yawn 10 78.3.3 Proportion of yawners yawn %&gt;% count(group, result) %&gt;% group_by(group) %&gt;% mutate(p_hat = n / sum(n)) #&gt; # A tibble: 4 x 4 #&gt; # Groups: group [2] #&gt; group result n p_hat #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 ctrl not yawn 12 0.75 #&gt; 2 ctrl yawn 4 0.25 #&gt; 3 trmt not yawn 24 0.706 #&gt; 4 trmt yawn 10 0.294 Proportion of yawners in the treatment group: \\(\\frac{10}{34} = 0.2941\\) Proportion of yawners in the control group: \\(\\frac{4}{16} = 0.25\\) Difference: \\(0.2941 - 0.25 = 0.0441\\) Our results match the ones calculated on the MythBusters episode. 78.3.4 Independence? Based on the proportions we calculated, do you think yawning is really contagious, i.e. are seeing someone yawn and yawning dependent? #&gt; # A tibble: 4 x 4 #&gt; # Groups: group [2] #&gt; group result n p_hat #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 ctrl not yawn 12 0.75 #&gt; 2 ctrl yawn 4 0.25 #&gt; 3 trmt not yawn 24 0.706 #&gt; 4 trmt yawn 10 0.294 78.3.5 Dependence, or another possible explanation? The observed differences might suggest that yawning is contagious, i.e. seeing someone yawn and yawning are dependent. But the differences are small enough that we might wonder if they might simple be due to chance. Perhaps if we were to repeat the experiment, we would see slightly different results. So we will do just that - well, somewhat - and see what happens. Instead of actually conducting the experiment many times, we will simulate our results. 78.3.6 Two competing claims There is nothing going on. Yawning and seeing someone yawn are independent, yawning is not contagious, observed difference in proportions is simply due to chance. \\(\\rightarrow\\) Null hypothesis There is something going on. Yawning and seeing someone yawn are dependent, yawning is contagious, observed difference in proportions is not due to chance. \\(\\rightarrow\\) Alternative hypothesis 78.3.7 Simulation setup A regular deck of cards is comprised of 52 cards: 4 aces, 4 of numbers 2-10, 4 jacks, 4 queens, and 4 kings. Take out two aces from the deck of cards and set them aside. The remaining 50 playing cards to represent each participant in the study: 14 face cards (including the 2 aces) represent the people who yawn. 36 non-face cards represent the people who dont yawn. 78.3.8 Running the simulation Shuffle the 50 cards at least 7 times1 to ensure that the cards counted out are from a random process. Count out the top 16 cards and set them aside. These cards represent the people in the control group. Out of the remaining 34 cards (treatment group) count the (the number of people who yawned in the treatment group). Calculate the difference in proportions of yawners (treatment - control), and plot it on the board. Mark the difference you find on the dot plot on the board. [1] http://www.dartmouth.edu/~chance/course/topics/winning_number.html 78.3.9 Simulation by hand Do the simulation results suggest that yawning is contagious, i.e. does seeing someone yawn and yawning appear to be dependent? yawn-sim-results 78.3.10 Simulation by computation null_dist &lt;- yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;trmt&quot;, &quot;ctrl&quot;)) Start with the data frame Specify the variables Since the response variable is categorical, specify the level which should be considered as success yawn %&gt;% {{ specify(response = result, explanatory = group, success = &quot;yawn&quot;) }} State the null hypothesis (yawning and whether or not you see someone yawn are independent) yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% {{ hypothesize(null = &quot;independence&quot;) }} Generate simulated differences via permutation yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% {{ generate(100, type = &quot;permute&quot;) }} Calculate the sample statistic of interest (difference in proportions) Since the explanatory variable is categorical, specify the order in which the subtraction should occur for the calculation of the sample statistic, \\((\\hat{p}_{treatment} - \\hat{p}_{control})\\). yawn %&gt;% specify(response = result, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% {{ calculate(stat = &quot;diff in props&quot;, order = c(&quot;trmt&quot;, &quot;ctrl&quot;)) }} 78.3.11 Recap Save the result Start with the data frame Specify the variables Since the response variable is categorical, specify the level which should be considered as success State the null hypothesis (yawning and whether or not you see someone yawn are independent) Generate simulated differences via permutation Calculate the sample statistic of interest (difference in proportions) Since the explanatory variable is categorical, specify the order in which the subtraction should occur for the calculation of the sample statistic, \\((\\hat{p}_{treatment} - \\hat{p}_{control})\\). {{null_dist &lt;- yawn %&gt;% }} specify(response = outcome, explanatory = group, success = &quot;yawn&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(100, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;treatment&quot;, &quot;control&quot;)) 78.3.12 Visualizing the null distribution What would you expect the center of the null distribution to be? ggplot(data = null_dist, mapping = aes(x = stat)) + geom_histogram(binwidth = 0.05) + labs(title = &quot;Null distribution&quot;) 78.3.13 Calculating the p-value, visually What is the p-value, i.e. in what % of the simulations was the simulated difference in sample proportion at least as extreme as the observed difference in sample proportions? 78.3.14 Calculating the p-value, directly null_dist %&gt;% filter(stat &gt;= 0.0441) %&gt;% summarize(p_value = n()/nrow(null_dist)) #&gt; # A tibble: 1 x 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.53 78.3.15 Conclusion What is the conclusion of the hypothesis test? Do you buy this conclusion? "],["lab11.html", "79 Lab: So what if you smoke when pregnant? 79.1 Simulation based inference 79.2 Getting started 79.3 Housekeeping 79.4 Warm up 79.5 Set a seed! 79.6 The data 79.7 Exercises", " 79 Lab: So what if you smoke when pregnant? 79.1 Simulation based inference In 2004, the state of North Carolina released a large data set containing information on births recorded in this state. This data set is useful to researchers studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of observations from this data set. 79.2 Getting started 79.2.1 Packages In this lab we will work with the tidyverse, infer, and openintro packages. We can install and load them with the following: library(tidyverse) library(infer) library(openintro) 79.3 Housekeeping 79.3.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 79.3.2 Project name Update the name of your project to match the labs title. 79.4 Warm up Before we introduce the data, lets warm up with some simple exercises. 79.4.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 79.4.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 79.5 Set a seed! In this lab, well be generating random samples. The last thing you want is those samples to change every time you knit your document. So, you should set a seed. Theres an R chunk in your R Markdown file set aside for this. Locate it and add a seed. 79.6 The data Load the ncbirths data from the openintro package: data(ncbirths) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is as follows. variable description fage fathers age in years. mage mothers age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. 79.7 Exercises What are the cases in this data set? How many cases are there in our sample? The first step in the analysis of a new dataset is getting acquainted with the data. Make summaries of the variables in your dataset, determine which variables are categorical and which are numerical. For numerical variables, are there outliers? If you arent sure or want to take a closer look at the data, make a graph. 79.7.1 Baby weights Wen, Shi Wu, Michael S. Kramer, and Robert H. Usher. Comparison of birth weight distributions between Chinese and Caucasian infants. American Journal of Epidemiology 141.12 (1995): 1177-1187. A 1995 study suggests that average weight of Caucasian babies born in the US is 3,369 grams (7.43 pounds). In this dataset, we only have information on mothers race, so we will make the simplifying assumption that babies of Caucasian mothers are also Caucasian, i.e. whitemom = \"white\". (Yes, I know that this assumption is a gross oversimplification). We want to evaluate whether the average weight of Caucasian babies has changed since 1995. Our null hypothesis should state there is nothing going on, i.e. no change since 1995: \\(H_0: \\mu = 7.43~pounds\\). Our alternative hypothesis should reflect the research question, i.e. some change since 1995. Since the research question doesnt state a direction for the change, we use a two sided alternative hypothesis: \\(H_A: \\mu \\ne 7.43~pounds\\). Create a filtered data frame called ncbirths_white that contain data only from white mothers. Then, calculate the mean of the weights of their babies. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Lets discuss how this test would work. Our goal is to simulate a null distribution of sample means that is centered at the null value of 7.43 pounds. In order to do so, we take a bootstrap sample of from the original sample, calculate this bootstrap samples mean, repeat these two steps a large number of times to create a bootstrap distribution of means centered at the observed sample mean, shift this distribution to be centered at the null value by subtracting / adding X to all boostrap mean (X = difference between mean of bootstrap distribution and null value), and calculate the p-value as the proportion of bootstrap samples that yielded a sample mean at least as extreme as the observed sample mean. Run the appropriate hypothesis test, visualize the null distribution, calculate the p-value, and interpret the results in context of the data and the hypothesis test. 79.7.2 Baby weight vs. smoking Consider the possible relationship between a mothers smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make side-by-side boxplots displaying the relationship between habit and weight. What does the plot highlight about the relationship between these two variables? Before moving forward, save a version of the dataset omitting observations where there are NAs for habit. You can call this version ncbirths_habitgiven. The box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the habit variable, and then calculate the meanweight in these groups using. ncbirths_habitgiven %&gt;% group_by(habit) %&gt;% summarise(mean_weight = mean(weight)) There is an observed difference, but is this difference statistically significant? In order to answer this question, we will conduct a hypothesis test. Write the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Run the appropriate hypothesis test, calculate the p-value, and interpret the results in context of the data and the hypothesis test. Construct a 95% confidence interval for the difference between the average weights of babies born to smoking and non-smoking mothers. 79.7.3 Baby weight vs. mothers age In this portion of the analysis, we focus on two variables. The first one is maturemom. First, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. The other variable of interest is lowbirthweight. Conduct a hypothesis test evaluating whether the proportion of low birth weight babies is higher for mature mothers. State the hypotheses, verify the conditions, run the test and calculate the p-value, and state your conclusion in context of the research question. Use \\(\\alpha = 0.05\\). If you find a significant difference, construct a confidence interval, at the equivalent level to the hypothesis test, for the difference between the proportions of low birth weight babies between mature and younger moms, and interpret this interval in context of the data. "],["rshiny.html", "80 Welcome to interactive web apps 80.1 Module Materials", " 80 Welcome to interactive web apps This module is designed to introduce you to interactive web apps. Well focus on rshiny. Please watch the videos and work your way through the notes. You can find the video playlist for this module here . Most of the slides used to make the videos in this module can be found in the slides repo. pic.twitter.com/keJCMuyAw0&mdash; Ian Gutierrez (@ianagutierrez) February 7, 2021 80.1 Module Materials Slides Interactive web apps Videos Curated Videos Readings Rstudio tutorial Written Lecture Notes Lab No Lab! "],["rshiny-overview.html", "81 RShiny Overview", " 81 RShiny Overview You can follow along with the slides here if they do not appear below. "],["practical-advice-from-the-data-professor.html", "82 Practical Advice from the Data Professor 82.1 Web Apps in R: Building your First Web Application in R 82.2 Web Apps in R: Build Interactive Histogram Web Application in R 82.3 Web Apps in R: Building Data-Driven Web Application in R 82.4 Web Apps in R: Building the Machine Learning Web Application in R 82.5 Web Apps in R: Build BMI Calculator web application in R for health monitoring", " 82 Practical Advice from the Data Professor Ive outsourced the practical videos because frankly, Im a novice when it comes to rshiny. I know enough to be dangerous, but not enough to wield that power for teaching. However, my good twitter friend, the Data Professor has that power. You should check out his youtube channel 82.1 Web Apps in R: Building your First Web Application in R 82.2 Web Apps in R: Build Interactive Histogram Web Application in R 82.3 Web Apps in R: Building Data-Driven Web Application in R 82.4 Web Apps in R: Building the Machine Learning Web Application in R 82.5 Web Apps in R: Build BMI Calculator web application in R for health monitoring "],["shiny-overview.html", "83 All the Shiny things 83.1 Building Slides 83.2 Building Shiny apps 83.3 Build the basic UI 83.4 Checkpoint: what our app looks like after implementing the UI 83.5 Ideas to improve our app", " 83 All the Shiny things These notes are adapted from Jenny Bryan and Dean Attali. 83.1 Building Slides See Building Shiny Apps slides by Dean Attali. 83.2 Building Shiny apps Shiny is a package from RStudio that can be used to build interactive web pages with R. While that may sound scary because of the words web pages, Shiny is geared to R users who have zero experience with web development, and you do not need to know any HTML/CSS/JavaScript. You can do quite a lot with Shiny: think of it as an easy way to make an interactive web page, and that web page can seamlessly interact with R and display R objects (plots, tables, of anything else you do in R). To get a sense of the wide range of things you can do with Shiny, you can visit my Shiny server (https://daattali.com/shiny/), which hosts some of my own Shiny apps. This tutorial is a hands-on activity complement to a set of presentation slides for learning how to build Shiny apps. In this activity, well walk through all the steps of building a Shiny app using a dataset that lets you explore the products available at the BC Liquor Store. The final version of the app, including a few extra features that are left as exercises for the reader, can be seen here: https://daattali.com/shiny/bcl/. Any activity deemed as an exercise throughout this tutorial is not mandatory for building our app, but they are good for getting more practice with Shiny. This tutorial should take approximately an hour to complete. If you want even more practice, another great tutorial is the official Shiny tutorial. RStudio also provides a handy cheatsheet to remember all the little details after you already learned the basics. 83.2.1 Before we begin Youll need to have the shiny package, so install it. install.packages(&quot;shiny&quot;) To ensure you successfully installed Shiny, try running one of the demo apps. library(shiny) runExample(&quot;01_hello&quot;) If the example app is running, press Escape to close the app, and you are ready to build your first Shiny app! Exercise: Visit https://www.showmeshiny.com/, which is a gallery of user-submitted Shiny apps, and click through some of the showcased apps. Get a feel for the wide range of things you can do with Shiny. 83.2.2 Shiny app basics Every Shiny app is composed of a two parts: a web page that shows the app to the user, and a computer that powers the app. The computer that runs the app can either be your own laptop (such as when youre running an app from RStudio) or a server somewhere else. You, as the Shiny app developer, need to write these two parts (youre not going to write a computer, but rather the code that powers the app). In Shiny terminology, they are called UI (user interface) and server. UI is just a web document that the user gets to see, its HTML that you write using Shinys functions. The UI is responsible for creating the layout of the app and telling Shiny exactly where things go. The server is responsible for the logic of the app; its the set of instructions that tell the web page what to show when the user interacts with the page. If you look at the app we will be building (http://daattali.com/shiny/bcl/), the page that you see is built with the UI code. Youll notice there are some controls that you, as the user, can manipulate. If you adjust the price or choose a country, youll notice that the plot and the table get updated. The UI is responsible for creating these controls and telling Shiny where to place the controls and where to place the plot and table, while the server is responsible for creating the actual plot or the data in the table. 83.2.3 Create an empty Shiny app All Shiny apps follow the same template: library(shiny) ui &lt;- fluidPage() server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) This template is by itself a working minimal Shiny app that doesnt do much. It initializes an empty UI and an empty server, and runs an app using these empty parts. Copy this template into a new file named app.R in a new folder. It is very important that the name of the file is app.R, otherwise it would not be recognized as a Shiny app. It is also very important that you place this app in its own folder, and not in a folder that already has other R scripts or files, unless those other files are used by your app. After saving the file, RStudio should recognize that this is a Shiny app, and you should see the usual Run button at the top change to Run App. Figure 83.1: Shiny run app button If you dont see the Run App button, it means you either have a very old version of RStudio, dont have Shiny installed, or didnt follow the file naming conventions. Click the Run App button, and now your app should run. You wont see much because its an empty app, but you should see that the R Console has some text printed in the form of Listening on http://127.0.0.1:5274 and that a little stop sign appeared at the top of the R Console Youll also notice that you cant run any commands in the R Console This is because R is busy - your R session is currently powering a Shiny app and listening for user interaction (which wont happen because the app has nothing in it yet). Click the stop button to stop the app, or press the Escape key. Figure 83.2: Shiny stop app button You may have noticed that when you click the Run App button, all its doing is just running the function shiny::runApp() in the R Console You can run that command instead of clicking the button if you prefer. Exercise: Try running the empty app using the runApp() function instead of using the Run App button. 83.2.3.1 Alternate way to create a Shiny app: separate UI and server files Another way to define a Shiny app is by separating the UI and server code into two files: ui.R and server.R. This is the preferable way to write Shiny apps when the app is complex and involves more code, but in this tutorial well stick to the simple single file. If you want to break up your app into these two files, you simply put all code that is assigned to the ui variable in ui.R and all the code assigned to the server function in server.R. When RStudio sees these two files in the same folder, it will know youre writing a Shiny app. Exercise: Try making a new Shiny app by creating the two files ui.R and server.R. Remember that they have to be in the same folder. Also remember to put them in a new, isolated folder (not where your app.R already exists). 83.2.3.2 Let RStudio fill out a Shiny app template for you You can also create a new Shiny app using RStudios menu by selecting File &gt; New File &gt; Shiny Web App. If you do this, RStudio will let you choose if you want a single-file app (app.R) or a two-file app (ui.R+server.R). RStudio will initialize a simple functional Shiny app with some code in it. I personally dont use this feature because I find it easier to simply type the few lines of a Shiny app and save the files. 83.2.4 Load the dataset The dataset well be using contains information about all the products sold by BC Liquor Store and is provided by OpenDataBC. They provide a direct link to download a CSV version of the data, and this data has the rare quality that it is immediately clean and useful. You can view the raw data they provide, but I have taken a few steps to simplify the dataset to make it more useful for our app. I removed some columns, renamed other columns, and dropped a few rare factor levels. The processed dataset well be using in this app is available here. Download it now and place this file in the same folder as your Shiny app. Make sure the file is named bcl-data.csv. Add a line in your app to load the data into a variable called bcl. It should look something like this bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) Place this line in your app as the second line, just after library(shiny). Make sure the file path and file name are correct, otherwise your app wont run. Try to run the app to make sure the file can be loaded without errors. If you want to verify that the app can successfully read the data, you can add a print() statement after reading the data. This wont make anything happen in your Shiny app, but you will see a summary of the dataset printed in the R Console, which should let you know that the dataset was indeed loaded correctly. You can place the following line after reading the data: print(str(bcl)) Once you get confirmation that the data is properly loaded, you can remove that line. In case youre curious, the code Jenny and Dean used to process the raw data into the data well be using is available as a gist. Exercise: Load the data file into R and get a feel for whats in it. How big is it, what variables are there, what are the normal price ranges, etc. 83.3 Build the basic UI Lets start populating our app with some elements visually. This is usually the first thing you do when writing a Shiny app - add elements to the UI. 83.3.1 Add plain text to the UI You can place R strings inside fluidPage() to render text. fluidPage(&quot;BC Liquor Store&quot;, &quot;prices&quot;) Replace the line in your app that assigns an empty fluidPage() into ui with the one above, and run the app. The entire UI will be built by passing comma-separated arguments into the fluidPage() function. By passing regular text, the web page will just render boring unformatted text. Exercise: Add several more strings to fluidPage() and run the app. Nothing too exciting is happening yet, but you should just see all the text appear in one contiguous block. 83.3.1.1 Add formatted text and other HTML elements If we want our text to be formatted nicer, Shiny has many functions that are wrappers around HTML tags that format text. We can use the h1() function for a top-level header (&lt;h1&gt; in HTML) h2() for a secondary header (&lt;h2&gt; in HTML) strong() to make text bold (&lt;strong&gt; in HTML) em() to make text italicized (&lt;em&gt; in HTML) and many more. There are also functions that are wrappers to other HTML tags, such as br() for a line break, img() for an image, a() for a hyperlink, and others. All of these functions are actually just wrappers to HTML tags with the equivalent name. You can add any arbitrary HTML tag using the tags object, which you can learn more about by reading the help file via ?tags. Just as a demonstration, try replacing the fluidPage() function in your UI with the following: fluidPage( h1(&quot;My app&quot;), &quot;BC&quot;, &quot;Liquor&quot;, br(), &quot;Store&quot;, strong(&quot;prices&quot;) ) Run the app with this code as the UI. Notice the formatting of the text and understand why it is rendered that way. For people who know basic HTML: any named argument you pass to an HTML function becomes an attribute of the HTML element, and any unnamed argument will be a child of the element. That means that you can, for example, create blue text with div(\"this is blue\", style = \"color: blue;\"). Exercise: Experiment with different HTML-wrapper functions inside fluidPage(). Run the fluidPage(...) function in the R Console and see the HTML that it creates. 83.3.1.2 Add a title We could add a title to the app with h1(), but Shiny also has a special function titlePanel(). Using titlePanel() not only adds a visible big title-like text to the top of the page, but it also sets the official title of the web page. This means that when you look at the name of the tab in the browser, youll see this title. Overwrite the fluidPage() that you experimented with so far, and replace it with the simple one below, that simply has a title and nothing else. fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;) ) Exercise: Look at the documentation for the titlePanel() function and notice it has another argument. Use that argument and see if you can see what it does. 83.3.1.3 Add a layout You may have noticed that so far, by just adding text and HTML tags, everything is unstructured and the elements simply stack up one below the other in one column. Well use sidebarLayout() to add a simple structure. It provides a simple two-column layout with a smaller sidebar and a larger main panel. Well build our app such that all the inputs that the user can manipulate will be in the sidebar, and the results will be shown in the main panel on the right. Add the following code after the titlePanel() sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) Remember that all the arguments inside fluidPage() need to be separated by commas. So far our complete app looks like this (hopefully this isnt a surprise to you): library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 83.3: Our Shiny app so far If you want to be a lot more flexible with the design, you can have much more fine control over where things go by using a grid layout. We wont cover that here, but if youre interested, look at the documentation for ?column and ?fluidRow. Exercise: Add some UI into each of the two panels (sidebar panel and main panel) and see how your app now has two columns. 83.3.1.4 All UI functions are simply HTML wrappers Its important to remember: the entire UI is just HTML, and Shiny simply gives you easy tools to write it without having to know HTML. To convince yourself of this reality, look at the output when printing the contents of the ui variable. print(ui) ## &lt;div class=&quot;container-fluid&quot;&gt; ## &lt;h2&gt;BC Liquor Store prices&lt;/h2&gt; ## &lt;div class=&quot;row&quot;&gt; ## &lt;div class=&quot;col-sm-4&quot;&gt; ## &lt;form class=&quot;well&quot;&gt;our inputs will go here&lt;/form&gt; ## &lt;/div&gt; ## &lt;div class=&quot;col-sm-8&quot;&gt;the results will go here&lt;/div&gt; ## &lt;/div&gt; ## &lt;/div&gt; This output should make you appreciate Shiny for not making you write horrendous HTML by hand. 83.3.2 Add inputs to the UI Inputs are what gives users a way to interact with a Shiny app. Shiny provides many input functions to support many kinds of interactions that the user could have with an app. For example, textInput() is used to let the user enter text, numericInput() lets the user select a number, dateInput() is for selecting a date, and selectInput() is for creating a select box (a.k.a. a dropdown menu). Figure 83.4: Shiny inputs All input functions have the same first two arguments: inputId and label. The inputId will be the name that Shiny will use to refer to this input when you want to retrieve its current value. It is important to note that every input must have a unique inputId. If you give more than one input the same inputId, Shiny will unfortunately not give you an explicit error, but your app wont work correctly. The label argument specifies the text in the display label that goes along with the input widget. Every input can also have multiple other arguments specific to that input type. The only way to find out what arguments you can use with a specific input function is to look at its help file. Exercise: Read the documentation of numericInput (via ?numericInput) and try adding a numeric input to the UI. Experiment with the different arguments. Run the app and see how you can interact with this input. Then try different inputs types. 83.3.2.1 Input for price The first input we want to have is for specifying a price range (minimum and maximum price). The most sensible types of input for this are either numericInput() or sliderInput() since they are both used for selecting numbers. If we use numericInput(), wed have to use two inputs, one for the minimum value and one for the maximum. Looking at the documentation for sliderInput(), youll see that by supplying a vector of length two as the value argument, it can be used to specify a range rather than a single number. This sounds like what we want in this case, so well use sliderInput(). To create a slider input, a maximum value needs to be provided. We could use the maximum price in the dataset, which is $30,250, but I doubt Id ever buy something that expensive. I think $100 is a more reasonable max price for me, and about 85% of the products in this dataset are below $100, so lets use that as our max. By looking at the documentation for the slider input function, the following piece of code can be constructed. sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, min = 0, max = 100, value = c(25, 40), pre = &quot;$&quot;) Place the code for the slider input inside sidebarPanel() (replace the text we wrote earlier with this input). Exercise: Run the code of the sliderInput() in the R Console and see what it returns. Change some of the parameters of sliderInput(), and see how that changes the result. Its important to truly understand that all these functions in the UI are simply a convenient way to write HTML, as is apparent whenever you run these functions on their own. 83.3.2.2 Input for product type Usually when going to the liquor store you know whether youre looking for beer or wine, and you dont want to waste your time in the wrong section. The same is true in our app, we should be able to choose what type of product we want. For this we want some kind of a text input. But allowing the user to enter text freely isnt the right solution because we want to restrict the user to only a few choices. We could either use radio buttons or a select box for our purpose. Lets use radio buttons for now since there are only a few options, so take a look at the documentation for radioButtons() and come up with a reasonable input function code. It should look like this: radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;) Add this input code inside sidebarPanel(), after the previous input (separate them with a comma). If you look at that input function and think what if there were 100 types, listing them by hand would not be fun, theres got to be a better way! then youre right. This is where uiOutput() comes in handy, but well talk about that later. 83.3.2.3 Input for country Sometimes Dean and Jenny like to feel fancy and only look for wines imported from France. We should add one last input, to select a Country. The most appropriate input type in this case is probably the select box. Look at the documentation for selectInput() and create an input function. For now lets only have \"CANADA\", \"FRANCE\", \"ITALY\" as options, and later well see how to include all countries. selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) Add this function as well to your app. If you followed along, your entire app should have this code: library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 83.5: Adding inputs to our Shiny app 83.3.3 Add placeholders for outputs After creating all the inputs, we should add elements to the UI to display the outputs. Outputs can be any object that R creates and that we want to display in our app - such as a plot, a table, or text. Were still only building the UI, so at this point we can only add placeholders for the outputs that will determine where an output will be and what its ID is, but it wont actually show anything. Each output needs to be constructed in the server code later. Shiny provides several output functions, one for each type of output. Similarly to the input functions, all the output functions have a outputId argument that is used to identify each output, and this argument must be unique for each output. 83.3.3.1 Output for a plot of the results At the top of the main panel well have a plot showing some visualization of the results. Since we want a plot, the function we use is plotOutput(). Add the following code into the mainPanel() (replace the existing text): plotOutput(&quot;coolplot&quot;) This will add a placeholder in the UI for a plot named coolplot. Exercise: To remind yourself that we are still merely constructing HTML and not creating actual plots yet, run the above plotOutput() function in the R Console to see that all it does is create some HTML. 83.3.4 Output for a table summary of the results Below the plot, we will have a table that shows all the results. To get a table, we use the tableOutput() function. Here is a simple way to create a UI element that will hold a table output: tableOutput(&quot;results&quot;) Add this output to the mainPanel() as well. Maybe add a couple br() in between the two outputs, just as a space buffer so that they arent too close to each other. 83.4 Checkpoint: what our app looks like after implementing the UI If youve followed along, your app should now have this code: library(shiny) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 83.4.1 Implement server logic to create outputs So far we only wrote code inside that was assigned to the ui variable (or code that was written in ui.R). Thats usually the easier part of a Shiny app. Now we have to write the server function, which will be responsible for listening to changes to the inputs and creating outputs to show in the app. If you look at the server function, youll notice that it is always defined with two arguments: input and output. You must define these two arguments! Both input and output are list-like objects. As the names suggest, input is a list you will read values from and output is a list you will write values to. input will contain the values of all the different inputs at any given time, and output is where you will save output objects (such as tables and plots) to display in your app. 83.4.1.1 Building an output Recall that we created two output placeholders: coolplot (a plot) and results (a table). We need to write code in R that will tell Shiny what kind of plot or table to display. There are three rules to build an output in Shiny: Save the output object into the output list (remember the app template - every server function has an output argument). Build the object with a render* function, where * is the type of output. Access input values using the input list (every server function has an input argument). The third rule is only required if you want your output to depend on some input, so lets first see how to build a very basic output using only the first two rules. Well create a plot and send it to the coolplot output. output$coolplot &lt;- renderPlot({ plot(rnorm(100)) }) This simple code shows the first two rules: were creating a plot inside the renderPlot() function, and assigning it to coolplot in the output list. Remember that every output created in the UI must have a unique ID, now we see why. In order to attach an R object to an output with ID x, we assign the R object to output$x. Since coolplot was defined as a plotOutput, we must use the renderPlot function, and we must create a plot inside the renderPlot function. If you add the code above inside the server function, you should see a plot with 100 random points in the app. Exercise: The code inside renderPlot() doesnt have to be only one line, it can be as long as youd like as long as it returns a plot. Try making a more complex plot using ggplot2. The plot doesnt have to use our dataset, it could be anything, just to make sure you can use renderPlot(). 83.4.1.2 Making an output react to an input Now well take the plot one step further. Instead of always plotting the same plot (100 random numbers), lets use the minimum price selected as the number of points to show. It doesnt make too much sense, but its just to learn how to make an output depend on an input. Replace the previous code in your server function with the code below, and run the app. output$coolplot &lt;- renderPlot({ plot(rnorm(input$priceInput[1])) }) Whenever you choose a new minimum price range, the plot will update with a new number of points. Notice that the only thing different in the code is that instead of using the number 100 we are using input$priceInput[1]. What does this mean? Just like the variable output contains a list of all the outputs (and we need to assign code into them), the variable input contains a list of all the inputs that are defined in the UI. input$priceInput return a vector of length two containing the minimum and maximum price. Whenever the user manipulates the slider in the app, these values are updated, and whatever code relies on it gets re-evaluated. This is a concept known as reactivity, which we will get to soon. Notice that these short three lines of code are using all the three rules for building outputs: We are saving to the output list (output$coolplot &lt;-). We are using a render* function to build the output (renderPlot({})). We are accessing an input value (input$priceInput[1]). 83.4.2 Building the plot output Now we have all the knowledge required to build a plot visualizing some aspect of the data. Well create a simple histogram of the alcohol content of the products by using the same three rules to create a plot output. First we need to make sure ggplot2 is attached, so add a library(ggplot2) at the top. Next well return a histogram of alcohol content from renderPlot(). Lets start with just a histogram of the whole data, unfiltered. output$coolplot &lt;- renderPlot({ ggplot(bcl, aes(Alcohol_Content)) + geom_histogram() }) If you run the app with this code inside your server, you should see a histogram in the app. But if you change the input values, nothing happens yet, so the next step is to actually filter the dataset based on the inputs. Recall that we have 3 inputs: priceInput, typeInput, and countryInput. We can filter the data based on the values of these three inputs. Well use dplyr functions to filter the data, so be sure to include dplyr at the top. Then well plot the filtered data instead of the original data. Place this code in your server function and run the app: output$coolplot &lt;- renderPlot({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) ggplot(filtered, aes(Alcohol_Content)) + geom_histogram() }) If you change any input, you should see the histogram update. The way I know the histogram is correct is by noticing that the alcohol content is about 5% when I select beer, 40% for spirits, and 13% for wine. That sounds right. Read this code and understand it. Youve successfully created an interactive app - the plot is changing according to the users selection. To make sure were on the same page, here is what your code should look like at this point: library(shiny) library(ggplot2) library(dplyr) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), selectInput(&quot;countryInput&quot;, &quot;Country&quot;, choices = c(&quot;CANADA&quot;, &quot;FRANCE&quot;, &quot;ITALY&quot;)) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) { output$coolplot &lt;- renderPlot({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) ggplot(filtered, aes(Alcohol_Content)) + geom_histogram() }) } shinyApp(ui = ui, server = server) Figure 83.6: Adding a plot to our Shiny app Exercise: The current plot doesnt look very nice, you could enhance the plot and make it much more pleasant to look at. 83.4.2.1 Building the table output Building the next output should be much easier now that weve done it once. The other output we have was called results (as defined in the UI) and should be a table of all the products that match the filters. Since its a table output, we should use the renderTable() function. Well do the exact same filtering on the data, and then simply return the data as a data.frame. Shiny will know that it needs to display it as a table because its defined as a tableOutput. The code for creating the table output should make sense to you without too much explanation: output$results &lt;- renderTable({ filtered &lt;- bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) filtered }) Add this code to your server. Dont overwrite the previous definition of output$coolplot, just add this code before or after that, but inside the server function. Run your app, and be amazed! You can now see a table showing all the products at the BC Liquor Store that match your criteria. Exercise: Add a new output. Either a new plot, a new table, or some piece of text that changes based on the inputs. For example, you could add a text output (textOutput() in the UI, renderText() in the server) that says how many results were found. If you choose to do this, I recommend first adding the output to the UI, then building the output in the server with static text to make sure you have the syntax correct. Only once you can see the text output in your app you should make it reflect the inputs. Pro-tip: since textOutput() is written in the UI, you can wrap it in other UI functions. For example, h2(textOutput(...)) will result in larger text. 83.4.3 Reactivity 101 Shiny uses a concept called reactive programming. This is what enables your outputs to react to changes in inputs. Reactivity in Shiny is complex, but as an extreme oversimplification, it means that when the value of a variable x changes, then anything that relies on x gets re-evaluated. Notice how this is very different from what you are used to in R. Consider the following code: x &lt;- 5 y &lt;- x + 1 x &lt;- 10 What is the value of y? Its 6. But in reactive programming, if x and y are reactive variables, then the value of y would be 11 because it would be updated whenever x is changed. This is a very powerful technique that is very useful for creating the responsiveness of Shiny apps, but it might be a bit weird at first because its a very different concept from what youre used to. Only reactive variables behave this way, and in Shiny all inputs are automatically reactive. Thats why you can always use input$x in render functions, and you can be sure that whatever output depends on x will use the updated value of x whenever x changes. You might be wondering what it means to depend on a variable. This is not the official terminology, but it simply means that the variable is referenced in the code. So by merely accessing the value of a reactive variable, it causes the current code block to depend on that variable. Consider the following sample code to create a plot with a specific number of points in a specific colour: output$someoutput &lt;- renderPlot({ col &lt;- input$mycolour num &lt;- input$mynumber plot(rnorm(num), col = col) }) The above render function accesses two different inputs: input$mycolour and input$mynumber. This means that this code block depends on both of these variables, so whenever either one of the two inputs is updated, the code gets re-executed with the new input values and output$someoutput is updated. 83.4.3.1 Creating and accessing reactive variables One very important thing to remember about reactive variables (such as the input list) is that they can only be used inside reactive contexts. Any render* function is a reactive context, so you can always use input$x or any other reactive variable inside render functions. There are two other common reactive contexts that well get to in a minute: reactive({}) and observe({}). To show you what this means, lets try accessing the price input value in the server function, without explicitly being inside a reactive context. Simply add print(input$priceInput) inside the server function, and you will get an error when running the app: Operation not allowed without an active reactive context. (You tried to do something that can only be done from inside a reactive expression or observer.) Shiny is very clear about what the error is: we are trying to access a reactive variable outside of a reactive context. To fix this, we can use the observe({}) function to access the input variable. Inside the server, replace print(input$priceInput) with observe({ print(input$priceInput) }), and now the app should run fine. Note that this observe({}) statement depends on input$priceInput, so whenever you change the value of the price, the code inside this observe({}) will run again, and the new value will be printed. This is actually a very simple yet useful debugging technique in Shiny: often you want to know what value a reactive variable holds, so you need to remember to wrap the cat(input$x) or print(input$x) by an observe({}). So far we only saw one reactive variable: the input list. You can also create your own reactive variables using the reactive({}) function. The reactive({}) function is similar to observe({}) in that it is also a reactive context, which means that it will get re-run whenever any of the reactive variables in it get updated. The difference between them is that reactive({}) returns a value. To see it in action, lets create a variable called priceDiff that will be the difference between the maximum and minimum price selected. If you try to naively define priceDiff &lt;- diff(input$priceInput), youll see the same error as before about doing something outside a reactive context. This is because input$priceInput is a reactive variable, and we cant use a reactive variable outside a reactive context. Since we want to assign a value, we use the reactive({}) function. Try adding the following line to your server: priceDiff &lt;- reactive({ diff(input$priceInput) }) Now your app will run. If you want to access a reactive variable defined with reactive({}), you must add parentheses after the variable name, as if its a function. To demonstrate this, add observe({ print(priceDiff()) }) to your server function. Notice that we use priceDiff() rather than priceDiff. Its very important to remember this, because you can get confusing unclear errors if you simply try to access a custom reactive variable without the parentheses. You can think of reactivity as causing a chain reaction: when one reactive value changes, anything that depends on it will get updated. If any of the updated values are themselves reactive variables, then any reactive contexts that depend on those variables will also get updated in turn. As a concrete example, lets think about what happens when you change the value of the priceInput on the page. Since input$priceInput is a reactive variable, any expression that uses it will get updated. This means the two render functions from earlier will execute because they both depend on input$priceInput, as well as the priceDiff variable because it also depends on it. But since priceDiff is itself a reactive variable, Shiny will check if there is anything that depends on priceDiff, and indeed there is - the observe({}) function that prints the value of priceDiff. So once priceDiff gets updated, the observe({}) function will run, and the value will get printed. Reactivity is usually the hardest part about Shiny to understand, so if you dont quite get it, dont feel bad. Try reading this section again, and I promise that with time and experience you will get more comfortable with reactivity. Once you do feel more confident with reactivity, it may be a good idea to read more advanced documentation describing reactivity, since this section greatly simplifies ideas to make them more understandable. A great resource is RStudios tutorial on reactivity. Before continuing to the next section, you can remove all the observe({}) and reactive({}) functions we wrote in this section since they were all just for learning purposes. Exercise: Read this section again and really understand what a reactive variable means, what the three main reactive contexts are, how you can define reactive variables, and how a reactivity chain of events works. 83.4.3.2 Using reactive variables to reduce code duplication You may have noticed that we have the exact same code filtering the dataset in two places, once in each render function. We can solve that problem by defining a reactive variable that will hold the filtered dataset, and use that variable in the render functions. The first step would be to create the reactive variable. The following code should be added to the server function. filtered &lt;- reactive({ bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) The variable filtered is being defined exactly like before, except the body is wrapped by a reactive({}), and its defined in the server function instead of inside the individual render functions. Now that we have our reactive variable, we can use it in the output render functions. Try it yourself, and when you think youre done, check the code below. Dont forget that in order to access the value of a reactive expression, you must follow the name of the variable with parentheses! This is how your server function should look now: server &lt;- function(input, output) { filtered &lt;- reactive({ bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) output$coolplot &lt;- renderPlot({ ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) output$results &lt;- renderTable({ filtered() }) } As a reminder, Shiny creates a dependency tree with all the reactive expressions to know what value depends on what other value. For example, when the price input changes, Shiny looks at what values depend on price, and sees that filtered is a reactive expression that depends on the price input, so it re-evaluates filtered. Then, because filtered is changed, Shiny now looks to see what expressions depend on filtered, and it finds that the two render functions use filtered. So Shiny re-executes the two render functions as well. 83.4.4 Using uiOutput() to create UI elements dynamically One of the output functions you can add in the UI is uiOutput(). According to the naming convention (e.g. plotOutput() is an output to render a plot), this is an output used to render more UI. This may sound a bit confusing, but its actually very useful. Its usually used to create inputs (or any other UI) from the server, or in other words - you can create inputs dynamically. Any input that you normally create in the UI is created when the app starts, and it cannot be changed. But what if one of your inputs depends on another input? In that case, you want to be able to create an input dynamically, in the server, and you would use uiOutput(). uiOutput() can be used to create any UI element, but its most often used to create input UI elements. The same rules regarding building outputs apply, which means the output (which is a UI element in this case) is created with the function renderUI(). 83.4.4.1 Basic example of uiOutput() As a very basic example, consider this app: library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Maximum slider value&quot;, 5), uiOutput(&quot;slider&quot;) ) server &lt;- function(input, output) { output$slider &lt;- renderUI({ sliderInput(&quot;slider&quot;, &quot;Slider&quot;, min = 0, max = input$num, value = 0) }) } shinyApp(ui = ui, server = server) If you run that tiny app, you will see that whenever you change the value of the numeric input, the slider input is re-generated. This behavior can come in handy often. 83.4.5 Use uiOutput() in our app to populate the countries We can use this concept in our app to populate the choices for the country selector. The country selector currently only holds 3 values that we manually entered, but instead we could render the country selector in the server and use the data to determine what countries it can have. First we need to replace the selectInput(\"countryInput\", ...) in the UI with: uiOutput(&quot;countryOutput&quot;) Then we need to create the output (which will create a UI element - yeah, it can be a bit confusing at first), so add the following code to the server function: output$countryOutput &lt;- renderUI({ selectInput(&quot;countryInput&quot;, &quot;Country&quot;, sort(unique(bcl$Country)), selected = &quot;CANADA&quot;) }) Now if you run the app, you should be able to see all the countries that BC Liquor stores import from. 83.4.5.1 Errors showing up and quickly disappearing You might notice that when you first run the app, each of the two outputs are throwing an error message, but the error message goes away after a second. The problem is that when the app initializes, filtered is trying to access the country input, but the country input hasnt been created yet. After Shiny finishes loading fully and the country input is generated, filtered tries accessing it again, this time its successful, and the error goes away. Once we understand why the error is happening, fixing it is simple. Inside the filtered reactive function, we should check if the country input exists, and if not then just return NULL. filtered &lt;- reactive({ if (is.null(input$countryInput)) { return(NULL) } bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) Now when the render function tries to access the data, it will get a NULL value before the app is fully loaded. You will still get an error, because the ggplot() function will not work with a NULL dataset, so we also need to make a similar check in the renderPlot() function. Only once the data is loaded, we can try to plot. output$coolplot &lt;- renderPlot({ if (is.null(filtered())) { return() } ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) The renderTable() function doesnt need this fix applied because Shiny doesnt have a problem rendering a NULL table. Exercise: Change the product type radio buttons to get generated in the server with the values from the dataset, instead of being created in the UI with the values entered manually. If youre feeling confident, try adding an input for subtype that will get re-generated every time a new type is chosen, and will be populated with all the subtype options available for the currently selected type (for example, if \"WINE\" is selected, then the subtypes are white wine, red wine, etc.). 83.4.6 Final Shiny app code In case you got lost somewhere, here is the final code. The app is now functional, but there are plenty of features you can add to make it better. library(shiny) library(ggplot2) library(dplyr) bcl &lt;- read.csv(&quot;bcl-data.csv&quot;, stringsAsFactors = FALSE) ui &lt;- fluidPage( titlePanel(&quot;BC Liquor Store prices&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;priceInput&quot;, &quot;Price&quot;, 0, 100, c(25, 40), pre = &quot;$&quot;), radioButtons(&quot;typeInput&quot;, &quot;Product type&quot;, choices = c(&quot;BEER&quot;, &quot;REFRESHMENT&quot;, &quot;SPIRITS&quot;, &quot;WINE&quot;), selected = &quot;WINE&quot;), uiOutput(&quot;countryOutput&quot;) ), mainPanel( plotOutput(&quot;coolplot&quot;), br(), br(), tableOutput(&quot;results&quot;) ) ) ) server &lt;- function(input, output) { output$countryOutput &lt;- renderUI({ selectInput(&quot;countryInput&quot;, &quot;Country&quot;, sort(unique(bcl$Country)), selected = &quot;CANADA&quot;) }) filtered &lt;- reactive({ if (is.null(input$countryInput)) { return(NULL) } bcl %&gt;% filter(Price &gt;= input$priceInput[1], Price &lt;= input$priceInput[2], Type == input$typeInput, Country == input$countryInput ) }) output$coolplot &lt;- renderPlot({ if (is.null(filtered())) { return() } ggplot(filtered(), aes(Alcohol_Content)) + geom_histogram() }) output$results &lt;- renderTable({ filtered() }) } shinyApp(ui = ui, server = server) 83.4.7 Share your app with the world Remember how every single app is a web page powered by an R session on a computer? So far, youve been running Shiny locally, which means your computer was used to power the app. It also means that the app was not accessible to anyone on the internet. If you want to share your app with the world, you need to host it somewhere. 83.4.7.1 Host on shinyapps.io RStudio provides a service called shinyapps.io which lets you host your apps for free. It is integrated seamlessly into RStudio so that you can publish your apps with the click of a button, and it has a free version. The free version allows a certain number of apps per user and a certain number of activity on each app, but it should be good enough for most of you. It also lets you see some basic stats about usage of your app. Hosting your app on shinyapps.io is the easy and recommended way of getting your app online. Go to www.shinyapps.io and sign up for an account. When youre ready to publish your app, click on the Publish Application button in RStudio and follow the instructions. You might be asked to install a couple packages if its your first time. Figure 83.7: Shiny publish application button After a successful deployment to shinyapps.io, you will be redirected to your app in the browser. You can use that URL to show off to your family what a cool app you wrote. 83.4.7.2 Host on a Shiny Server The other option for hosting your app is on your own private Shiny Server. Shiny Server is also a product by RStudio that lets you host apps on your own server. This means that instead of RStudio hosting the app for you, you have it on your own private server. This means you have a lot more freedom and flexibility, but it also means you need to have a server and be comfortable administering a server. I currently host all my apps on my own Shiny Server just because I like having the extra control, but when I first learned about Shiny I used shinyapps.io for several months. If youre feeling adventurous and want to host your own server, you can follow my tutorial for hosting a Shiny Server. 83.4.8 More Shiny features to check out Shiny is extremely powerful and has lots of features that we havent covered. Heres a sneak peek of just a few other common Shiny features that are not too advanced. 83.4.8.1 Shiny in R Markdown You can include Shiny inputs and outputs in an R Markdown document! This means that your R Markdown document can be interactive. Learn more here. Heres a simple example of how to include interactive Shiny elements in an R Markdown: --- output: html_document runtime: shiny --- ```{r echo=FALSE} sliderInput(&quot;num&quot;, &quot;Choose a number&quot;, 0, 100, 20) renderPlot({ plot(seq(input$num)) }) ``` 83.4.8.2 Use conditionalPanel() to conditionally show UI elements You can use conditionalPanel() to either show or hide a UI element based on a simple condition, such as the value of another input. Learn more via ?conditionalPanel. library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Number&quot;, 5, 1, 10), conditionalPanel( &quot;input.num &gt;=5&quot;, &quot;Hello!&quot; ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 83.4.8.3 Use navbarPage() or tabsetPanel() to have multiple tabs in the UI If your apps requires more than a single view, you can have separate tabs. Learn more via ?navbarPage or ?tabsetPanel. library(shiny) ui &lt;- fluidPage( tabsetPanel( tabPanel(&quot;Tab 1&quot;, &quot;Hello&quot;), tabPanel(&quot;Tab 2&quot;, &quot;there!&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 83.4.8.4 Use DT for beautiful, interactive tables Whenever you use tableOutput() + renderTable(), the table that Shiny creates is a static and boring-looking table. If you download the DT package, you can replace the default table with a much sleeker table by just using DT::dataTableOutput() + DT::renderDataTable(). Its worth trying. Learn more on DTs website. 83.4.8.5 Use isolate() function to remove a dependency on a reactive variable When you have multiple reactive variables inside a reactive context, the whole code block will get re-executed whenever any of the reactive variables change because all the variables become dependencies of the code. If you want to suppress this behavior and cause a reactive variable to not be a dependency, you can wrap the code that uses that variable inside the isolate() function. Any reactive variables that are inside isolate() will not result in the code re-executing when their value is changed. Read more about this behavior via ?isolate. 83.4.8.6 Use update*Input() functions to update input values programmatically Any input function has an equivalent update*Input function that can be used to update any of its parameters. library(shiny) ui &lt;- fluidPage( sliderInput(&quot;slider&quot;, &quot;Move me&quot;, value = 5, 1, 10), numericInput(&quot;num&quot;, &quot;Number&quot;, value = 5, 1, 10) ) server &lt;- function(input, output, session) { observe({ updateNumericInput(session, &quot;num&quot;, value = input$slider) }) } shinyApp(ui = ui, server = server) Note that we used an additional argument session when defining the server function. While the input and output arguments are mandatory, the session argument is optional. You need to define the session argument when you want to use functions that need to access the session. The session parameter actually has some useful information in it, you can learn more about this via ?shiny::session. 83.4.9 Scoping rules in Shiny apps Scoping is very important to understand in Shiny once you want to support more than one user at a time. Since your app can be hosted online, multiple users can use your app simultaneously. If there are any variables (such as datasets or global parameters) that should be shared by all users, then you can safely define them globally. But any variable that should be specific to each users session should be not be defined globally. You can think of the server function as a sandbox for each user. Any code outside of the server function is run once and is shared by all the instances of your Shiny app. Any code inside the server is run once for every user that visits your app. This means that any user-specific variables should be defined inside server. If you look at the code in our BC Liquor Store app, youll see that we followed this rule: the raw dataset was loaded outside the server and is therefore available to all users, but the filtered object is constructed inside the server so that every user has their own version of it. If filtered was a global variable, then when one user changes the values in your app, all other users connected to your app would see the change happen. You can learn more about the scoping rules in Shiny here. 83.4.9.1 Use global.R to define objects available to both ui.R and server.R If there are objects that you want to have available to both ui.R and server.R, you can place them in global.R. You can learn more about global.R and other scoping rules here. 83.4.10 Add images You can add an image to your Shiny app by placing an image under the www/ folder and using the UI function img(src = \"image.png\"). Shiny will know to automatically look in the www/ folder for the image. 83.4.10.1 Add JavaScript/CSS If you know JavaScript or CSS you are more than welcome to use some in your app. library(shiny) ui &lt;- fluidPage( tags$head(tags$script(&quot;alert(&#39;Hello!&#39;);&quot;)), tags$head(tags$style(&quot;body{ color: blue; }&quot;)), &quot;Hello&quot; ) server &lt;- function(input, output) { } shinyApp(ui = ui, server = server) If you do want to add some JavaScript or use common JavaScript functions in your apps, you might want to check out shinyjs. 83.5 Ideas to improve our app The app we developed is functional, but there are plenty of improvements that can be made. You can compare the app we developed to Deans version of this app to get an idea of what a (slightly) more functional app could include. Here are some suggestions of varying difficulties. Each idea also has a hint, Dean recommends only reading the hint if youre stuck for 10 minutes. Split the app into two separate files: ui.R and server.R. Hint: All the code assigned into the ui variable goes into ui.R and all the code for the server function goes into server.R. You do not need to explicitly call the shinyApp() function. Add an option to sort the results table by price. Hint: Use checkboxInput() to get TRUE/FALSE values from the user. Add an image of the BC Liquor Store to the UI. Hint: Place the image in a folder named www, and use img(src = \"imagename.png\") to add the image. Share your app with everyone on the internet by deploying to shinyapps.io. Hint: Go to shinyapps.io, register for an account, then click the Publish App button in RStudio. Use the DT package to turn the current results table into an interactive table. Hint: Install the DT package, replace tableOutput() with DT::dataTableOutput() and replace renderTable() with DT::renderDataTable(). Add parameters to the plot. Hint: You will need to add input functions that will be used as parameters for the plot. You could use shinyjs::colourInput() to let the user decide on the colours of the bars in the plot. The app currently behaves strangely when the user selects filters that return 0 results. For example, try searching for wines from Belgium. There will be an empty plot and empty table generated, and there will be a warning message in the R Console. Try to figure out why this warning message is appearing, and how to fix it. Hint: The problem happens because renderPlot() and renderTable() are trying to render an empty data frame. To fix this issue, the filtered reactive expression should check for the number of rows in the filtered data, and if that number is 0 then return NULL instead of a 0-row data frame. Place the plot and the table in separate tabs. Hint: Use tabsetPanel() to create an interface with multiple tabs. If you know CSS, add CSS to make your app look nicer. Hint: Add a CSS file under www and use the function includeCSS() to use it in your app. Experiment with packages that add extra features to Shiny, such as shinyjs, leaflet, shinydashboard, shinythemes, ggvis. Hint: Each package is unique and has a different purpose, so you need to read the documentation of each package in order to know what it provides and how to use it. Show the number of results found whenever the filters change. For example, when searching for Italian wines $20-$40, the app would show the text We found 122 options for you. Hint: Add a textOutput() to the UI, and in its corresponding renderText() use the number of rows in the filtered() object. Allow the user to download the results table as a .csv file. Hint: Look into the downloadButton() and downloadHandler() functions. When the user wants to see only wines, show a new input that allows the user to filter by sweetness level. Only show this input if wines are selected. Hint: Create a new input function for the sweetness level, and use it in the server code that filters the data. Use conditionalPanel() to conditionally show this new input. The condition argument of conditionalPanel should be something like input.typeInput == \"WINE\". Allow the user to search for multiple alcohol types simultaneously, instead of being able to choose only wines/beers/etc. Hint: There are two approaches to do this. Either change the typeInput radio buttons into checkboxes (checkboxGroupInput()) since checkboxes support choosing multiple items, or change typeInput into a select box (selectInput()) with the argument multiple = TRUE to support choosing multiple options. If you look at the dataset, youll see that each product has a type (beer, wine, spirit, or refreshment) and also a subtype (red wine, rum, cider, etc.). Add an input for subtype that will let the user filter for only a specific subtype of products. Since each type has different subtype options, the choices for subtype should get re-generated every time a new type is chosen. For example, if wine is selected, then the subtypes available should be white wine, red wine, etc. Hint: Use uiOutput() to create this input in the server code. Provide a way for the user to show results from all countries (instead of forcing a filter by only one specific country). Hint: There are two ways to approach this. You can either add a value of All to the dropdown list of country options, you can include a checkbox for Filter by country and only show the dropdown. "],["shiny-resources.html", "84 Shiny Resources 84.1 Awesome add-on packages to Shiny", " 84 Shiny Resources Shiny is a very popular package and has lots of resources on the web. Heres a compiled list of a few resources I recommend, which are all fairly easy to read and understand. Shiny official website Shiny official tutorial Shiny cheatsheet Lots of short useful articles about different topics in Shiny - highly recommended Shiny in R Markdown Get help from the Shiny Google group or StackOverflow Publish your apps for free with shinyapps.io Host your app on your own Shiny server Learn about how reactivity works Learn about useful debugging techniques 84.1 Awesome add-on packages to Shiny Many people have written packages that enhance Shiny in some way or add extra functionality. Here is a list of several popular packages that people often use together with Shiny: shinythemes - Easily alter the appearance of your app (CRAN). shinyjs - Enhance user experience in Shiny apps using JavaScript functions without knowing JavaScript (CRAN; GitHub). leaflet - Add interactive maps to your apps (CRAN; GitHub). ggvis - Similar to ggplot2, but the plots are focused on being web-based and are more interactive (CRAN). shinydashboard - Gives you tools to create visual dashboards (CRAN; GitHub). "],["special-topics-machine-learn.html", "85 Special Topics: Machine, Learn! 85.1 Module Materials", " 85 Special Topics: Machine, Learn! This bonus module is designed to introduce machine learning and neural networks. Please watch the curated videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. me, a statistician: *interviewing for a machine learning job* as i always say &quot;machine learning is basically statistics!&quot; pic.twitter.com/OBY9KKDSRv&mdash;  Kareem Carr  (@kareem_carr) March 2, 2021 I hate to tell you this but most deep learning models are just three or more logistic regressions in a trench coat.&mdash;  Kareem Carr  (@kareem_carr) January 19, 2021 85.1 Module Materials Readings A visual introduction to machine learning "],["neural-networks.html", "86 Neural Networks! 86.1 What is a Neural Network?", " 86 Neural Networks! 86.1 What is a Neural Network? This is what #Tesla Autopilot sees using #neuralnetworks that take 70.000 GPU hours to train and output 1,000 tensors at each timestep#AI #DeepLearning #AutonomousVehicles@mvollmer1 @Hana_ElSayyed @JeroenBartelse @kalydeoo @WSWMUC @pascal_bornet @CurieuxExplorer @fogle_shane pic.twitter.com/h6ZOUSsoNF&mdash; Franco Ronconi  (@FrRonconi) February 6, 2021 "],["natural-language-processing.html", "87 Natural Language Processing", " 87 Natural Language Processing Resources: - https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing - https://app.inferkit.com/demo "],["dont-miss-the-last-module.html", "Dont Miss The Last Module 87.1 Important Wake Forest Stuff", " Dont Miss The Last Module This course was designed to be a starting point. You have learned so much in a short span. I am so proud of each and every one of you!!!!! #library(embedr) #embed_audio(&quot;assets/audio/SoLongFarewellAllParts.mid&quot;) Before you go  I have some important practical things to walk you thru. Most of these are Wake Forest Specific such as making sure to connect your github account to a non-WFU email. As well as a well other things 87.1 Important Wake Forest Stuff For those of you who are graduating from Wake Forest University, you may not be aware that your email address gets shut off very soon after you graduate. Why do they do this? I do not know and this it is a silly policy. Regardless, before you lose access to your WFU email, you need to add a 2nd email address to your github account. Otherwise you will not be able to get access to your materials after you graduate. Github has some incredibly useful guides to do this How to add an email address to your github account Hot to change your primary email address [How to set up a backup email address] (https://docs.github.com/en/github/setting-up-and-managing-your-github-user-account/setting-a-backup-email-address) Please do not procrastinate this! It is really important to do this before you lose access!!!! "],["data-as-rhetoric.html", "Data as rhetoric 87.2 Module Materials", " Data as rhetoric This is not a module. Although it is designed to introduce you to misleading graphics and data. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-16]. The slides used to make the videos in this module can be found in the slides repo. 87.2 Module Materials Slides Data and visualization Suggested Readings Darrel Huff Activities Lab 87.2.1 Estimated Video Length 87.2.2 Technically the Truth Academic friends!! I found this online. How can we incorporate this in our lectures?? I feel like there&#39;s some important message here. pic.twitter.com/AoV2ZrDAH5&mdash; Yuya Kiuchi (@YuyaKiuchi) February 5, 2021 "],["reading-error-codes.html", "88 Reading Error Codes", " 88 Reading Error Codes "],["welcome-to-base-r.html", "Welcome to Base R 88.1 Module Materials", " Welcome to Base R This is not a module. Although it is designed to introduce you to base R. Please watch the videos and work your way through the notes. You can find the video playlist for this module [here][ds4p-pl-17]. The slides used to make the videos in this module can be found in the slides repo. 88.1 Module Materials Slides Suggested Readings Activities Lab a friend made this. we have diverging thoughts, but she is indeed really fast with base R. anyone on her side? #rstats #dplyr #code #rstudio #tidyverse pic.twitter.com/elAPijFJsy&mdash; Simona Bisiani (@BisianiSimona) April 9, 2021 88.1.1 Estimated Video Length "],["odd-legacy-data-types.html", "ODD: Legacy Data Types 88.2 Punchcards 88.3 Magnetic Type 88.4 Paper!", " ODD: Legacy Data Types This optional deep dive contains examples of legacy data structures and formatting. Why? I have found it very helpful to remind myself that whatever problem Im grappling with in R could be so much worse. 88.2 Punchcards 88.2.1 The Keypunch Machine (IBM) 88.2.2 Punch Card Programming - Computerphile 88.3 Magnetic Type 88.4 Paper! "],["thoughts-from-hadley-wickham-on-tidyverse.html", "Thoughts from Hadley Wickham on Tidyverse 88.5 Dive into Hadley Wickhams Tidyverse 88.6 Current State of the Tidyverse (2020) 88.7 Cheatsheets", " Thoughts from Hadley Wickham on Tidyverse Although Im perpetually reluctant to embed videos that I dont host, I think hearing from Hadley Wickham is worthwhile. 88.5 Dive into Hadley Wickhams Tidyverse 88.6 Current State of the Tidyverse (2020) 88.7 Cheatsheets Rstudio has a glorious number of cheatsheets, including: Data Wrangling "],["good-resources.html", "89 Good Resources", " 89 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ Visualizing Linear Models: An R Bag of Tricks "],["media-without-a-home-yet.html", "90 Media without a home yet 90.1 Visualizing Linear Models: An R Bag of Tricks 90.2 For new programmers learning keyboard shortcuts 90.3 Are you a student? If yes, this is the best data science project for you! 90.4 rstudio is magic 90.5 automation quote 90.6 How computer memory works! 90.7 Is Coding a Math Skill or a Language Skill? Neither? Both? 90.8 Quantum Computers Explained! 90.9 The Rise of the Machines  Why Automation is Different this Time 90.10 Who Would Be King of America if George Washington had been made a monarch? 90.11 Emergence  How Stupid Things Become Smart Together 90.12 The Birthday Paradox 90.13 Why cant you divide by zero? 90.14 Yea hes chewing up my stats homework but that face though 90.15 Coding Kitty 90.16 Democratic databases: science on GitHub 90.17 Ten simple rules for getting started on Twitter as a scientist 90.18 NYT data ethics stuff 90.19 ", " 90 Media without a home yet 90.1 Visualizing Linear Models: An R Bag of Tricks I&#39;m starting a 3-week #rstats short course, Visualizing Linear Models: An R Bag of Tricks.One week on univariate models, two weeks on models for multivariate responses. Lectures notes, examples and exercises are at: https://t.co/LF1iVPZOPs&mdash; Michael Friendly (@datavisFriendly) February 27, 2021 90.2 For new programmers learning keyboard shortcuts https://www.shortcutfoo.com/ 90.3 Are you a student? If yes, this is the best data science project for you! 90.4 rstudio is magic Multiple cursors in @RStudio are so handy! Holding down the option key and drag gives me multiple synced cursors  pic.twitter.com/nQKzqIwsou&mdash; Emil Hvitfeldt (@Emil_Hvitfeldt) February 2, 2021 90.5 automation quote &quot;Ive always objected to doing anything over again if I had already done it once.&quot;  Grace Hopper&mdash; Programming Wisdom (@CodeWisdom) February 8, 2021 90.6 How computer memory works! 90.7 Is Coding a Math Skill or a Language Skill? Neither? Both? 90.8 Quantum Computers Explained! 90.9 The Rise of the Machines  Why Automation is Different this Time 90.10 Who Would Be King of America if George Washington had been made a monarch? 90.11 Emergence  How Stupid Things Become Smart Together 90.12 The Birthday Paradox 90.13 Why cant you divide by zero? 90.14 Yea hes chewing up my stats homework but that face though Yea hes chewing up my stats homework but that face though from r/CatsBeingCats 90.15 Coding Kitty https://hostrider.com/ 90.16 Democratic databases: science on GitHub Nature: Democratic databases: science on GitHub (Perkel, 2016). 90.17 Ten simple rules for getting started on Twitter as a scientist https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007513 90.18 NYT data ethics stuff https://www.nytimes.com/2021/01/31/technology/facial-recognition-photo-tool.html 90.19 Art! https://t.co/XuDToJAmnp&mdash; S. Mason Garrison, PhD (@SMasonGarrison) March 18, 2021 "],["welcome-to-the-template-module.html", "91 Welcome to the !template module! 91.1 Module Materials", " 91 Welcome to the !template module! This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 91.1 Module Materials Slides from Lectures LINK Activities LINK Suggested Readings All subchapters of this module, including [Notes on Functions][#functions-part1] r4ds Sections on functions, and Iterations Lab [LAB A][lab08] [LAB B][lab08b] "],["important-topic-a.html", "Important Topic A! Activity", " Important Topic A! Wow Im so humbly grateful much love to yall&mdash; Missy Elliott (@MissyElliott) April 26, 2021 You can follow along with the slides here if they do not appear below. Activity You can find the materials for this activity here. "],["references.html", "References", " References Spector, P. 2008. Data Manipulation with r. Use r! Springer. https://books.google.com/books?id=grfuq1twFe4C. White, Ethan P., Elita Baldridge, Zachary T. Brym, Kenneth J. Locey, Daniel J. McGlinn, and Sarah R. Supp. 2013. Nine Simple Ways to Make It Easier to (Re)use Your Data. PeerJ PrePrints 1 (July): e7v2. https://doi.org/10.7287/peerj.preprints.7v2. Wickham, H. 2015. Advanced r. Chapman &amp; Hall/CRC the r Series. CRC Press. https://books.google.com/books?id=FfsYCwAAQBAJ. Wickham, Hadley. 2011a. Testthat: Get Started with Testing. The R Journal 3 (1): 510. . 2011b. The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, Articles 40 (1): 129. https://doi.org/10.18637/jss.v040.i01. . 2014. Tidy Data. Journal of Statistical Software, Articles 59 (10): 123. https://doi.org/10.18637/jss.v059.i10. Wickham, Hadley, and Jenny Bryan. In progress. R Packages. 2nd ed. https://r-pkgs.org. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. OReilly Media. https://books.google.com/books?id=vfi3DQAAQBAJ. "]]

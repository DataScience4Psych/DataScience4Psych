# ACT: Working with OpenAI's API

This module introduces the basics of interacting with OpenAI's API from R. We'll explore how to make API calls, handle responses, and integrate AI capabilities into data science workflows. You can find the API documentation [here](https://platform.openai.com/docs/api-reference) and the R package documentation for `httr` and `jsonlite` for making HTTP requests and handling JSON data.



```{r llmapi-setup, include = FALSE}
source("common.R")

ds4p_funyoutube <- read.csv("metadata/ds4p_funyoutube.csv", sep = "")
ds4p_urls <- read.csv("./metadata/ds4p_urls.csv")

# install.packages("devtools")

if (!require("tweetrmd")) devtools::install_github("gadenbuie/tweetrmd")
library(tweetrmd) # ... embedding tweets
library(vembedr) # ... embedding youtube videos
library(knitr)
library(tidyverse)
```


## Getting Started

First, we need to load the required packages:

```{r llmapi-include-content, echo=FALSE, message=FALSE}
library(httr) # For making API requests
library(jsonlite) # For handling JSON responses
library(tidyverse) # For data wrangling
```

### API Authentication

To use OpenAI's API, you'll need an API key. Like we learned with other APIs, it's important to keep this secure:

```
# Store API key securely (NEVER commit to Git!)
openai_api_key <- readLines("path/to/api_key.txt")
```

```{r llmapi-setup-dup-config, echo=FALSE, include=FALSE}
if(file.exists("secrets/openai_api_key.txt")){
openai_api_key <- readLines("secrets/openai_api_key.txt")
} else {
  openai_api_key <- Sys.getenv("OPENAI_API_KEY")
}

API_cache <- TRUE
```

### Making API Requests

The core workflow involves:

- Constructing the API request
- Sending it to OpenAI's endpoint
- Processing the response

Next, we define a function to generate text using OpenAI's API. The function takes a prompt as input and returns the generated text.

Here's a basic function for text generation:


```{r llmapi-code-demo, echo=TRUE}
generate_text <- function(prompt, model = "gpt-5-nano", max_output_tokens = 200) {
  response <- POST(
    # curl https://api.openai.com/v1/chat/completions
    url = "https://api.openai.com/v1/chat/completions",
    # -H "Authorization: Bearer $OPENAI_API_KEY"
    add_headers(Authorization = paste("Bearer", openai_api_key)),
    # -H "Content-Type: application/json"
    content_type_json(),
    # -d '{
    #   "model": "gpt-5-nano",
    #   "messages": [{"role": "user", "content": "What is a banana?"}]
    # }'
    encode = "json",
    body = list(
      model = model,
      messages = list(list(role = "user", content = prompt,
                           max_output_tokens = max_output_tokens
                           ))
    )
  )

  str_content <- content(response, "text", encoding = "UTF-8")
  parsed <- fromJSON(str_content)

  # return(parsed$choices[[1]]$text)
  return(parsed)
}
```

I have included comments in the code to show how the API request corresponds to a typical `curl` command you might use in the terminal.


## Example Usage and Handling the Response

Now that we’ve defined our generate_text() function, let’s test it by sending a request to OpenAI’s API and working with the response.

### Step 1: Send a Request

```{r llmapi-code-data, cache = API_cache}
prompt <- "Write a haiku about data science."
generated_text <- generate_text(prompt)
```


### Step 2: Examine the Raw API Response


When we call the `generate_text(prompt)` function, OpenAI's API returns a structured response in JSON format, which R reads as a list. This response contains multiple components, but the most important part is the generated text.

Let's print the raw response to see its structure.

```{r llmapi-code-response-raw, cache = API_cache}
print(generated_text)
```

As you can see, the response is a nested list containing various metadata (e.g., request ID, model name, creation time), the AI-generated response (inside `$choices[[1]]$message$content`), token usage information (inside $usage$total_tokens), and more.

### Step 3: Extract the AI-Generated Text

Since the response contains both metadata and content, we need to extract only the generated text. The key part of the response is stored in:

```{r llmapi-code-extract-text, cache = API_cache}
ai_response <- generated_text$choices$message$content
```

Now, let's print the AI-generated text:

```{r llmapi-code-print-text, cache = API_cache}
print(ai_response)
```

Ok, so that wasn't really readable. Let's try to format it a bit better:


```{r llmapi-code-format-text, results='asis', cache = API_cache}
cat(ai_response, sep = "\n")
```

Now we can see the haiku about data science that the model generated in response to our prompt. This is the core workflow for interacting with OpenAI's API: send a request, receive a structured response, and extract the relevant content for use in your applications.

### Step 4: Understanding Token Usage

Since OpenAI charges based on token usage, it's useful to monitor how many tokens are used per request. The API response includes:

- usage$prompt_tokens → Tokens in the input prompt
- usage$completion_tokens → Tokens generated by the model
- usage$total_tokens → The total token count for billing

To check token usage:

```{r llmapi-code-token-usage, cache = API_cache}
print(generated_text$usage$total_tokens) # Total tokens used
print(generated_text$usage$completion_tokens) # Tokens used for output
print(generated_text$usage$prompt_tokens) # Tokens used for input
```

The token usage information can help you optimize your prompts and manage costs when using the API. 


## Error Handling

Like we've seen with other APIs, it's important to handle errors gracefully. As with any API call, errors can occur due to network issues, invalid requests, or rate limits. To ensure our script doesn’t crash, we can wrap API calls in `tryCatch()`:

```{r llmapi-code-error-handling, echo=TRUE, cache = API_cache}
generate_text_safe <- function(prompt) {
  tryCatch(
    {
      generate_text(prompt)
    },
    error = function(e) {
      warning("API call failed: ", e$message)
      return(NULL)
    }
  )
}
```

Now, we can use `generate_text_safe()` to handle errors. If an error occurs, the function will return `NULL` and print a warning message.

## Processing Multiple Requests

When working with multiple prompts, we can use `purrr::map_chr()` to process them efficiently:


```{r llmapi-code-packages, cache = API_cache}
library(purrr)
prompts <- c(
  "Define p-value",
  "Explain Type I error",
  "What is statistical power?"
)
responses <- list()
responses <- map(prompts, generate_text_safe)
```

This code generates text for each prompt in the `prompts` vector. If an error occurs, the response will be `NULL`. After running this code, we can examine the responses and handle any errors. I've included a table below to display the responses.

```{r llmapi-include-responses-table, message = FALSE, warning = FALSE, echo = FALSE, cache = API_cache}
# can we make this table with the prompt and the response?
responses_df <- tibble(
  prompt = prompts,
  ai_response = map_chr(responses, ~ .x$choices$message$content),
  tokens_used = map_dbl(responses, ~ .x$usage$total_tokens),
  model = map_chr(responses, ~ .x$model),
  completion_time = map_dbl(responses, ~ .x$created)
)

responses_df <- responses_df %>%
  mutate(completion_time = as.POSIXct(completion_time, tz = "UTC"))

responses_df %>%
  DT::datatable(
    rownames = FALSE,
    class = "cell-border stripe",
    # filter = list(position = 'top'),
    options = list(
      pageLength = nrow(responses_df),
      autoWidth = TRUE,
      bInfo = FALSE,
      paging = FALSE
    )
  )
```

As you can see, the table displays the prompts, AI-generated responses, token usage, model name, and completion time for each request. This information can help us monitor the API usage and response quality.


### Rate Limiting

OpenAI has rate limits we need to respect. We can add delays between requests to avoid exceeding these limits. Here's a throttled version of the `generate_text()` function:

```{r llmapi-code-rate-limiting }
generate_text_throttled <- function(prompt) {
  Sys.sleep(1) # Wait 1 second between requests
  generate_text_safe(prompt)
}
```

This function adds a 1-second delay between requests to avoid exceeding OpenAI's rate limits. You can adjust the delay as needed based on the API's rate limits.

## Your Turn!

Now it's your turn to experiment with the OpenAI API! Try different prompts, explore various models, and see how you can integrate AI-generated text into your projects. Remember to monitor your token usage and handle errors gracefully as you work with the API.

I've crafted a prompt to generate your very own activity for this module. You can modify the prompt to create different activities or explore other topics. Here's the prompt I used:

```{r llmapi-code-activity-prompt, cache = API_cache}
activity_prompt <- "Create a meme-tastic data science activity for graduate students learning about using OpenAI's API in Tidyverse. The activity should involve making API calls, handling responses, and analyzing the results. Include clear instructions and learning objectives."


activity_response <- generate_text(activity_prompt, 
                                   model = "gpt-5-nano", 
                                   max_output_tokens = 7000)

writeLines(activity_response$choices$message$content, "includes/activity.txt")
```



Because the response is quite long (at `r activity_response$usage$total_tokens` tokens), I've written it to a text file in the `includes` directory. You can open that file to see the generated activity. The activity is designed to help students learn how to use OpenAI's API in R, including making API calls, handling responses, and analyzing results. It includes clear instructions and learning objectives to guide students through the process. 

You may notice that the activity is generated each time I render this book. If you want to keep a specific version of the activity, you can find it in the [commit history](https://github.com/DataScience4Psych/DataScience4Psych/commits/main/includes/activity.txt) of the `includes/activity.txt` file in the GitHub repository for this book. You can also modify the prompt to generate a new activity or explore different topics as you see fit. Happy experimenting!

Remember that this activity is generated by the OpenAI API, so it requires careful review and editing to ensure it is accurate, clear, and appropriate. Always review AI-generated content before using it in an educational setting to ensure it meets your standards and learning objectives. Don't be just an [AI passenger](https://www.nytimes.com/2026/02/23/us/politics/pentagon-anthropic-ai.html). Trust but verify, as they say.


<details>
<summary>Click to see the generated activity</summary>
```{r llmapi-code-activity-response, cache = API_cache, results='asis'}
if(exists("activity_response")) {
  cat(activity_response$choices$message$content, sep = "\n")} else if (
  file.exists("includes/activity.txt")) {
  activity_content <- readLines("includes/activity.txt")
  cat(activity_content, sep = "\n")
} else {
  cat("Activity file not found. Please run the code to generate the activity.", sep = "\n")
}
```
</details>

## Best Practices

- Always keep your API key secure and never hard-code it in your scripts.
- Monitor token usage to manage costs effectively.
- Handle errors gracefully to ensure your application remains robust.
- Use batching and throttling to manage multiple requests and respect rate limits.
- Regularly check OpenAI's API documentation for updates and changes to endpoints, parameters, and best practices.

## Conclusion

In this guide, we've covered how to generate text using OpenAI's API in R. We've defined a function to interact with the API, handled responses, extracted generated text, monitored token usage, and processed multiple requests. We've also discussed error handling, rate limiting, and best practices for working with the API. 

Now that you have the basics down, you can start experimenting with different prompts, models, and applications. The OpenAI API is powerful and flexible, allowing you to integrate AI capabilities into a wide range of projects, from chatbots to content generation to data analysis. Happy coding!
